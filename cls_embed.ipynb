{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Classification\n",
    "\n",
    "Let's now retrieve the classification embeddings from the chromadb vector database and perform classification on them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb007503dc588dee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = client.get_collection(\"nomic_classification_v1\")"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:32:42.259725Z",
     "start_time": "2024-02-23T23:32:41.111367500Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us get the words along with their classes and division and sections from the json file we created earlier."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84452ecc48ce9511"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"hierarchy.json\", \"r\") as f:\n",
    "    categories = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:32:42.280724400Z",
     "start_time": "2024-02-23T23:32:42.259725Z"
    }
   },
   "id": "6185d3b32070a313",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's retrieve the embeddings and the words along with the class, division, and section from the chromadb and create a dataframe with them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11d5ef59ed70ec91"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                   word                                          embedding  \\\n0             Existence  [-0.029754638671875, -0.0263671875, -0.0127792...   \n1           Inexistence  [-0.01029205322265625, 0.019256591796875, -0.0...   \n2         Consanguinity  [0.0386962890625, -0.01026153564453125, -0.013...   \n3            Trisection  [0.007350921630859375, -0.001903533935546875, ...   \n4             Innocence  [0.0338134765625, 0.0139923095703125, -0.03097...   \n...                 ...                                                ...   \n1052              Knave  [0.0032215118408203125, 0.01201629638671875, -...   \n1053  Disinterestedness  [-0.01776123046875, 0.020965576171875, -0.0221...   \n1054        Selfishness  [0.01071929931640625, 0.049468994140625, -0.03...   \n1055             Virtue  [0.0201416015625, 0.006626129150390625, -0.035...   \n1056               Vice  [0.0035228729248046875, -0.028533935546875, -0...   \n\n                                                class division  \\\n0                 WORDS EXPRESSING ABSTRACT RELATIONS      N/A   \n1                 WORDS EXPRESSING ABSTRACT RELATIONS      N/A   \n2                 WORDS EXPRESSING ABSTRACT RELATIONS      N/A   \n3                 WORDS EXPRESSING ABSTRACT RELATIONS      N/A   \n4     WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n...                                               ...      ...   \n1052  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n1053  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n1054  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n1055  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n1056  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n\n               section  \n0            EXISTENCE  \n1            EXISTENCE  \n2             RELATION  \n3               NUMBER  \n4     MORAL AFFECTIONS  \n...                ...  \n1052  MORAL AFFECTIONS  \n1053  MORAL AFFECTIONS  \n1054  MORAL AFFECTIONS  \n1055  MORAL AFFECTIONS  \n1056  MORAL AFFECTIONS  \n\n[1057 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>embedding</th>\n      <th>class</th>\n      <th>division</th>\n      <th>section</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Existence</td>\n      <td>[-0.029754638671875, -0.0263671875, -0.0127792...</td>\n      <td>WORDS EXPRESSING ABSTRACT RELATIONS</td>\n      <td>N/A</td>\n      <td>EXISTENCE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Inexistence</td>\n      <td>[-0.01029205322265625, 0.019256591796875, -0.0...</td>\n      <td>WORDS EXPRESSING ABSTRACT RELATIONS</td>\n      <td>N/A</td>\n      <td>EXISTENCE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Consanguinity</td>\n      <td>[0.0386962890625, -0.01026153564453125, -0.013...</td>\n      <td>WORDS EXPRESSING ABSTRACT RELATIONS</td>\n      <td>N/A</td>\n      <td>RELATION</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Trisection</td>\n      <td>[0.007350921630859375, -0.001903533935546875, ...</td>\n      <td>WORDS EXPRESSING ABSTRACT RELATIONS</td>\n      <td>N/A</td>\n      <td>NUMBER</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Innocence</td>\n      <td>[0.0338134765625, 0.0139923095703125, -0.03097...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1052</th>\n      <td>Knave</td>\n      <td>[0.0032215118408203125, 0.01201629638671875, -...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>1053</th>\n      <td>Disinterestedness</td>\n      <td>[-0.01776123046875, 0.020965576171875, -0.0221...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>1054</th>\n      <td>Selfishness</td>\n      <td>[0.01071929931640625, 0.049468994140625, -0.03...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>1055</th>\n      <td>Virtue</td>\n      <td>[0.0201416015625, 0.006626129150390625, -0.035...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>1056</th>\n      <td>Vice</td>\n      <td>[0.0035228729248046875, -0.028533935546875, -0...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n  </tbody>\n</table>\n<p>1057 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Retrieve the embeddings, words, and metadata\n",
    "words = collection.get(include=[\"embeddings\", \"documents\"])['documents']\n",
    "embeddings = collection.get(include=[\"embeddings\", \"documents\"])['embeddings']\n",
    "metadata = collection.get(include=[\"embeddings\", \"documents\", \"metadatas\"])['metadatas']\n",
    "\n",
    "# Create a dataframe with the embeddings and the words\n",
    "cls_df = pd.DataFrame({'word': words, 'embedding': embeddings})\n",
    "\n",
    "# Add the class, division, and section from metadata to the dataframe\n",
    "cls_df['class'] = [md['class'] for md in metadata]\n",
    "cls_df['division'] = [md['division'] for md in metadata]\n",
    "cls_df['section'] = [md['section'] for md in metadata]\n",
    "\n",
    "cls_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:32:42.762385500Z",
     "start_time": "2024-02-23T23:32:42.263725100Z"
    }
   },
   "id": "b09af619c5292133",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's prepare the data for classification by converting the class, division, and section to numerical values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3501a068561a6136"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                   word                                          embedding  \\\n0             Existence  [-0.029754638671875, -0.0263671875, -0.0127792...   \n1           Inexistence  [-0.01029205322265625, 0.019256591796875, -0.0...   \n2         Consanguinity  [0.0386962890625, -0.01026153564453125, -0.013...   \n3            Trisection  [0.007350921630859375, -0.001903533935546875, ...   \n4             Innocence  [0.0338134765625, 0.0139923095703125, -0.03097...   \n...                 ...                                                ...   \n1052              Knave  [0.0032215118408203125, 0.01201629638671875, -...   \n1053  Disinterestedness  [-0.01776123046875, 0.020965576171875, -0.0221...   \n1054        Selfishness  [0.01071929931640625, 0.049468994140625, -0.03...   \n1055             Virtue  [0.0201416015625, 0.006626129150390625, -0.035...   \n1056               Vice  [0.0035228729248046875, -0.028533935546875, -0...   \n\n      class  division  section  \n0         0         4        7  \n1         0         4        7  \n2         0         4       29  \n3         0         4       19  \n4         4         4       16  \n...     ...       ...      ...  \n1052      4         4       16  \n1053      4         4       16  \n1054      4         4       16  \n1055      4         4       16  \n1056      4         4       16  \n\n[1057 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>embedding</th>\n      <th>class</th>\n      <th>division</th>\n      <th>section</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Existence</td>\n      <td>[-0.029754638671875, -0.0263671875, -0.0127792...</td>\n      <td>0</td>\n      <td>4</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Inexistence</td>\n      <td>[-0.01029205322265625, 0.019256591796875, -0.0...</td>\n      <td>0</td>\n      <td>4</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Consanguinity</td>\n      <td>[0.0386962890625, -0.01026153564453125, -0.013...</td>\n      <td>0</td>\n      <td>4</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Trisection</td>\n      <td>[0.007350921630859375, -0.001903533935546875, ...</td>\n      <td>0</td>\n      <td>4</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Innocence</td>\n      <td>[0.0338134765625, 0.0139923095703125, -0.03097...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1052</th>\n      <td>Knave</td>\n      <td>[0.0032215118408203125, 0.01201629638671875, -...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1053</th>\n      <td>Disinterestedness</td>\n      <td>[-0.01776123046875, 0.020965576171875, -0.0221...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1054</th>\n      <td>Selfishness</td>\n      <td>[0.01071929931640625, 0.049468994140625, -0.03...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1055</th>\n      <td>Virtue</td>\n      <td>[0.0201416015625, 0.006626129150390625, -0.035...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1056</th>\n      <td>Vice</td>\n      <td>[0.0035228729248046875, -0.028533935546875, -0...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n<p>1057 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all classes and divisions/sections as numerical values\n",
    "cls_df['class'] = pd.Categorical(cls_df['class'])\n",
    "cls_df['division'] = pd.Categorical(cls_df['division'])\n",
    "cls_df['section'] = pd.Categorical(cls_df['section'])\n",
    "\n",
    "cls_df['class'] = cls_df['class'].cat.codes\n",
    "cls_df['division'] = cls_df['division'].cat.codes\n",
    "cls_df['section'] = cls_df['section'].cat.codes\n",
    "cls_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:32:42.774609600Z",
     "start_time": "2024-02-23T23:32:42.763385200Z"
    }
   },
   "id": "516a1edf25fbdd15",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also prepare the embeddings for classification by converting them to a numpy array."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "891ef0bbf5ddeecc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.vstack(cls_df['embedding'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:32:42.827010700Z",
     "start_time": "2024-02-23T23:32:42.774609600Z"
    }
   },
   "id": "1d2b8c0fca50cf83",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class logistic regression\n",
    "\n",
    "Let us start by training a multi-class logistic regression model on the embeddings and the class.\n",
    "\n",
    "Before we do that, however, let's split the data into training and testing sets, using 80% of the data for training and 20% for testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0da1d957a8e1533"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = cls_df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:32:44.354695700Z",
     "start_time": "2024-02-23T23:32:43.306184400Z"
    }
   },
   "id": "e541551066686048",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's train a logistic regression model on the training data and evaluate it on the testing data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b8a7c1f9edc9e63"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=1000, multi_class='ovr', n_jobs=-1, random_state=42)",
      "text/html": "<style>#sk-container-id-5 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-5 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-5 pre {\n  padding: 0;\n}\n\n#sk-container-id-5 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-5 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-5 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-5 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-5 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-5 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-5 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-5 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-5 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-5 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-5 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-5 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-5 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-5 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-5 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-5 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-5 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-5 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-5 div.sk-label label.sk-toggleable__label,\n#sk-container-id-5 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-5 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-5 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-5 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-5 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-5 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-5 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-5 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-5 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-5 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-5 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;ovr&#x27;, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;ovr&#x27;, n_jobs=-1, random_state=42)</pre></div> </div></div></div></div>"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, multi_class='ovr', n_jobs=-1, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:12:20.169952700Z",
     "start_time": "2024-02-23T23:12:16.592596900Z"
    }
   },
   "id": "2a20c0e4ce5e28ab",
   "execution_count": 99
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m metrics\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Generate classification report\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(metrics\u001B[38;5;241m.\u001B[39mclassification_report(y_test, y_pred))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Generate classification report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:32:58.116359800Z",
     "start_time": "2024-02-23T23:32:58.099605500Z"
    }
   },
   "id": "8caa5ddbc9c9df03",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having trained and evaluated the model, we can see that the model has an accuracy of 0.64, \n",
    "which is not bad considering the previous results we got from the clustering of the embeddings.\n",
    "\n",
    "However, let's use a hyperparameter tuning library such as `optuna` to find the best hyperparameters for the model.\n",
    "\n",
    "We will use the `cross_val_score` function from `sklearn` to evaluate the model using cross-validation and the `optuna` library to find the best hyperparameters for the model based on the maximization of the accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f52da790af8442a3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-23 12:26:45,484] A new study created in memory with name: no-name-305a575b-7f01-493b-a017-e69eaec63beb\n",
      "[I 2024-02-23 12:26:51,664] Trial 0 finished with value: 0.6023668639053255 and parameters: {'multi_class': 'ovr', 'C': 661.376255805555}. Best is trial 0 with value: 0.6023668639053255.\n",
      "[I 2024-02-23 12:26:57,083] Trial 1 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'multinomial', 'C': 6.385372774006955}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:02,258] Trial 2 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'multinomial', 'C': 0.0011120920175576884}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:07,827] Trial 3 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'multinomial', 'C': 18.9677376569383}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:13,553] Trial 4 finished with value: 0.5917159763313611 and parameters: {'multi_class': 'multinomial', 'C': 770.3969372004187}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:18,846] Trial 5 finished with value: 0.6307692307692307 and parameters: {'multi_class': 'multinomial', 'C': 1.2739863944024226}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:24,045] Trial 6 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'multinomial', 'C': 0.0003812640729113557}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:29,278] Trial 7 finished with value: 0.5952662721893491 and parameters: {'multi_class': 'multinomial', 'C': 0.43421583486972876}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:34,995] Trial 8 finished with value: 0.5905325443786982 and parameters: {'multi_class': 'multinomial', 'C': 845.63135619237}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:40,229] Trial 9 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 5.2102742278469645}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:27:45,437] Trial 10 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'ovr', 'C': 0.017055805956169127}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:27:50,663] Trial 11 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'ovr', 'C': 10.059386790458108}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:27:55,797] Trial 12 finished with value: 0.30532544378698223 and parameters: {'multi_class': 'ovr', 'C': 0.08425911639227203}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:01,116] Trial 13 finished with value: 0.6414201183431953 and parameters: {'multi_class': 'ovr', 'C': 16.59365955781666}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:06,346] Trial 14 finished with value: 0.6260355029585798 and parameters: {'multi_class': 'ovr', 'C': 59.50760529299438}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:11,523] Trial 15 finished with value: 0.591715976331361 and parameters: {'multi_class': 'ovr', 'C': 7216.5593305356115}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:16,703] Trial 16 finished with value: 0.6071005917159763 and parameters: {'multi_class': 'ovr', 'C': 0.7901032638322626}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:21,994] Trial 17 finished with value: 0.6236686390532544 and parameters: {'multi_class': 'ovr', 'C': 96.87649470981486}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:27,115] Trial 18 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'ovr', 'C': 0.01473177987863997}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:32,421] Trial 19 finished with value: 0.6366863905325443 and parameters: {'multi_class': 'ovr', 'C': 2.796732742045829}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:37,619] Trial 20 finished with value: 0.3739644970414201 and parameters: {'multi_class': 'ovr', 'C': 0.1224591334896701}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:43,325] Trial 21 finished with value: 0.6390532544378698 and parameters: {'multi_class': 'multinomial', 'C': 6.648596341660178}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:48,505] Trial 22 finished with value: 0.6295857988165681 and parameters: {'multi_class': 'ovr', 'C': 50.68157333010183}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:53,858] Trial 23 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'multinomial', 'C': 4.0417741981614785}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:59,052] Trial 24 finished with value: 0.6201183431952663 and parameters: {'multi_class': 'ovr', 'C': 112.3754937620061}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:04,239] Trial 25 finished with value: 0.5230769230769231 and parameters: {'multi_class': 'ovr', 'C': 0.28707050854235727}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:09,871] Trial 26 finished with value: 0.6355029585798817 and parameters: {'multi_class': 'multinomial', 'C': 16.639448040057466}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:15,183] Trial 27 finished with value: 0.5928994082840238 and parameters: {'multi_class': 'ovr', 'C': 365.9690017954215}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:20,468] Trial 28 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'multinomial', 'C': 2.3071119730258167}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:25,680] Trial 29 finished with value: 0.5940828402366864 and parameters: {'multi_class': 'ovr', 'C': 3630.6840145203096}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:30,987] Trial 30 finished with value: 0.6071005917159764 and parameters: {'multi_class': 'ovr', 'C': 302.235452264418}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:36,400] Trial 31 finished with value: 0.6355029585798817 and parameters: {'multi_class': 'multinomial', 'C': 3.945286668819994}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:42,003] Trial 32 finished with value: 0.634319526627219 and parameters: {'multi_class': 'multinomial', 'C': 10.332592844614958}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:47,546] Trial 33 finished with value: 0.6343195266272189 and parameters: {'multi_class': 'multinomial', 'C': 28.398455460462856}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:53,035] Trial 34 finished with value: 0.6402366863905326 and parameters: {'multi_class': 'multinomial', 'C': 8.211781468875872}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:58,351] Trial 35 finished with value: 0.6224852071005917 and parameters: {'multi_class': 'multinomial', 'C': 1.0357543481531144}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:04,022] Trial 36 finished with value: 0.6071005917159764 and parameters: {'multi_class': 'multinomial', 'C': 160.01055238934677}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:09,607] Trial 37 finished with value: 0.6343195266272189 and parameters: {'multi_class': 'multinomial', 'C': 24.208438251671875}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:15,228] Trial 38 finished with value: 0.6390532544378698 and parameters: {'multi_class': 'multinomial', 'C': 6.885732966665508}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:20,469] Trial 39 finished with value: 0.4650887573964497 and parameters: {'multi_class': 'multinomial', 'C': 0.16755469596442507}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:25,641] Trial 40 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'multinomial', 'C': 0.014295772856372542}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:31,139] Trial 41 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'multinomial', 'C': 9.60642215484635}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:36,600] Trial 42 finished with value: 0.6390532544378698 and parameters: {'multi_class': 'multinomial', 'C': 6.217965034063374}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:41,931] Trial 43 finished with value: 0.6355029585798817 and parameters: {'multi_class': 'multinomial', 'C': 1.8226486222643368}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:47,172] Trial 44 finished with value: 0.6023668639053255 and parameters: {'multi_class': 'multinomial', 'C': 0.4970341888476986}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:52,864] Trial 45 finished with value: 0.634319526627219 and parameters: {'multi_class': 'multinomial', 'C': 35.62201117319802}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:58,412] Trial 46 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'multinomial', 'C': 16.465104804530142}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:03,612] Trial 47 finished with value: 0.6390532544378698 and parameters: {'multi_class': 'ovr', 'C': 4.7949183968320535}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:08,913] Trial 48 finished with value: 0.6307692307692307 and parameters: {'multi_class': 'multinomial', 'C': 1.1726580601816083}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:14,081] Trial 49 finished with value: 0.22485207100591714 and parameters: {'multi_class': 'ovr', 'C': 0.0478587296704084}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:19,373] Trial 50 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'ovr', 'C': 0.0035093330905880712}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:24,775] Trial 51 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'multinomial', 'C': 6.3297806322736605}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:30,276] Trial 52 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'multinomial', 'C': 7.688967878037122}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:35,535] Trial 53 finished with value: 0.6047337278106509 and parameters: {'multi_class': 'multinomial', 'C': 0.6244994307197752}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:41,251] Trial 54 finished with value: 0.6213017751479291 and parameters: {'multi_class': 'multinomial', 'C': 62.996774096346016}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:46,633] Trial 55 finished with value: 0.6355029585798817 and parameters: {'multi_class': 'multinomial', 'C': 2.078839473459044}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:51,868] Trial 56 finished with value: 0.6390532544378699 and parameters: {'multi_class': 'ovr', 'C': 26.2547003123418}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:57,135] Trial 57 finished with value: 0.591715976331361 and parameters: {'multi_class': 'ovr', 'C': 1220.6023861193805}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:02,387] Trial 58 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 15.792046645759205}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:07,570] Trial 59 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'ovr', 'C': 0.00015352858302659125}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:12,835] Trial 60 finished with value: 0.6142011834319527 and parameters: {'multi_class': 'ovr', 'C': 165.47585425202297}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:18,083] Trial 61 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 14.648631693359567}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:23,404] Trial 62 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 17.50253244549469}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:28,620] Trial 63 finished with value: 0.621301775147929 and parameters: {'multi_class': 'ovr', 'C': 66.04473729032397}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:33,795] Trial 64 finished with value: 0.6426035502958579 and parameters: {'multi_class': 'ovr', 'C': 17.88270057993157}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:38,969] Trial 65 finished with value: 0.6402366863905326 and parameters: {'multi_class': 'ovr', 'C': 17.667140072483264}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:44,148] Trial 66 finished with value: 0.634319526627219 and parameters: {'multi_class': 'ovr', 'C': 41.99389949594814}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:49,325] Trial 67 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 16.891057826172432}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:54,512] Trial 68 finished with value: 0.6106508875739645 and parameters: {'multi_class': 'ovr', 'C': 234.80038306804983}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:59,689] Trial 69 finished with value: 0.5905325443786983 and parameters: {'multi_class': 'ovr', 'C': 640.0606412891965}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:04,866] Trial 70 finished with value: 0.6248520710059171 and parameters: {'multi_class': 'ovr', 'C': 96.77608961581541}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:10,041] Trial 71 finished with value: 0.6402366863905326 and parameters: {'multi_class': 'ovr', 'C': 14.076753577795639}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:15,217] Trial 72 finished with value: 0.6331360946745562 and parameters: {'multi_class': 'ovr', 'C': 3.3326868233309828}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:20,403] Trial 73 finished with value: 0.6390532544378699 and parameters: {'multi_class': 'ovr', 'C': 10.64315933366284}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:25,578] Trial 74 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'ovr', 'C': 35.85815491493345}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:30,743] Trial 75 finished with value: 0.6343195266272189 and parameters: {'multi_class': 'ovr', 'C': 1.6291475015023487}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:35,919] Trial 76 finished with value: 0.6426035502958579 and parameters: {'multi_class': 'ovr', 'C': 17.09475533954898}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:41,106] Trial 77 finished with value: 0.6177514792899408 and parameters: {'multi_class': 'ovr', 'C': 92.3346445053013}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:46,649] Trial 78 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 16.434419047914194}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:51,825] Trial 79 finished with value: 0.6331360946745562 and parameters: {'multi_class': 'ovr', 'C': 3.3671202273770136}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:56,999] Trial 80 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'ovr', 'C': 19.713894728713353}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:02,176] Trial 81 finished with value: 0.6390532544378698 and parameters: {'multi_class': 'ovr', 'C': 13.639402766158021}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:07,364] Trial 82 finished with value: 0.6260355029585799 and parameters: {'multi_class': 'ovr', 'C': 51.767958696541534}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:12,542] Trial 83 finished with value: 0.6414201183431952 and parameters: {'multi_class': 'ovr', 'C': 27.085673128730605}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:17,720] Trial 84 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'ovr', 'C': 5.566740318860573}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:22,897] Trial 85 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'ovr', 'C': 11.663986934718494}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:28,072] Trial 86 finished with value: 0.6355029585798817 and parameters: {'multi_class': 'ovr', 'C': 2.9361826385008905}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:33,248] Trial 87 finished with value: 0.6402366863905324 and parameters: {'multi_class': 'ovr', 'C': 19.16753329768419}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:38,434] Trial 88 finished with value: 0.6343195266272189 and parameters: {'multi_class': 'ovr', 'C': 41.49601158098556}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:43,610] Trial 89 finished with value: 0.6260355029585799 and parameters: {'multi_class': 'ovr', 'C': 73.16040581483503}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:48,784] Trial 90 finished with value: 0.6402366863905324 and parameters: {'multi_class': 'ovr', 'C': 4.561689325633564}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:53,958] Trial 91 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'ovr', 'C': 28.81946050144084}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:59,144] Trial 92 finished with value: 0.6165680473372781 and parameters: {'multi_class': 'ovr', 'C': 149.0801805434398}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:35:04,319] Trial 93 finished with value: 0.6390532544378698 and parameters: {'multi_class': 'ovr', 'C': 25.62085438236995}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:35:09,494] Trial 94 finished with value: 0.6402366863905324 and parameters: {'multi_class': 'ovr', 'C': 12.516103841010564}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:35:14,669] Trial 95 finished with value: 0.6437869822485206 and parameters: {'multi_class': 'ovr', 'C': 8.038461586013085}. Best is trial 95 with value: 0.6437869822485206.\n",
      "[I 2024-02-23 12:35:19,845] Trial 96 finished with value: 0.6414201183431952 and parameters: {'multi_class': 'ovr', 'C': 8.971620727605226}. Best is trial 95 with value: 0.6437869822485206.\n",
      "[I 2024-02-23 12:35:25,021] Trial 97 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 4.627839059606699}. Best is trial 95 with value: 0.6437869822485206.\n",
      "[I 2024-02-23 12:35:30,195] Trial 98 finished with value: 0.6343195266272189 and parameters: {'multi_class': 'ovr', 'C': 2.0813198836245714}. Best is trial 95 with value: 0.6437869822485206.\n",
      "[I 2024-02-23 12:35:35,372] Trial 99 finished with value: 0.6260355029585799 and parameters: {'multi_class': 'ovr', 'C': 1.3974374559335296}. Best is trial 95 with value: 0.6437869822485206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'multi_class': 'ovr', 'C': 8.038461586013085}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    multi_class = trial.suggest_categorical('multi_class', ['ovr', 'multinomial'])\n",
    "    C = trial.suggest_float('C', 1e-4, 1e4, log=True)\n",
    "\n",
    "    # Create logistic regression model with suggested hyperparameters\n",
    "    model = LogisticRegression(C=C, random_state=42, max_iter=1000, multi_class=multi_class, n_jobs=-1)\n",
    "    # Perform cross-validation and return the mean score\n",
    "    score = cross_val_score(model, X, y, scoring='accuracy')\n",
    "    return score.mean()\n",
    "\n",
    "\n",
    "# Create a study object and specify the optimization direction\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print('Best trial:', study.best_trial.params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T10:35:35.377104500Z",
     "start_time": "2024-02-23T10:26:45.022047500Z"
    }
   },
   "id": "9e76682b23c63239",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class SVM\n",
    "\n",
    "Let's now train a multi-class SVM model on the embeddings and the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e86e3dc9de132389"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "SVC(random_state=22)",
      "text/html": "<style>#sk-container-id-16 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-16 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-16 pre {\n  padding: 0;\n}\n\n#sk-container-id-16 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-16 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-16 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-16 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-16 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-16 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-16 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-16 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-16 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-16 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-16 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-16 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-16 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-16 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-16 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-16 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-16 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-16 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-16 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-16 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-16 div.sk-label label.sk-toggleable__label,\n#sk-container-id-16 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-16 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-16 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-16 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-16 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-16 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-16 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-16 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-16 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-16 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-16 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-16 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(random_state=22)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(random_state=22)</pre></div> </div></div></div></div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='rbf', random_state=22)\n",
    "svm_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T13:02:39.162338200Z",
     "start_time": "2024-02-23T13:02:39.075291700Z"
    }
   },
   "id": "2c66c7edab0d64",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.73      0.70        44\n",
      "           1       0.62      0.53      0.57        30\n",
      "           2       0.71      0.48      0.58        31\n",
      "           3       0.73      0.81      0.77        27\n",
      "           4       0.62      0.77      0.69        31\n",
      "           5       0.63      0.63      0.63        49\n",
      "\n",
      "    accuracy                           0.66       212\n",
      "   macro avg       0.67      0.66      0.66       212\n",
      "weighted avg       0.66      0.66      0.66       212\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T13:02:39.325760700Z",
     "start_time": "2024-02-23T13:02:39.280369500Z"
    }
   },
   "id": "62835de5722c9451",
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use the `optuna` library to find the best hyperparameters for the model based on the maximization of the accuracy.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a87564b264fa6a83"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-23 15:00:44,770] A new study created in memory with name: no-name-e83effc6-f9ff-42c6-819a-7d7b2e053cbe\n",
      "[I 2024-02-23 15:00:45,653] Trial 0 finished with value: 0.21570240543682373 and parameters: {'C': 0.6858905527983484, 'kernel': 'poly', 'gamma': 'auto', 'degree': 5}. Best is trial 0 with value: 0.21570240543682373.\n",
      "[I 2024-02-23 15:00:46,416] Trial 1 finished with value: 0.21570240543682373 and parameters: {'C': 0.00029909714045756334, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 0 with value: 0.21570240543682373.\n",
      "[I 2024-02-23 15:00:46,847] Trial 2 finished with value: 0.4219708486094966 and parameters: {'C': 401.6327806521961, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 2 with value: 0.4219708486094966.\n",
      "[I 2024-02-23 15:00:47,779] Trial 3 finished with value: 0.21570240543682373 and parameters: {'C': 0.05489127315400744, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.4219708486094966.\n",
      "[I 2024-02-23 15:00:48,560] Trial 4 finished with value: 0.21570240543682373 and parameters: {'C': 2.853036044275423, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.4219708486094966.\n",
      "[I 2024-02-23 15:00:49,004] Trial 5 finished with value: 0.5336314048108737 and parameters: {'C': 0.9367968795384186, 'kernel': 'poly', 'gamma': 'scale', 'degree': 3}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:49,910] Trial 6 finished with value: 0.21570240543682373 and parameters: {'C': 0.0030401619329988093, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:50,487] Trial 7 finished with value: 0.21570240543682373 and parameters: {'C': 28.873617996146482, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:51,298] Trial 8 finished with value: 0.21570240543682373 and parameters: {'C': 0.0007490059321435479, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:52,029] Trial 9 finished with value: 0.21570240543682373 and parameters: {'C': 0.00020321716745001284, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:52,665] Trial 10 finished with value: 0.21570240543682373 and parameters: {'C': 0.038371129551696734, 'kernel': 'poly', 'gamma': 'scale', 'degree': 2}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:53,063] Trial 11 finished with value: 0.4626263077886078 and parameters: {'C': 5045.7980319618, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:53,470] Trial 12 finished with value: 0.4626263077886078 and parameters: {'C': 2695.0544686715093, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:53,852] Trial 13 finished with value: 0.4626263077886078 and parameters: {'C': 57.12216783313387, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:54,235] Trial 14 finished with value: 0.4626263077886078 and parameters: {'C': 5907.2629182623505, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:54,896] Trial 15 finished with value: 0.5251184834123223 and parameters: {'C': 2.4146232938275527, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:55,454] Trial 16 finished with value: 0.5364750067066082 and parameters: {'C': 0.8330039710770271, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:56,008] Trial 17 finished with value: 0.4834346776356971 and parameters: {'C': 0.25672270426019855, 'kernel': 'poly', 'gamma': 'scale', 'degree': 3}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:56,884] Trial 18 finished with value: 0.5175310739515335 and parameters: {'C': 19.193727365164328, 'kernel': 'poly', 'gamma': 'scale', 'degree': 3}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:57,442] Trial 19 finished with value: 0.21570240543682373 and parameters: {'C': 0.012001114204796359, 'kernel': 'poly', 'gamma': 'scale', 'degree': 1}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:57,955] Trial 20 finished with value: 0.30841455781096305 and parameters: {'C': 0.1503502678381934, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:58,480] Trial 21 finished with value: 0.5270052758651526 and parameters: {'C': 3.0053666391153078, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:59,005] Trial 22 finished with value: 0.5222704104444246 and parameters: {'C': 5.206806082944573, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:59,478] Trial 23 finished with value: 0.5374139318608602 and parameters: {'C': 0.5574736539390779, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:00:59,954] Trial 24 finished with value: 0.537409460788697 and parameters: {'C': 0.5710010704739024, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:01:00,459] Trial 25 finished with value: 0.21570240543682373 and parameters: {'C': 0.00900565080246091, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:01:00,958] Trial 26 finished with value: 0.4020611642671913 and parameters: {'C': 0.2274328892636367, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:01:01,492] Trial 27 finished with value: 0.5288920683179826 and parameters: {'C': 80.39150177629784, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:01:02,028] Trial 28 finished with value: 0.5288920683179826 and parameters: {'C': 10.232186946875991, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:01:02,499] Trial 29 finished with value: 0.5165921487972815 and parameters: {'C': 0.35929075279492606, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:01:02,939] Trial 30 finished with value: 0.5449879281051596 and parameters: {'C': 1.0185860890704288, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:03,400] Trial 31 finished with value: 0.536461593490119 and parameters: {'C': 0.782917999203791, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:03,917] Trial 32 finished with value: 0.21570240543682373 and parameters: {'C': 0.0765970107839401, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:04,365] Trial 33 finished with value: 0.5411964589108468 and parameters: {'C': 0.8891815875481585, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:04,884] Trial 34 finished with value: 0.21570240543682373 and parameters: {'C': 0.01647351032103306, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:05,398] Trial 35 finished with value: 0.21570240543682373 and parameters: {'C': 6.5580976045012775, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:05,790] Trial 36 finished with value: 0.41722256997227936 and parameters: {'C': 272.4697304013837, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:06,194] Trial 37 finished with value: 0.5421532683537513 and parameters: {'C': 1.7455358794464686, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:06,700] Trial 38 finished with value: 0.21570240543682373 and parameters: {'C': 1.7337022687894112, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:07,218] Trial 39 finished with value: 0.21570240543682373 and parameters: {'C': 0.09132691816657261, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:07,729] Trial 40 finished with value: 0.21570240543682373 and parameters: {'C': 14.568529744128709, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:08,215] Trial 41 finished with value: 0.4928954663328266 and parameters: {'C': 0.5558256752155564, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:08,634] Trial 42 finished with value: 0.5431011356523294 and parameters: {'C': 1.3915608732583902, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:09,011] Trial 43 finished with value: 0.5317311991415542 and parameters: {'C': 2.819516663850495, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:09,538] Trial 44 finished with value: 0.21570240543682373 and parameters: {'C': 0.04018250814350936, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:10,080] Trial 45 finished with value: 0.5364615934901189 and parameters: {'C': 1.443961722416836, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:10,445] Trial 46 finished with value: 0.4456049360636681 and parameters: {'C': 36.74009892934497, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:10,960] Trial 47 finished with value: 0.21570240543682373 and parameters: {'C': 6.191261541193038, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:11,474] Trial 48 finished with value: 0.21570240543682373 and parameters: {'C': 0.12767078101655344, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:12,117] Trial 49 finished with value: 0.21570240543682373 and parameters: {'C': 0.0008903807858525186, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:12,480] Trial 50 finished with value: 0.4626263077886078 and parameters: {'C': 182.18971384708402, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:12,964] Trial 51 finished with value: 0.5004739336492892 and parameters: {'C': 0.5794925566166821, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:13,470] Trial 52 finished with value: 0.42100062595010285 and parameters: {'C': 0.3722912680888432, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:13,911] Trial 53 finished with value: 0.5449879281051596 and parameters: {'C': 0.9888002310498957, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:14,278] Trial 54 finished with value: 0.5232093355986767 and parameters: {'C': 3.655195409031323, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:14,710] Trial 55 finished with value: 0.5459357954037378 and parameters: {'C': 1.1024927438677246, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:15,121] Trial 56 finished with value: 0.5383573280872753 and parameters: {'C': 1.5664565990685015, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:15,628] Trial 57 finished with value: 0.21570240543682373 and parameters: {'C': 1.143942725755809, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:16,044] Trial 58 finished with value: 0.42480997943306803 and parameters: {'C': 1279.6636374340785, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:16,559] Trial 59 finished with value: 0.23837521237592774 and parameters: {'C': 0.21578932374329343, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:16,900] Trial 60 finished with value: 0.4815657694715193 and parameters: {'C': 9.790669566961917, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:17,327] Trial 61 finished with value: 0.5421577394259144 and parameters: {'C': 1.283658220072994, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:17,717] Trial 62 finished with value: 0.5364750067066082 and parameters: {'C': 2.2856034593866843, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:18,157] Trial 63 finished with value: 0.5478270589287311 and parameters: {'C': 1.0428184569634444, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:18,512] Trial 64 finished with value: 0.5147053563444514 and parameters: {'C': 4.849687352262712, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:19,142] Trial 65 finished with value: 0.4720781543414111 and parameters: {'C': 0.31822696471349954, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:19,478] Trial 66 finished with value: 0.4815881248323349 and parameters: {'C': 17.75776602316465, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:20,021] Trial 67 finished with value: 0.21570240543682373 and parameters: {'C': 0.02270826486074277, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:20,534] Trial 68 finished with value: 0.21570240543682373 and parameters: {'C': 0.15262186347727472, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:20,878] Trial 69 finished with value: 0.4976750424751855 and parameters: {'C': 8.429254046971945, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:21,246] Trial 70 finished with value: 0.5251050701958329 and parameters: {'C': 3.305818202714844, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:21,696] Trial 71 finished with value: 0.5402530626844317 and parameters: {'C': 0.8904763161618924, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:22,133] Trial 72 finished with value: 0.5497138513815613 and parameters: {'C': 1.0629033978826774, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:22,539] Trial 73 finished with value: 0.541205401055173 and parameters: {'C': 1.654511203025566, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:23,045] Trial 74 finished with value: 0.45412232853438256 and parameters: {'C': 0.40679067430084803, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:23,432] Trial 75 finished with value: 0.5431011356523294 and parameters: {'C': 1.053044941397734, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:23,824] Trial 76 finished with value: 0.535522668335867 and parameters: {'C': 0.9501447665487768, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:24,298] Trial 77 finished with value: 0.34907895913440046 and parameters: {'C': 0.19525645783848636, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:24,779] Trial 78 finished with value: 0.21570240543682373 and parameters: {'C': 0.0868858891060779, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:25,125] Trial 79 finished with value: 0.5128140928194581 and parameters: {'C': 3.6083601443402857, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:25,703] Trial 80 finished with value: 0.5430921935080032 and parameters: {'C': 0.6475012997439689, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:26,296] Trial 81 finished with value: 0.5364750067066082 and parameters: {'C': 0.528625065412582, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:26,846] Trial 82 finished with value: 0.5345926853259412 and parameters: {'C': 1.2483620596006244, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:27,397] Trial 83 finished with value: 0.5336090494500582 and parameters: {'C': 2.283922608137933, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:27,968] Trial 84 finished with value: 0.5383617991594385 and parameters: {'C': 0.7309935233581463, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:28,430] Trial 85 finished with value: 0.4843870160064384 and parameters: {'C': 0.35735762189230674, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:28,780] Trial 86 finished with value: 0.506187963873737 and parameters: {'C': 5.889925610084395, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:29,422] Trial 87 finished with value: 0.4370830725207905 and parameters: {'C': 0.24636871190910423, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:29,852] Trial 88 finished with value: 0.545001341321649 and parameters: {'C': 1.167103644722507, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:30,346] Trial 89 finished with value: 0.21570240543682373 and parameters: {'C': 0.1324785066699525, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:30,734] Trial 90 finished with value: 0.5355316104801932 and parameters: {'C': 2.379351268455524, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:31,234] Trial 91 finished with value: 0.21570240543682373 and parameters: {'C': 0.00010032262764299192, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:31,670] Trial 92 finished with value: 0.5506617186801395 and parameters: {'C': 1.0744695722664126, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:32,162] Trial 93 finished with value: 0.470195832960744 and parameters: {'C': 0.4928572331214867, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:32,629] Trial 94 finished with value: 0.5374139318608602 and parameters: {'C': 0.744737965353237, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:33,057] Trial 95 finished with value: 0.5440534740230708 and parameters: {'C': 1.2286191543843268, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:33,417] Trial 96 finished with value: 0.5203836179915944 and parameters: {'C': 4.414098823389576, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:33,817] Trial 97 finished with value: 0.5402620048287579 and parameters: {'C': 1.9366672742460471, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:34,251] Trial 98 finished with value: 0.545001341321649 and parameters: {'C': 1.1687543006089869, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:34,589] Trial 99 finished with value: 0.49009657515872307 and parameters: {'C': 14.241952756514161, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'C': 1.0744695722664126, 'kernel': 'sigmoid', 'gamma': 'scale'}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    C = trial.suggest_float('C', 1e-4, 1e4, log=True)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "\n",
    "    # If kernel is 'poly', we also tune degree\n",
    "    if kernel == 'poly':\n",
    "        degree = trial.suggest_int('degree', 1, 5)\n",
    "\n",
    "    # Create and train the SVM model\n",
    "    if kernel == 'poly':\n",
    "        model = SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, random_state=42)\n",
    "    else:\n",
    "        model = SVC(C=C, kernel=kernel, gamma=gamma, random_state=42)\n",
    "\n",
    "    score = cross_val_score(model, X, y, scoring='accuracy')\n",
    "    return score.mean()\n",
    "\n",
    "\n",
    "# Create a study object and specify the optimization direction\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print('Best trial:', study.best_trial.params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T13:01:34.593616600Z",
     "start_time": "2024-02-23T13:00:44.770874700Z"
    }
   },
   "id": "3352fe02bac0f7c4",
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class SGD\n",
    "\n",
    "Let's now train a multi-class SGD model on the embeddings and the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78bd39336c8fa4c1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "SGDClassifier(alpha=2.6545371602330262e-06, eta0=0.007698135990041223,\n              learning_rate='adaptive', loss='squared_hinge', n_jobs=-1,\n              random_state=42)",
      "text/html": "<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(alpha=2.6545371602330262e-06, eta0=0.007698135990041223,\n              learning_rate=&#x27;adaptive&#x27;, loss=&#x27;squared_hinge&#x27;, n_jobs=-1,\n              random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SGDClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.SGDClassifier.html\">?<span>Documentation for SGDClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SGDClassifier(alpha=2.6545371602330262e-06, eta0=0.007698135990041223,\n              learning_rate=&#x27;adaptive&#x27;, loss=&#x27;squared_hinge&#x27;, n_jobs=-1,\n              random_state=42)</pre></div> </div></div></div></div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_model = SGDClassifier(loss='squared_hinge', max_iter=1000, n_jobs=-1, random_state=42,\n",
    "                          penalty='l2', alpha=2.6545371602330262e-06,\n",
    "                          learning_rate='adaptive', eta0=0.007698135990041223)\n",
    "sgd_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:32:49.750605200Z",
     "start_time": "2024-02-23T23:32:49.558580400Z"
    }
   },
   "id": "52c78921d9b40eea",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.70      0.67        44\n",
      "           1       0.64      0.53      0.58        30\n",
      "           2       0.67      0.58      0.62        31\n",
      "           3       0.68      0.70      0.69        27\n",
      "           4       0.65      0.77      0.71        31\n",
      "           5       0.65      0.61      0.63        49\n",
      "\n",
      "    accuracy                           0.65       212\n",
      "   macro avg       0.65      0.65      0.65       212\n",
      "weighted avg       0.65      0.65      0.65       212\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report\n",
    "y_pred = sgd_model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:33:05.233533400Z",
     "start_time": "2024-02-23T23:33:05.226532100Z"
    }
   },
   "id": "fe95d6f1c77a888d",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use the `optuna` library to find the best hyperparameters for the model based on the maximization of the accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc1256b67572c199"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-23 14:55:04,575] A new study created in memory with name: no-name-6d4cad58-8733-45af-90cf-fd4a9269c631\n",
      "[I 2024-02-23 14:55:06,032] Trial 0 finished with value: 0.5372781065088759 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 0.0002843539275930434, 'learning_rate': 'optimal', 'eta0': 4.780347017456094e-05}. Best is trial 0 with value: 0.5372781065088759.\n",
      "[I 2024-02-23 14:55:06,383] Trial 1 finished with value: 0.5195266272189348 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 0.002442571598507189, 'learning_rate': 'invscaling', 'eta0': 0.0020001568877933305}. Best is trial 0 with value: 0.5372781065088759.\n",
      "[I 2024-02-23 14:55:06,995] Trial 2 finished with value: 0.16804733727810653 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.00333383263276087, 'learning_rate': 'constant', 'eta0': 0.01355398656226266}. Best is trial 0 with value: 0.5372781065088759.\n",
      "[I 2024-02-23 14:55:07,110] Trial 3 finished with value: 0.5609467455621302 and parameters: {'loss': 'perceptron', 'penalty': 'l2', 'alpha': 0.003873194749247985, 'learning_rate': 'invscaling', 'eta0': 0.014525023788061198}. Best is trial 3 with value: 0.5609467455621302.\n",
      "[I 2024-02-23 14:55:11,691] Trial 4 finished with value: 0.21183431952662723 and parameters: {'loss': 'log_loss', 'penalty': 'l1', 'alpha': 4.201252276181476e-06, 'learning_rate': 'adaptive', 'eta0': 0.00013237043454297412}. Best is trial 3 with value: 0.5609467455621302.\n",
      "[I 2024-02-23 14:55:11,952] Trial 5 finished with value: 0.5644970414201185 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 4.703141643199157e-06, 'learning_rate': 'invscaling', 'eta0': 0.00024931822306579235}. Best is trial 5 with value: 0.5644970414201185.\n",
      "[I 2024-02-23 14:55:15,652] Trial 6 finished with value: 0.21183431952662723 and parameters: {'loss': 'log_loss', 'penalty': 'elasticnet', 'alpha': 0.034451729006116046, 'learning_rate': 'invscaling', 'eta0': 0.0332569797287207}. Best is trial 5 with value: 0.5644970414201185.\n",
      "[I 2024-02-23 14:55:21,444] Trial 7 finished with value: 0.6260355029585798 and parameters: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 0.00013807805287910079, 'learning_rate': 'adaptive', 'eta0': 0.01396226601173444}. Best is trial 7 with value: 0.6260355029585798.\n",
      "[I 2024-02-23 14:55:23,767] Trial 8 finished with value: 0.21183431952662723 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.540781924320066e-05, 'learning_rate': 'invscaling', 'eta0': 0.0002764078066546451}. Best is trial 7 with value: 0.6260355029585798.\n",
      "[I 2024-02-23 14:55:24,990] Trial 9 finished with value: 0.6082840236686391 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 1.6141634951721352e-05, 'learning_rate': 'optimal', 'eta0': 0.0007062315395433922}. Best is trial 7 with value: 0.6260355029585798.\n",
      "[I 2024-02-23 14:55:25,670] Trial 10 finished with value: 0.6355029585798816 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0001502856247839524, 'learning_rate': 'adaptive', 'eta0': 0.003901726908846741}. Best is trial 10 with value: 0.6355029585798816.\n",
      "[I 2024-02-23 14:55:26,347] Trial 11 finished with value: 0.6366863905325444 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0001466537374998443, 'learning_rate': 'adaptive', 'eta0': 0.0033601873324193175}. Best is trial 11 with value: 0.6366863905325444.\n",
      "[I 2024-02-23 14:55:26,938] Trial 12 finished with value: 0.6402366863905326 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 7.528939373175857e-05, 'learning_rate': 'adaptive', 'eta0': 0.002458677394200516}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:30,566] Trial 13 finished with value: 0.536094674556213 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 3.9368020338987256e-05, 'learning_rate': 'adaptive', 'eta0': 1.0223625591063912e-05}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:31,249] Trial 14 finished with value: 0.6378698224852071 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0008193200858309525, 'learning_rate': 'adaptive', 'eta0': 0.0038186015790885675}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:31,795] Trial 15 finished with value: 0.6142011834319527 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 0.0012560849125271144, 'learning_rate': 'constant', 'eta0': 0.0911286680452879}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:32,411] Trial 16 finished with value: 0.6390532544378699 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.03593646696069724, 'learning_rate': 'adaptive', 'eta0': 0.001034610693863933}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:33,031] Trial 17 finished with value: 0.6319526627218934 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.033144035727560583, 'learning_rate': 'adaptive', 'eta0': 0.0008238601563755858}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:35,555] Trial 18 finished with value: 0.2828402366863906 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.011939801427060045, 'learning_rate': 'adaptive', 'eta0': 0.0011185709655906232}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:36,563] Trial 19 finished with value: 0.21183431952662723 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.4084071391033366e-06, 'learning_rate': 'constant', 'eta0': 2.7971539270882222e-05}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:36,775] Trial 20 finished with value: 0.21183431952662723 and parameters: {'loss': 'log_loss', 'penalty': 'l2', 'alpha': 0.0964492269445362, 'learning_rate': 'optimal', 'eta0': 0.00027537648341266426}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:37,481] Trial 21 finished with value: 0.634319526627219 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0007554821676218763, 'learning_rate': 'adaptive', 'eta0': 0.005280402839018609}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:38,076] Trial 22 finished with value: 0.6355029585798817 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 3.240840585299495e-05, 'learning_rate': 'adaptive', 'eta0': 0.0014835742301318846}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:38,778] Trial 23 finished with value: 0.6307692307692309 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.011197922357053894, 'learning_rate': 'adaptive', 'eta0': 0.006108195591229212}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:39,345] Trial 24 finished with value: 0.6071005917159764 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.000515374669371858, 'learning_rate': 'adaptive', 'eta0': 0.0004330761485342858}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:40,668] Trial 25 finished with value: 0.6378698224852071 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 6.537261909307648e-05, 'learning_rate': 'adaptive', 'eta0': 0.0022385624921684904}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:41,384] Trial 26 finished with value: 0.6248520710059171 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.007364673043653249, 'learning_rate': 'adaptive', 'eta0': 0.006712232229209226}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:42,081] Trial 27 finished with value: 0.578698224852071 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.08046497058704552, 'learning_rate': 'adaptive', 'eta0': 0.00011839039285091934}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:43,389] Trial 28 finished with value: 0.6071005917159763 and parameters: {'loss': 'log_loss', 'penalty': 'l2', 'alpha': 0.0013188658583821093, 'learning_rate': 'constant', 'eta0': 0.034375007541521924}. Best is trial 12 with value: 0.6402366863905326.\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "[I 2024-02-23 14:55:56,730] Trial 29 finished with value: 0.6236686390532544 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 0.00029134865083689667, 'learning_rate': 'optimal', 'eta0': 0.0006642121609630765}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:56,910] Trial 30 finished with value: 0.6011834319526628 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.032258845592292446, 'learning_rate': 'optimal', 'eta0': 0.0026804651341440443}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:58,226] Trial 31 finished with value: 0.6366863905325444 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 7.06948460549565e-05, 'learning_rate': 'adaptive', 'eta0': 0.001718018027072834}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:59,544] Trial 32 finished with value: 0.634319526627219 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 6.505559354571794e-05, 'learning_rate': 'adaptive', 'eta0': 0.001867801025109297}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:01,412] Trial 33 finished with value: 0.6248520710059171 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.00038760355262999464, 'learning_rate': 'adaptive', 'eta0': 0.006651350798821405}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:06,056] Trial 34 finished with value: 0.5798816568047337 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 2.1071298551329704e-05, 'learning_rate': 'adaptive', 'eta0': 0.010210483296183508}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:07,225] Trial 35 finished with value: 0.6189349112426035 and parameters: {'loss': 'perceptron', 'penalty': 'l1', 'alpha': 7.736999341057069e-06, 'learning_rate': 'adaptive', 'eta0': 0.0029448639004582508}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:09,600] Trial 36 finished with value: 0.5928994082840238 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.0026671144238702214, 'learning_rate': 'adaptive', 'eta0': 0.0011604093409858774}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:09,866] Trial 37 finished with value: 0.5644970414201185 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 5.644827027516843e-05, 'learning_rate': 'invscaling', 'eta0': 0.029255948593762352}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:10,515] Trial 38 finished with value: 0.21183431952662723 and parameters: {'loss': 'log_loss', 'penalty': 'l2', 'alpha': 0.00012260680270162087, 'learning_rate': 'constant', 'eta0': 0.0005392721969365657}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:12,228] Trial 39 finished with value: 0.6260355029585799 and parameters: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 0.005929711418721617, 'learning_rate': 'adaptive', 'eta0': 0.002241101465156539}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:12,617] Trial 40 finished with value: 0.5704142011834319 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 8.415821172864536e-06, 'learning_rate': 'invscaling', 'eta0': 0.011983875755429117}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:13,305] Trial 41 finished with value: 0.634319526627219 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.00016642578004349873, 'learning_rate': 'adaptive', 'eta0': 0.004656491079260712}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:13,896] Trial 42 finished with value: 0.6402366863905326 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.00021027745577761477, 'learning_rate': 'adaptive', 'eta0': 0.0029852549777640283}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:14,508] Trial 43 finished with value: 0.634319526627219 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0007442069731342302, 'learning_rate': 'adaptive', 'eta0': 0.0010857957839912774}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:15,067] Trial 44 finished with value: 0.6094674556213018 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.00021688534655629272, 'learning_rate': 'adaptive', 'eta0': 0.00046107707639953265}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:15,454] Trial 45 finished with value: 0.6295857988165681 and parameters: {'loss': 'perceptron', 'penalty': 'l2', 'alpha': 0.0001009179054768441, 'learning_rate': 'adaptive', 'eta0': 0.00017486206938871658}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:20,675] Trial 46 finished with value: 0.6378698224852071 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 3.3146059088394704e-05, 'learning_rate': 'adaptive', 'eta0': 0.02154722275175702}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:20,934] Trial 47 finished with value: 0.5609467455621301 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0014153725034913632, 'learning_rate': 'invscaling', 'eta0': 0.008419735749026921}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:21,416] Trial 48 finished with value: 0.6011834319526628 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 1.954458777827079e-06, 'learning_rate': 'optimal', 'eta0': 0.0036844474717363748}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:22,500] Trial 49 finished with value: 0.21183431952662723 and parameters: {'loss': 'log_loss', 'penalty': 'l2', 'alpha': 2.114463473365723e-05, 'learning_rate': 'adaptive', 'eta0': 0.0014580124330189482}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:22,654] Trial 50 finished with value: 0.6082840236686391 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0004545261557204174, 'learning_rate': 'constant', 'eta0': 0.002717508624196871}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:28,326] Trial 51 finished with value: 0.6331360946745562 and parameters: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 3.877287533775401e-05, 'learning_rate': 'adaptive', 'eta0': 0.022913617363996656}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:33,005] Trial 52 finished with value: 0.6248520710059171 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 1.0698228370387801e-05, 'learning_rate': 'adaptive', 'eta0': 0.06226822152759029}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:38,159] Trial 53 finished with value: 0.6355029585798817 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 8.84965110769968e-05, 'learning_rate': 'adaptive', 'eta0': 0.01415445854699769}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:43,316] Trial 54 finished with value: 0.6378698224852071 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 2.8894937784095334e-05, 'learning_rate': 'adaptive', 'eta0': 0.02163092251181473}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:49,656] Trial 55 finished with value: 0.6153846153846154 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 0.0002309562685886662, 'learning_rate': 'adaptive', 'eta0': 0.0008775228233492432}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:55,810] Trial 56 finished with value: 0.6402366863905324 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 3.154405537556414e-06, 'learning_rate': 'adaptive', 'eta0': 0.0021179364445815343}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:58,731] Trial 57 finished with value: 0.6414201183431951 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 0.0006906327812424678, 'learning_rate': 'adaptive', 'eta0': 0.004159905231008274}. Best is trial 57 with value: 0.6414201183431951.\n",
      "[I 2024-02-23 14:57:01,832] Trial 58 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.7616571024226938e-06, 'learning_rate': 'adaptive', 'eta0': 0.00461086024328032}. Best is trial 58 with value: 0.642603550295858.\n",
      "[I 2024-02-23 14:57:04,925] Trial 59 finished with value: 0.6437869822485207 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 3.320710986324315e-06, 'learning_rate': 'adaptive', 'eta0': 0.00438133371029827}. Best is trial 59 with value: 0.6437869822485207.\n",
      "[I 2024-02-23 14:57:05,361] Trial 60 finished with value: 0.21183431952662723 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 4.318127424023731e-06, 'learning_rate': 'invscaling', 'eta0': 0.0045700892913599425}. Best is trial 59 with value: 0.6437869822485207.\n",
      "[I 2024-02-23 14:57:08,133] Trial 61 finished with value: 0.6449704142011834 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.3555193064698493e-06, 'learning_rate': 'adaptive', 'eta0': 0.007887094167831584}. Best is trial 61 with value: 0.6449704142011834.\n",
      "[I 2024-02-23 14:57:10,957] Trial 62 finished with value: 0.6461538461538462 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.6545371602330262e-06, 'learning_rate': 'adaptive', 'eta0': 0.007698135990041223}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:13,668] Trial 63 finished with value: 0.6402366863905324 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.0019152643615249e-06, 'learning_rate': 'adaptive', 'eta0': 0.00876192414608011}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:16,542] Trial 64 finished with value: 0.6437869822485207 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.2221949881261483e-06, 'learning_rate': 'adaptive', 'eta0': 0.006776554100679505}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:19,357] Trial 65 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.6545418136934685e-06, 'learning_rate': 'adaptive', 'eta0': 0.007390816994505168}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:22,141] Trial 66 finished with value: 0.6449704142011834 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.5613264053668994e-06, 'learning_rate': 'adaptive', 'eta0': 0.007921396892606392}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:22,673] Trial 67 finished with value: 0.5822485207100592 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.8308852509923897e-06, 'learning_rate': 'optimal', 'eta0': 0.018315228347967154}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:25,435] Trial 68 finished with value: 0.6449704142011834 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.923972865332898e-06, 'learning_rate': 'adaptive', 'eta0': 0.007984988369010196}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:27,354] Trial 69 finished with value: 0.6378698224852071 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 5.538329516512447e-06, 'learning_rate': 'constant', 'eta0': 0.01299119478567728}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:30,332] Trial 70 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.8976081721257722e-06, 'learning_rate': 'adaptive', 'eta0': 0.00568235854222267}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:33,162] Trial 71 finished with value: 0.6461538461538462 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.0000114129509014e-06, 'learning_rate': 'adaptive', 'eta0': 0.007693697022145743}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:35,759] Trial 72 finished with value: 0.6343195266272189 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.0334385120388943e-06, 'learning_rate': 'adaptive', 'eta0': 0.009977625565760845}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:38,265] Trial 73 finished with value: 0.6319526627218935 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.5890867701116983e-06, 'learning_rate': 'adaptive', 'eta0': 0.015303533813411315}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:40,603] Trial 74 finished with value: 0.6343195266272189 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 3.675053250274435e-06, 'learning_rate': 'adaptive', 'eta0': 0.03845990255031534}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:43,466] Trial 75 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 5.489197794074045e-06, 'learning_rate': 'adaptive', 'eta0': 0.0069555067326768924}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:46,091] Trial 76 finished with value: 0.6331360946745562 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.0015487821510132e-06, 'learning_rate': 'adaptive', 'eta0': 0.010652745235428104}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:49,060] Trial 77 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.2169108223472496e-05, 'learning_rate': 'adaptive', 'eta0': 0.005725439378179114}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:51,629] Trial 78 finished with value: 0.6355029585798817 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 6.712234488232065e-06, 'learning_rate': 'adaptive', 'eta0': 0.016325517713274177}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:52,106] Trial 79 finished with value: 0.5857988165680472 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.4085252336433864e-06, 'learning_rate': 'optimal', 'eta0': 0.003499540185077609}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:54,859] Trial 80 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.430315773822764e-06, 'learning_rate': 'adaptive', 'eta0': 0.00811692482713877}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:57,610] Trial 81 finished with value: 0.6414201183431952 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.5728819742460055e-06, 'learning_rate': 'adaptive', 'eta0': 0.008217199333106547}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:00,648] Trial 82 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 3.8508681764401255e-06, 'learning_rate': 'adaptive', 'eta0': 0.004964472238746803}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:03,468] Trial 83 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.380838881624293e-06, 'learning_rate': 'adaptive', 'eta0': 0.007401712702687876}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:06,043] Trial 84 finished with value: 0.6378698224852071 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 5.125158506394291e-06, 'learning_rate': 'adaptive', 'eta0': 0.011786116181366664}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:09,045] Trial 85 finished with value: 0.6437869822485207 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.3920214389107398e-06, 'learning_rate': 'adaptive', 'eta0': 0.005220901932982213}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:12,046] Trial 86 finished with value: 0.6426035502958579 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.1793631179450648e-06, 'learning_rate': 'adaptive', 'eta0': 0.005390028730477817}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:12,560] Trial 87 finished with value: 0.21183431952662723 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.9734857758370485e-06, 'learning_rate': 'invscaling', 'eta0': 0.003672108553247853}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:12,678] Trial 88 finished with value: 0.5301775147928994 and parameters: {'loss': 'perceptron', 'penalty': 'l2', 'alpha': 3.538437461801765e-06, 'learning_rate': 'constant', 'eta0': 0.010065546956304615}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:15,991] Trial 89 finished with value: 0.634319526627219 and parameters: {'loss': 'log_loss', 'penalty': 'l2', 'alpha': 9.489498668664933e-06, 'learning_rate': 'adaptive', 'eta0': 0.029003779258033557}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:19,351] Trial 90 finished with value: 0.6319526627218935 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 6.390683497316898e-06, 'learning_rate': 'adaptive', 'eta0': 0.0014956649773087225}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:22,240] Trial 91 finished with value: 0.6449704142011835 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.3835180322120584e-06, 'learning_rate': 'adaptive', 'eta0': 0.006491974650039931}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:25,343] Trial 92 finished with value: 0.6426035502958579 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.6417849965952658e-06, 'learning_rate': 'adaptive', 'eta0': 0.004538160810101424}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:28,566] Trial 93 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.3653172483806027e-06, 'learning_rate': 'adaptive', 'eta0': 0.0031381615495628882}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:31,451] Trial 94 finished with value: 0.6437869822485207 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 4.437910708194503e-06, 'learning_rate': 'adaptive', 'eta0': 0.006727163601079937}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:34,407] Trial 95 finished with value: 0.6402366863905326 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 4.309779516059477e-06, 'learning_rate': 'adaptive', 'eta0': 0.006365176496213275}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:36,943] Trial 96 finished with value: 0.6355029585798817 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.435943922872648e-05, 'learning_rate': 'adaptive', 'eta0': 0.012749283322758148}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:39,566] Trial 97 finished with value: 0.6355029585798817 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.3296323442640159e-06, 'learning_rate': 'adaptive', 'eta0': 0.009405206695751607}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:42,097] Trial 98 finished with value: 0.6414201183431953 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 7.405382681005497e-06, 'learning_rate': 'adaptive', 'eta0': 0.01825036599468478}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:45,072] Trial 99 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 3.410501360698341e-06, 'learning_rate': 'adaptive', 'eta0': 0.00562905910748539}. Best is trial 62 with value: 0.6461538461538462.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.6545371602330262e-06, 'learning_rate': 'adaptive', 'eta0': 0.007698135990041223}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    loss = trial.suggest_categorical('loss', ['hinge', 'log_loss', 'squared_hinge', 'perceptron'])\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2', 'l1', 'elasticnet'])\n",
    "    alpha = trial.suggest_float('alpha', 1e-6, 1e-1, log=True)\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'optimal', 'invscaling', 'adaptive'])\n",
    "    eta0 = trial.suggest_float('eta0', 1e-5, 1e-1, log=True)  # Only relevant for certain learning rates\n",
    "\n",
    "    # Create and train the SGD Classifier\n",
    "    model = SGDClassifier(loss=loss, penalty=penalty, alpha=alpha, learning_rate=learning_rate, eta0=eta0,\n",
    "                          random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Perform cross-validation and return the mean score\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)  # cv=5 for 5-fold cross-validation\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "# Create a study object and specify the optimization direction\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)  # Adjust the number of trials as needed\n",
    "\n",
    "# Best hyperparameters\n",
    "print('Best trial:', study.best_trial.params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:58:45.075343400Z",
     "start_time": "2024-02-23T12:55:04.576219Z"
    }
   },
   "id": "4477cdb859019617",
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "We notice that the SGD model gave the best accuracy of 0.65, which is the highest among the models we have tried so far.\n",
    "\n",
    "So, we will get the best hyperparameters for the model and use them to train the model again, for later use."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f9ee6d9062ffe6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "sgd_model = SGDClassifier(**best_params, random_state=42)\n",
    "sgd_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "114f2bb29b45ae2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class Naive Bayes\n",
    "\n",
    "It is known that Naive Bayes is a simple and effective algorithm for classification for NLP tasks. \n",
    "Let's now train a multi-class Naive Bayes model on the embeddings and the class, to see how it performs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fab4ff2e2e09606"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.70      0.72        44\n",
      "           1       0.55      0.53      0.54        30\n",
      "           2       0.58      0.45      0.51        31\n",
      "           3       0.74      0.85      0.79        27\n",
      "           4       0.51      0.61      0.56        31\n",
      "           5       0.61      0.61      0.61        49\n",
      "\n",
      "    accuracy                           0.63       212\n",
      "   macro avg       0.62      0.63      0.62       212\n",
      "weighted avg       0.63      0.63      0.63       212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T00:02:04.765241900Z",
     "start_time": "2024-02-22T00:02:04.749418400Z"
    }
   },
   "id": "fa13a14098b3636c",
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the Naive Bayes model has an accuracy of 0.63, which does not differ much from the logistic regression model.\n",
    "So there is no need to use `optuna` to find the best hyperparameters for the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e859bc93b6dcd506"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class Random Forest\n",
    "\n",
    "Let's now train a multi-class Random Forest model on the embeddings and the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c9e62a3e6912a48"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.67        44\n",
      "           1       0.61      0.47      0.53        30\n",
      "           2       0.86      0.39      0.53        31\n",
      "           3       0.77      0.63      0.69        27\n",
      "           4       0.56      0.77      0.65        31\n",
      "           5       0.52      0.71      0.60        49\n",
      "\n",
      "    accuracy                           0.62       212\n",
      "   macro avg       0.67      0.61      0.61       212\n",
      "weighted avg       0.65      0.62      0.61       212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=15,\n",
    "                               max_depth=100, min_samples_split=2, min_samples_leaf=1, max_features='sqrt')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T23:59:20.973163800Z",
     "start_time": "2024-02-21T23:59:19.524753300Z"
    }
   },
   "id": "a8fc25a6a41a309b",
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class CatBoost\n",
    "\n",
    "Having tried the above models, let us now proceed to more advanced models such as CatBoost, \n",
    "which is a gradient boosting library that is known to perform well on tabular data, to see how it performs \n",
    "and if it can outperform the other models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28cda8d8bf460600"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.134538\n",
      "0:\tlearn: 1.7228745\ttotal: 25.9ms\tremaining: 10.3s\n",
      "1:\tlearn: 1.6604869\ttotal: 47.1ms\tremaining: 9.37s\n",
      "2:\tlearn: 1.6080402\ttotal: 68.8ms\tremaining: 9.11s\n",
      "3:\tlearn: 1.5585689\ttotal: 89.7ms\tremaining: 8.88s\n",
      "4:\tlearn: 1.5091478\ttotal: 110ms\tremaining: 8.72s\n",
      "5:\tlearn: 1.4662697\ttotal: 131ms\tremaining: 8.62s\n",
      "6:\tlearn: 1.4287979\ttotal: 152ms\tremaining: 8.52s\n",
      "7:\tlearn: 1.3910584\ttotal: 173ms\tremaining: 8.46s\n",
      "8:\tlearn: 1.3552419\ttotal: 194ms\tremaining: 8.41s\n",
      "9:\tlearn: 1.3165946\ttotal: 214ms\tremaining: 8.36s\n",
      "10:\tlearn: 1.2851901\ttotal: 235ms\tremaining: 8.32s\n",
      "11:\tlearn: 1.2504925\ttotal: 257ms\tremaining: 8.31s\n",
      "12:\tlearn: 1.2190597\ttotal: 279ms\tremaining: 8.31s\n",
      "13:\tlearn: 1.1924688\ttotal: 301ms\tremaining: 8.29s\n",
      "14:\tlearn: 1.1640413\ttotal: 323ms\tremaining: 8.3s\n",
      "15:\tlearn: 1.1362085\ttotal: 346ms\tremaining: 8.29s\n",
      "16:\tlearn: 1.1100424\ttotal: 367ms\tremaining: 8.27s\n",
      "17:\tlearn: 1.0878752\ttotal: 388ms\tremaining: 8.22s\n",
      "18:\tlearn: 1.0624523\ttotal: 408ms\tremaining: 8.19s\n",
      "19:\tlearn: 1.0358713\ttotal: 429ms\tremaining: 8.16s\n",
      "20:\tlearn: 1.0105685\ttotal: 451ms\tremaining: 8.14s\n",
      "21:\tlearn: 0.9889355\ttotal: 471ms\tremaining: 8.1s\n",
      "22:\tlearn: 0.9693468\ttotal: 492ms\tremaining: 8.07s\n",
      "23:\tlearn: 0.9472586\ttotal: 513ms\tremaining: 8.04s\n",
      "24:\tlearn: 0.9278074\ttotal: 535ms\tremaining: 8.02s\n",
      "25:\tlearn: 0.9062782\ttotal: 556ms\tremaining: 7.99s\n",
      "26:\tlearn: 0.8873076\ttotal: 576ms\tremaining: 7.96s\n",
      "27:\tlearn: 0.8661164\ttotal: 597ms\tremaining: 7.94s\n",
      "28:\tlearn: 0.8476778\ttotal: 619ms\tremaining: 7.92s\n",
      "29:\tlearn: 0.8313398\ttotal: 640ms\tremaining: 7.89s\n",
      "30:\tlearn: 0.8165745\ttotal: 660ms\tremaining: 7.85s\n",
      "31:\tlearn: 0.8015096\ttotal: 681ms\tremaining: 7.84s\n",
      "32:\tlearn: 0.7840770\ttotal: 704ms\tremaining: 7.82s\n",
      "33:\tlearn: 0.7662083\ttotal: 725ms\tremaining: 7.8s\n",
      "34:\tlearn: 0.7498120\ttotal: 746ms\tremaining: 7.78s\n",
      "35:\tlearn: 0.7370386\ttotal: 766ms\tremaining: 7.74s\n",
      "36:\tlearn: 0.7256692\ttotal: 787ms\tremaining: 7.72s\n",
      "37:\tlearn: 0.7139216\ttotal: 804ms\tremaining: 7.66s\n",
      "38:\tlearn: 0.6983130\ttotal: 826ms\tremaining: 7.64s\n",
      "39:\tlearn: 0.6876474\ttotal: 846ms\tremaining: 7.61s\n",
      "40:\tlearn: 0.6749084\ttotal: 867ms\tremaining: 7.59s\n",
      "41:\tlearn: 0.6631446\ttotal: 887ms\tremaining: 7.56s\n",
      "42:\tlearn: 0.6547389\ttotal: 907ms\tremaining: 7.53s\n",
      "43:\tlearn: 0.6416111\ttotal: 928ms\tremaining: 7.51s\n",
      "44:\tlearn: 0.6300260\ttotal: 950ms\tremaining: 7.49s\n",
      "45:\tlearn: 0.6227455\ttotal: 968ms\tremaining: 7.45s\n",
      "46:\tlearn: 0.6116992\ttotal: 985ms\tremaining: 7.4s\n",
      "47:\tlearn: 0.6017839\ttotal: 1s\tremaining: 7.37s\n",
      "48:\tlearn: 0.5913679\ttotal: 1.03s\tremaining: 7.36s\n",
      "49:\tlearn: 0.5811772\ttotal: 1.05s\tremaining: 7.33s\n",
      "50:\tlearn: 0.5744012\ttotal: 1.06s\tremaining: 7.29s\n",
      "51:\tlearn: 0.5661624\ttotal: 1.09s\tremaining: 7.27s\n",
      "52:\tlearn: 0.5583602\ttotal: 1.1s\tremaining: 7.23s\n",
      "53:\tlearn: 0.5498467\ttotal: 1.12s\tremaining: 7.21s\n",
      "54:\tlearn: 0.5410277\ttotal: 1.15s\tremaining: 7.19s\n",
      "55:\tlearn: 0.5344122\ttotal: 1.17s\tremaining: 7.16s\n",
      "56:\tlearn: 0.5256119\ttotal: 1.19s\tremaining: 7.14s\n",
      "57:\tlearn: 0.5168590\ttotal: 1.21s\tremaining: 7.11s\n",
      "58:\tlearn: 0.5090309\ttotal: 1.22s\tremaining: 7.08s\n",
      "59:\tlearn: 0.5007300\ttotal: 1.24s\tremaining: 7.05s\n",
      "60:\tlearn: 0.4930892\ttotal: 1.26s\tremaining: 7.02s\n",
      "61:\tlearn: 0.4862400\ttotal: 1.28s\tremaining: 7s\n",
      "62:\tlearn: 0.4796660\ttotal: 1.3s\tremaining: 6.97s\n",
      "63:\tlearn: 0.4744049\ttotal: 1.32s\tremaining: 6.93s\n",
      "64:\tlearn: 0.4672496\ttotal: 1.34s\tremaining: 6.91s\n",
      "65:\tlearn: 0.4599536\ttotal: 1.36s\tremaining: 6.9s\n",
      "66:\tlearn: 0.4526283\ttotal: 1.38s\tremaining: 6.88s\n",
      "67:\tlearn: 0.4464831\ttotal: 1.4s\tremaining: 6.85s\n",
      "68:\tlearn: 0.4403173\ttotal: 1.42s\tremaining: 6.83s\n",
      "69:\tlearn: 0.4353264\ttotal: 1.44s\tremaining: 6.79s\n",
      "70:\tlearn: 0.4292748\ttotal: 1.46s\tremaining: 6.77s\n",
      "71:\tlearn: 0.4231607\ttotal: 1.48s\tremaining: 6.74s\n",
      "72:\tlearn: 0.4173319\ttotal: 1.5s\tremaining: 6.71s\n",
      "73:\tlearn: 0.4120874\ttotal: 1.52s\tremaining: 6.69s\n",
      "74:\tlearn: 0.4075504\ttotal: 1.54s\tremaining: 6.66s\n",
      "75:\tlearn: 0.4036470\ttotal: 1.55s\tremaining: 6.63s\n",
      "76:\tlearn: 0.3971942\ttotal: 1.58s\tremaining: 6.61s\n",
      "77:\tlearn: 0.3923635\ttotal: 1.59s\tremaining: 6.59s\n",
      "78:\tlearn: 0.3871207\ttotal: 1.62s\tremaining: 6.57s\n",
      "79:\tlearn: 0.3824068\ttotal: 1.64s\tremaining: 6.54s\n",
      "80:\tlearn: 0.3781196\ttotal: 1.65s\tremaining: 6.52s\n",
      "81:\tlearn: 0.3720005\ttotal: 1.67s\tremaining: 6.49s\n",
      "82:\tlearn: 0.3662377\ttotal: 1.69s\tremaining: 6.47s\n",
      "83:\tlearn: 0.3621305\ttotal: 1.71s\tremaining: 6.44s\n",
      "84:\tlearn: 0.3580423\ttotal: 1.73s\tremaining: 6.41s\n",
      "85:\tlearn: 0.3542212\ttotal: 1.75s\tremaining: 6.39s\n",
      "86:\tlearn: 0.3493992\ttotal: 1.77s\tremaining: 6.37s\n",
      "87:\tlearn: 0.3446388\ttotal: 1.79s\tremaining: 6.35s\n",
      "88:\tlearn: 0.3398332\ttotal: 1.81s\tremaining: 6.33s\n",
      "89:\tlearn: 0.3350392\ttotal: 1.83s\tremaining: 6.31s\n",
      "90:\tlearn: 0.3310729\ttotal: 1.85s\tremaining: 6.29s\n",
      "91:\tlearn: 0.3277042\ttotal: 1.87s\tremaining: 6.25s\n",
      "92:\tlearn: 0.3239233\ttotal: 1.89s\tremaining: 6.23s\n",
      "93:\tlearn: 0.3200518\ttotal: 1.91s\tremaining: 6.21s\n",
      "94:\tlearn: 0.3159525\ttotal: 1.93s\tremaining: 6.19s\n",
      "95:\tlearn: 0.3117227\ttotal: 1.95s\tremaining: 6.17s\n",
      "96:\tlearn: 0.3078341\ttotal: 1.97s\tremaining: 6.14s\n",
      "97:\tlearn: 0.3037615\ttotal: 1.99s\tremaining: 6.13s\n",
      "98:\tlearn: 0.3009603\ttotal: 2.01s\tremaining: 6.11s\n",
      "99:\tlearn: 0.2972008\ttotal: 2.03s\tremaining: 6.08s\n",
      "100:\tlearn: 0.2940256\ttotal: 2.04s\tremaining: 6.05s\n",
      "101:\tlearn: 0.2912744\ttotal: 2.06s\tremaining: 6.03s\n",
      "102:\tlearn: 0.2874840\ttotal: 2.08s\tremaining: 6s\n",
      "103:\tlearn: 0.2851025\ttotal: 2.1s\tremaining: 5.97s\n",
      "104:\tlearn: 0.2817369\ttotal: 2.12s\tremaining: 5.96s\n",
      "105:\tlearn: 0.2787950\ttotal: 2.14s\tremaining: 5.93s\n",
      "106:\tlearn: 0.2753153\ttotal: 2.16s\tremaining: 5.91s\n",
      "107:\tlearn: 0.2726117\ttotal: 2.18s\tremaining: 5.89s\n",
      "108:\tlearn: 0.2700277\ttotal: 2.2s\tremaining: 5.86s\n",
      "109:\tlearn: 0.2671003\ttotal: 2.22s\tremaining: 5.84s\n",
      "110:\tlearn: 0.2641920\ttotal: 2.24s\tremaining: 5.82s\n",
      "111:\tlearn: 0.2622460\ttotal: 2.25s\tremaining: 5.8s\n",
      "112:\tlearn: 0.2597229\ttotal: 2.27s\tremaining: 5.77s\n",
      "113:\tlearn: 0.2567605\ttotal: 2.29s\tremaining: 5.75s\n",
      "114:\tlearn: 0.2538125\ttotal: 2.31s\tremaining: 5.74s\n",
      "115:\tlearn: 0.2512457\ttotal: 2.33s\tremaining: 5.72s\n",
      "116:\tlearn: 0.2493987\ttotal: 2.35s\tremaining: 5.69s\n",
      "117:\tlearn: 0.2470901\ttotal: 2.37s\tremaining: 5.67s\n",
      "118:\tlearn: 0.2436367\ttotal: 2.39s\tremaining: 5.64s\n",
      "119:\tlearn: 0.2412490\ttotal: 2.41s\tremaining: 5.62s\n",
      "120:\tlearn: 0.2382601\ttotal: 2.43s\tremaining: 5.6s\n",
      "121:\tlearn: 0.2358525\ttotal: 2.45s\tremaining: 5.57s\n",
      "122:\tlearn: 0.2335635\ttotal: 2.46s\tremaining: 5.55s\n",
      "123:\tlearn: 0.2313625\ttotal: 2.48s\tremaining: 5.53s\n",
      "124:\tlearn: 0.2298685\ttotal: 2.5s\tremaining: 5.5s\n",
      "125:\tlearn: 0.2276978\ttotal: 2.52s\tremaining: 5.48s\n",
      "126:\tlearn: 0.2262614\ttotal: 2.54s\tremaining: 5.45s\n",
      "127:\tlearn: 0.2237244\ttotal: 2.55s\tremaining: 5.43s\n",
      "128:\tlearn: 0.2216352\ttotal: 2.57s\tremaining: 5.41s\n",
      "129:\tlearn: 0.2196897\ttotal: 2.59s\tremaining: 5.38s\n",
      "130:\tlearn: 0.2182188\ttotal: 2.61s\tremaining: 5.36s\n",
      "131:\tlearn: 0.2159438\ttotal: 2.63s\tremaining: 5.33s\n",
      "132:\tlearn: 0.2135043\ttotal: 2.65s\tremaining: 5.31s\n",
      "133:\tlearn: 0.2115320\ttotal: 2.67s\tremaining: 5.3s\n",
      "134:\tlearn: 0.2094532\ttotal: 2.69s\tremaining: 5.27s\n",
      "135:\tlearn: 0.2079651\ttotal: 2.71s\tremaining: 5.25s\n",
      "136:\tlearn: 0.2062597\ttotal: 2.72s\tremaining: 5.23s\n",
      "137:\tlearn: 0.2040880\ttotal: 2.74s\tremaining: 5.21s\n",
      "138:\tlearn: 0.2021655\ttotal: 2.76s\tremaining: 5.18s\n",
      "139:\tlearn: 0.2006026\ttotal: 2.77s\tremaining: 5.16s\n",
      "140:\tlearn: 0.1985812\ttotal: 2.79s\tremaining: 5.13s\n",
      "141:\tlearn: 0.1965836\ttotal: 2.81s\tremaining: 5.11s\n",
      "142:\tlearn: 0.1948308\ttotal: 2.83s\tremaining: 5.08s\n",
      "143:\tlearn: 0.1927285\ttotal: 2.85s\tremaining: 5.07s\n",
      "144:\tlearn: 0.1907884\ttotal: 2.87s\tremaining: 5.05s\n",
      "145:\tlearn: 0.1892524\ttotal: 2.89s\tremaining: 5.03s\n",
      "146:\tlearn: 0.1876610\ttotal: 2.91s\tremaining: 5.01s\n",
      "147:\tlearn: 0.1858838\ttotal: 2.93s\tremaining: 4.99s\n",
      "148:\tlearn: 0.1840987\ttotal: 2.95s\tremaining: 4.96s\n",
      "149:\tlearn: 0.1826783\ttotal: 2.96s\tremaining: 4.94s\n",
      "150:\tlearn: 0.1809587\ttotal: 2.98s\tremaining: 4.92s\n",
      "151:\tlearn: 0.1793601\ttotal: 3s\tremaining: 4.9s\n",
      "152:\tlearn: 0.1783546\ttotal: 3.02s\tremaining: 4.87s\n",
      "153:\tlearn: 0.1774691\ttotal: 3.04s\tremaining: 4.85s\n",
      "154:\tlearn: 0.1758001\ttotal: 3.05s\tremaining: 4.83s\n",
      "155:\tlearn: 0.1744380\ttotal: 3.07s\tremaining: 4.81s\n",
      "156:\tlearn: 0.1731060\ttotal: 3.09s\tremaining: 4.79s\n",
      "157:\tlearn: 0.1715699\ttotal: 3.11s\tremaining: 4.77s\n",
      "158:\tlearn: 0.1696787\ttotal: 3.13s\tremaining: 4.75s\n",
      "159:\tlearn: 0.1682388\ttotal: 3.15s\tremaining: 4.73s\n",
      "160:\tlearn: 0.1673331\ttotal: 3.17s\tremaining: 4.7s\n",
      "161:\tlearn: 0.1659585\ttotal: 3.19s\tremaining: 4.68s\n",
      "162:\tlearn: 0.1643622\ttotal: 3.2s\tremaining: 4.66s\n",
      "163:\tlearn: 0.1636660\ttotal: 3.22s\tremaining: 4.63s\n",
      "164:\tlearn: 0.1621060\ttotal: 3.24s\tremaining: 4.61s\n",
      "165:\tlearn: 0.1607724\ttotal: 3.26s\tremaining: 4.59s\n",
      "166:\tlearn: 0.1594912\ttotal: 3.27s\tremaining: 4.57s\n",
      "167:\tlearn: 0.1576651\ttotal: 3.3s\tremaining: 4.55s\n",
      "168:\tlearn: 0.1567447\ttotal: 3.31s\tremaining: 4.53s\n",
      "169:\tlearn: 0.1552850\ttotal: 3.33s\tremaining: 4.51s\n",
      "170:\tlearn: 0.1543368\ttotal: 3.35s\tremaining: 4.49s\n",
      "171:\tlearn: 0.1526851\ttotal: 3.37s\tremaining: 4.46s\n",
      "172:\tlearn: 0.1516548\ttotal: 3.39s\tremaining: 4.44s\n",
      "173:\tlearn: 0.1503451\ttotal: 3.4s\tremaining: 4.42s\n",
      "174:\tlearn: 0.1489448\ttotal: 3.42s\tremaining: 4.4s\n",
      "175:\tlearn: 0.1479130\ttotal: 3.44s\tremaining: 4.38s\n",
      "176:\tlearn: 0.1468269\ttotal: 3.46s\tremaining: 4.35s\n",
      "177:\tlearn: 0.1457061\ttotal: 3.48s\tremaining: 4.33s\n",
      "178:\tlearn: 0.1447622\ttotal: 3.49s\tremaining: 4.31s\n",
      "179:\tlearn: 0.1438747\ttotal: 3.51s\tremaining: 4.29s\n",
      "180:\tlearn: 0.1430759\ttotal: 3.53s\tremaining: 4.27s\n",
      "181:\tlearn: 0.1418740\ttotal: 3.55s\tremaining: 4.25s\n",
      "182:\tlearn: 0.1406074\ttotal: 3.57s\tremaining: 4.23s\n",
      "183:\tlearn: 0.1393027\ttotal: 3.59s\tremaining: 4.21s\n",
      "184:\tlearn: 0.1384271\ttotal: 3.61s\tremaining: 4.19s\n",
      "185:\tlearn: 0.1379717\ttotal: 3.62s\tremaining: 4.17s\n",
      "186:\tlearn: 0.1372064\ttotal: 3.64s\tremaining: 4.15s\n",
      "187:\tlearn: 0.1363515\ttotal: 3.66s\tremaining: 4.13s\n",
      "188:\tlearn: 0.1353696\ttotal: 3.68s\tremaining: 4.1s\n",
      "189:\tlearn: 0.1345634\ttotal: 3.69s\tremaining: 4.08s\n",
      "190:\tlearn: 0.1334954\ttotal: 3.71s\tremaining: 4.06s\n",
      "191:\tlearn: 0.1325677\ttotal: 3.73s\tremaining: 4.04s\n",
      "192:\tlearn: 0.1315723\ttotal: 3.75s\tremaining: 4.02s\n",
      "193:\tlearn: 0.1309626\ttotal: 3.76s\tremaining: 4s\n",
      "194:\tlearn: 0.1300535\ttotal: 3.78s\tremaining: 3.98s\n",
      "195:\tlearn: 0.1288905\ttotal: 3.8s\tremaining: 3.96s\n",
      "196:\tlearn: 0.1277924\ttotal: 3.82s\tremaining: 3.94s\n",
      "197:\tlearn: 0.1265409\ttotal: 3.84s\tremaining: 3.92s\n",
      "198:\tlearn: 0.1255739\ttotal: 3.86s\tremaining: 3.9s\n",
      "199:\tlearn: 0.1248684\ttotal: 3.88s\tremaining: 3.88s\n",
      "200:\tlearn: 0.1241837\ttotal: 3.89s\tremaining: 3.86s\n",
      "201:\tlearn: 0.1232042\ttotal: 3.91s\tremaining: 3.83s\n",
      "202:\tlearn: 0.1222603\ttotal: 3.93s\tremaining: 3.81s\n",
      "203:\tlearn: 0.1213459\ttotal: 3.95s\tremaining: 3.79s\n",
      "204:\tlearn: 0.1205817\ttotal: 3.97s\tremaining: 3.78s\n",
      "205:\tlearn: 0.1198251\ttotal: 3.99s\tremaining: 3.76s\n",
      "206:\tlearn: 0.1189689\ttotal: 4.01s\tremaining: 3.74s\n",
      "207:\tlearn: 0.1181024\ttotal: 4.03s\tremaining: 3.72s\n",
      "208:\tlearn: 0.1172190\ttotal: 4.04s\tremaining: 3.69s\n",
      "209:\tlearn: 0.1166187\ttotal: 4.06s\tremaining: 3.67s\n",
      "210:\tlearn: 0.1158746\ttotal: 4.08s\tremaining: 3.65s\n",
      "211:\tlearn: 0.1153167\ttotal: 4.1s\tremaining: 3.63s\n",
      "212:\tlearn: 0.1148056\ttotal: 4.11s\tremaining: 3.61s\n",
      "213:\tlearn: 0.1139654\ttotal: 4.13s\tremaining: 3.59s\n",
      "214:\tlearn: 0.1133325\ttotal: 4.15s\tremaining: 3.57s\n",
      "215:\tlearn: 0.1127448\ttotal: 4.17s\tremaining: 3.55s\n",
      "216:\tlearn: 0.1118552\ttotal: 4.19s\tremaining: 3.53s\n",
      "217:\tlearn: 0.1109735\ttotal: 4.21s\tremaining: 3.51s\n",
      "218:\tlearn: 0.1101663\ttotal: 4.23s\tremaining: 3.49s\n",
      "219:\tlearn: 0.1094057\ttotal: 4.25s\tremaining: 3.47s\n",
      "220:\tlearn: 0.1086842\ttotal: 4.27s\tremaining: 3.46s\n",
      "221:\tlearn: 0.1081635\ttotal: 4.29s\tremaining: 3.44s\n",
      "222:\tlearn: 0.1074615\ttotal: 4.3s\tremaining: 3.42s\n",
      "223:\tlearn: 0.1067836\ttotal: 4.32s\tremaining: 3.4s\n",
      "224:\tlearn: 0.1061558\ttotal: 4.34s\tremaining: 3.38s\n",
      "225:\tlearn: 0.1054296\ttotal: 4.36s\tremaining: 3.36s\n",
      "226:\tlearn: 0.1047350\ttotal: 4.38s\tremaining: 3.34s\n",
      "227:\tlearn: 0.1040358\ttotal: 4.4s\tremaining: 3.32s\n",
      "228:\tlearn: 0.1033799\ttotal: 4.41s\tremaining: 3.3s\n",
      "229:\tlearn: 0.1028751\ttotal: 4.43s\tremaining: 3.28s\n",
      "230:\tlearn: 0.1021686\ttotal: 4.45s\tremaining: 3.26s\n",
      "231:\tlearn: 0.1014326\ttotal: 4.47s\tremaining: 3.23s\n",
      "232:\tlearn: 0.1009049\ttotal: 4.49s\tremaining: 3.21s\n",
      "233:\tlearn: 0.1003008\ttotal: 4.5s\tremaining: 3.19s\n",
      "234:\tlearn: 0.0999129\ttotal: 4.52s\tremaining: 3.17s\n",
      "235:\tlearn: 0.0993210\ttotal: 4.54s\tremaining: 3.15s\n",
      "236:\tlearn: 0.0988379\ttotal: 4.56s\tremaining: 3.13s\n",
      "237:\tlearn: 0.0982119\ttotal: 4.58s\tremaining: 3.11s\n",
      "238:\tlearn: 0.0976786\ttotal: 4.59s\tremaining: 3.1s\n",
      "239:\tlearn: 0.0970612\ttotal: 4.61s\tremaining: 3.08s\n",
      "240:\tlearn: 0.0966120\ttotal: 4.63s\tremaining: 3.06s\n",
      "241:\tlearn: 0.0959420\ttotal: 4.65s\tremaining: 3.04s\n",
      "242:\tlearn: 0.0953684\ttotal: 4.67s\tremaining: 3.02s\n",
      "243:\tlearn: 0.0947231\ttotal: 4.69s\tremaining: 3s\n",
      "244:\tlearn: 0.0942221\ttotal: 4.7s\tremaining: 2.98s\n",
      "245:\tlearn: 0.0936252\ttotal: 4.72s\tremaining: 2.96s\n",
      "246:\tlearn: 0.0930829\ttotal: 4.74s\tremaining: 2.94s\n",
      "247:\tlearn: 0.0926261\ttotal: 4.76s\tremaining: 2.92s\n",
      "248:\tlearn: 0.0920309\ttotal: 4.78s\tremaining: 2.9s\n",
      "249:\tlearn: 0.0913719\ttotal: 4.79s\tremaining: 2.88s\n",
      "250:\tlearn: 0.0908123\ttotal: 4.81s\tremaining: 2.86s\n",
      "251:\tlearn: 0.0901615\ttotal: 4.83s\tremaining: 2.84s\n",
      "252:\tlearn: 0.0895904\ttotal: 4.85s\tremaining: 2.82s\n",
      "253:\tlearn: 0.0889963\ttotal: 4.87s\tremaining: 2.8s\n",
      "254:\tlearn: 0.0884139\ttotal: 4.89s\tremaining: 2.78s\n",
      "255:\tlearn: 0.0880217\ttotal: 4.91s\tremaining: 2.76s\n",
      "256:\tlearn: 0.0875964\ttotal: 4.92s\tremaining: 2.74s\n",
      "257:\tlearn: 0.0870218\ttotal: 4.94s\tremaining: 2.72s\n",
      "258:\tlearn: 0.0865999\ttotal: 4.96s\tremaining: 2.7s\n",
      "259:\tlearn: 0.0860540\ttotal: 4.98s\tremaining: 2.68s\n",
      "260:\tlearn: 0.0853690\ttotal: 5s\tremaining: 2.66s\n",
      "261:\tlearn: 0.0847425\ttotal: 5.02s\tremaining: 2.64s\n",
      "262:\tlearn: 0.0841182\ttotal: 5.03s\tremaining: 2.62s\n",
      "263:\tlearn: 0.0835568\ttotal: 5.05s\tremaining: 2.6s\n",
      "264:\tlearn: 0.0830860\ttotal: 5.08s\tremaining: 2.58s\n",
      "265:\tlearn: 0.0826953\ttotal: 5.09s\tremaining: 2.56s\n",
      "266:\tlearn: 0.0821504\ttotal: 5.11s\tremaining: 2.54s\n",
      "267:\tlearn: 0.0816057\ttotal: 5.13s\tremaining: 2.53s\n",
      "268:\tlearn: 0.0811668\ttotal: 5.15s\tremaining: 2.51s\n",
      "269:\tlearn: 0.0806370\ttotal: 5.17s\tremaining: 2.49s\n",
      "270:\tlearn: 0.0803147\ttotal: 5.18s\tremaining: 2.47s\n",
      "271:\tlearn: 0.0799147\ttotal: 5.2s\tremaining: 2.45s\n",
      "272:\tlearn: 0.0796684\ttotal: 5.22s\tremaining: 2.43s\n",
      "273:\tlearn: 0.0791950\ttotal: 5.24s\tremaining: 2.41s\n",
      "274:\tlearn: 0.0787248\ttotal: 5.26s\tremaining: 2.39s\n",
      "275:\tlearn: 0.0782419\ttotal: 5.28s\tremaining: 2.37s\n",
      "276:\tlearn: 0.0780074\ttotal: 5.29s\tremaining: 2.35s\n",
      "277:\tlearn: 0.0775958\ttotal: 5.31s\tremaining: 2.33s\n",
      "278:\tlearn: 0.0771180\ttotal: 5.33s\tremaining: 2.31s\n",
      "279:\tlearn: 0.0767341\ttotal: 5.35s\tremaining: 2.29s\n",
      "280:\tlearn: 0.0763011\ttotal: 5.37s\tremaining: 2.27s\n",
      "281:\tlearn: 0.0758194\ttotal: 5.38s\tremaining: 2.25s\n",
      "282:\tlearn: 0.0754358\ttotal: 5.4s\tremaining: 2.23s\n",
      "283:\tlearn: 0.0750667\ttotal: 5.42s\tremaining: 2.21s\n",
      "284:\tlearn: 0.0747107\ttotal: 5.44s\tremaining: 2.19s\n",
      "285:\tlearn: 0.0742531\ttotal: 5.46s\tremaining: 2.17s\n",
      "286:\tlearn: 0.0737926\ttotal: 5.48s\tremaining: 2.16s\n",
      "287:\tlearn: 0.0735905\ttotal: 5.49s\tremaining: 2.14s\n",
      "288:\tlearn: 0.0732208\ttotal: 5.51s\tremaining: 2.12s\n",
      "289:\tlearn: 0.0727946\ttotal: 5.53s\tremaining: 2.1s\n",
      "290:\tlearn: 0.0724306\ttotal: 5.55s\tremaining: 2.08s\n",
      "291:\tlearn: 0.0720317\ttotal: 5.57s\tremaining: 2.06s\n",
      "292:\tlearn: 0.0716278\ttotal: 5.58s\tremaining: 2.04s\n",
      "293:\tlearn: 0.0712362\ttotal: 5.6s\tremaining: 2.02s\n",
      "294:\tlearn: 0.0707410\ttotal: 5.62s\tremaining: 2s\n",
      "295:\tlearn: 0.0704118\ttotal: 5.64s\tremaining: 1.98s\n",
      "296:\tlearn: 0.0700204\ttotal: 5.66s\tremaining: 1.96s\n",
      "297:\tlearn: 0.0696531\ttotal: 5.68s\tremaining: 1.94s\n",
      "298:\tlearn: 0.0692932\ttotal: 5.7s\tremaining: 1.92s\n",
      "299:\tlearn: 0.0689641\ttotal: 5.72s\tremaining: 1.91s\n",
      "300:\tlearn: 0.0687036\ttotal: 5.74s\tremaining: 1.89s\n",
      "301:\tlearn: 0.0682465\ttotal: 5.75s\tremaining: 1.87s\n",
      "302:\tlearn: 0.0679109\ttotal: 5.77s\tremaining: 1.85s\n",
      "303:\tlearn: 0.0675415\ttotal: 5.79s\tremaining: 1.83s\n",
      "304:\tlearn: 0.0671175\ttotal: 5.81s\tremaining: 1.81s\n",
      "305:\tlearn: 0.0668401\ttotal: 5.83s\tremaining: 1.79s\n",
      "306:\tlearn: 0.0665337\ttotal: 5.84s\tremaining: 1.77s\n",
      "307:\tlearn: 0.0662038\ttotal: 5.86s\tremaining: 1.75s\n",
      "308:\tlearn: 0.0659299\ttotal: 5.88s\tremaining: 1.73s\n",
      "309:\tlearn: 0.0656950\ttotal: 5.9s\tremaining: 1.71s\n",
      "310:\tlearn: 0.0653329\ttotal: 5.92s\tremaining: 1.69s\n",
      "311:\tlearn: 0.0650142\ttotal: 5.93s\tremaining: 1.67s\n",
      "312:\tlearn: 0.0647410\ttotal: 5.95s\tremaining: 1.65s\n",
      "313:\tlearn: 0.0644334\ttotal: 5.97s\tremaining: 1.63s\n",
      "314:\tlearn: 0.0641182\ttotal: 5.99s\tremaining: 1.61s\n",
      "315:\tlearn: 0.0638325\ttotal: 6s\tremaining: 1.6s\n",
      "316:\tlearn: 0.0635122\ttotal: 6.02s\tremaining: 1.58s\n",
      "317:\tlearn: 0.0631894\ttotal: 6.04s\tremaining: 1.56s\n",
      "318:\tlearn: 0.0628921\ttotal: 6.06s\tremaining: 1.54s\n",
      "319:\tlearn: 0.0625070\ttotal: 6.08s\tremaining: 1.52s\n",
      "320:\tlearn: 0.0621223\ttotal: 6.1s\tremaining: 1.5s\n",
      "321:\tlearn: 0.0619032\ttotal: 6.11s\tremaining: 1.48s\n",
      "322:\tlearn: 0.0615653\ttotal: 6.13s\tremaining: 1.46s\n",
      "323:\tlearn: 0.0613404\ttotal: 6.15s\tremaining: 1.44s\n",
      "324:\tlearn: 0.0611619\ttotal: 6.17s\tremaining: 1.42s\n",
      "325:\tlearn: 0.0608453\ttotal: 6.19s\tremaining: 1.4s\n",
      "326:\tlearn: 0.0605806\ttotal: 6.2s\tremaining: 1.39s\n",
      "327:\tlearn: 0.0603465\ttotal: 6.22s\tremaining: 1.36s\n",
      "328:\tlearn: 0.0600413\ttotal: 6.24s\tremaining: 1.35s\n",
      "329:\tlearn: 0.0597975\ttotal: 6.26s\tremaining: 1.33s\n",
      "330:\tlearn: 0.0595617\ttotal: 6.28s\tremaining: 1.31s\n",
      "331:\tlearn: 0.0593571\ttotal: 6.29s\tremaining: 1.29s\n",
      "332:\tlearn: 0.0591896\ttotal: 6.31s\tremaining: 1.27s\n",
      "333:\tlearn: 0.0589029\ttotal: 6.33s\tremaining: 1.25s\n",
      "334:\tlearn: 0.0586641\ttotal: 6.35s\tremaining: 1.23s\n",
      "335:\tlearn: 0.0584050\ttotal: 6.37s\tremaining: 1.21s\n",
      "336:\tlearn: 0.0581356\ttotal: 6.38s\tremaining: 1.19s\n",
      "337:\tlearn: 0.0579161\ttotal: 6.4s\tremaining: 1.17s\n",
      "338:\tlearn: 0.0575914\ttotal: 6.42s\tremaining: 1.16s\n",
      "339:\tlearn: 0.0574563\ttotal: 6.44s\tremaining: 1.14s\n",
      "340:\tlearn: 0.0571344\ttotal: 6.46s\tremaining: 1.12s\n",
      "341:\tlearn: 0.0568670\ttotal: 6.47s\tremaining: 1.1s\n",
      "342:\tlearn: 0.0566196\ttotal: 6.49s\tremaining: 1.08s\n",
      "343:\tlearn: 0.0563139\ttotal: 6.51s\tremaining: 1.06s\n",
      "344:\tlearn: 0.0560689\ttotal: 6.53s\tremaining: 1.04s\n",
      "345:\tlearn: 0.0557970\ttotal: 6.55s\tremaining: 1.02s\n",
      "346:\tlearn: 0.0555744\ttotal: 6.57s\tremaining: 1s\n",
      "347:\tlearn: 0.0553663\ttotal: 6.58s\tremaining: 984ms\n",
      "348:\tlearn: 0.0550987\ttotal: 6.61s\tremaining: 965ms\n",
      "349:\tlearn: 0.0548580\ttotal: 6.63s\tremaining: 947ms\n",
      "350:\tlearn: 0.0546271\ttotal: 6.64s\tremaining: 928ms\n",
      "351:\tlearn: 0.0544627\ttotal: 6.66s\tremaining: 909ms\n",
      "352:\tlearn: 0.0542735\ttotal: 6.68s\tremaining: 890ms\n",
      "353:\tlearn: 0.0540566\ttotal: 6.7s\tremaining: 871ms\n",
      "354:\tlearn: 0.0537670\ttotal: 6.72s\tremaining: 852ms\n",
      "355:\tlearn: 0.0534931\ttotal: 6.74s\tremaining: 833ms\n",
      "356:\tlearn: 0.0532458\ttotal: 6.76s\tremaining: 814ms\n",
      "357:\tlearn: 0.0530554\ttotal: 6.78s\tremaining: 795ms\n",
      "358:\tlearn: 0.0528663\ttotal: 6.8s\tremaining: 777ms\n",
      "359:\tlearn: 0.0525924\ttotal: 6.82s\tremaining: 757ms\n",
      "360:\tlearn: 0.0523859\ttotal: 6.83s\tremaining: 738ms\n",
      "361:\tlearn: 0.0521683\ttotal: 6.85s\tremaining: 719ms\n",
      "362:\tlearn: 0.0519234\ttotal: 6.87s\tremaining: 700ms\n",
      "363:\tlearn: 0.0516844\ttotal: 6.89s\tremaining: 681ms\n",
      "364:\tlearn: 0.0515064\ttotal: 6.9s\tremaining: 662ms\n",
      "365:\tlearn: 0.0512855\ttotal: 6.92s\tremaining: 643ms\n",
      "366:\tlearn: 0.0511138\ttotal: 6.94s\tremaining: 624ms\n",
      "367:\tlearn: 0.0508781\ttotal: 6.96s\tremaining: 605ms\n",
      "368:\tlearn: 0.0506872\ttotal: 6.97s\tremaining: 586ms\n",
      "369:\tlearn: 0.0504707\ttotal: 6.99s\tremaining: 567ms\n",
      "370:\tlearn: 0.0502115\ttotal: 7.01s\tremaining: 548ms\n",
      "371:\tlearn: 0.0500338\ttotal: 7.03s\tremaining: 529ms\n",
      "372:\tlearn: 0.0498045\ttotal: 7.04s\tremaining: 510ms\n",
      "373:\tlearn: 0.0495770\ttotal: 7.07s\tremaining: 491ms\n",
      "374:\tlearn: 0.0494049\ttotal: 7.08s\tremaining: 472ms\n",
      "375:\tlearn: 0.0491638\ttotal: 7.11s\tremaining: 454ms\n",
      "376:\tlearn: 0.0489267\ttotal: 7.12s\tremaining: 435ms\n",
      "377:\tlearn: 0.0487657\ttotal: 7.14s\tremaining: 416ms\n",
      "378:\tlearn: 0.0485915\ttotal: 7.16s\tremaining: 397ms\n",
      "379:\tlearn: 0.0484395\ttotal: 7.18s\tremaining: 378ms\n",
      "380:\tlearn: 0.0482877\ttotal: 7.2s\tremaining: 359ms\n",
      "381:\tlearn: 0.0480858\ttotal: 7.21s\tremaining: 340ms\n",
      "382:\tlearn: 0.0478687\ttotal: 7.24s\tremaining: 321ms\n",
      "383:\tlearn: 0.0476853\ttotal: 7.25s\tremaining: 302ms\n",
      "384:\tlearn: 0.0474993\ttotal: 7.27s\tremaining: 283ms\n",
      "385:\tlearn: 0.0472943\ttotal: 7.29s\tremaining: 264ms\n",
      "386:\tlearn: 0.0470829\ttotal: 7.31s\tremaining: 246ms\n",
      "387:\tlearn: 0.0468742\ttotal: 7.33s\tremaining: 227ms\n",
      "388:\tlearn: 0.0466857\ttotal: 7.35s\tremaining: 208ms\n",
      "389:\tlearn: 0.0465460\ttotal: 7.37s\tremaining: 189ms\n",
      "390:\tlearn: 0.0463967\ttotal: 7.38s\tremaining: 170ms\n",
      "391:\tlearn: 0.0462451\ttotal: 7.4s\tremaining: 151ms\n",
      "392:\tlearn: 0.0460232\ttotal: 7.42s\tremaining: 132ms\n",
      "393:\tlearn: 0.0458569\ttotal: 7.44s\tremaining: 113ms\n",
      "394:\tlearn: 0.0455861\ttotal: 7.46s\tremaining: 94.4ms\n",
      "395:\tlearn: 0.0454279\ttotal: 7.47s\tremaining: 75.5ms\n",
      "396:\tlearn: 0.0452607\ttotal: 7.49s\tremaining: 56.6ms\n",
      "397:\tlearn: 0.0450495\ttotal: 7.51s\tremaining: 37.7ms\n",
      "398:\tlearn: 0.0449415\ttotal: 7.53s\tremaining: 18.9ms\n",
      "399:\tlearn: 0.0447709\ttotal: 7.55s\tremaining: 0us\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.61      0.63        44\n",
      "           1       0.56      0.60      0.58        30\n",
      "           2       0.64      0.52      0.57        31\n",
      "           3       0.65      0.63      0.64        27\n",
      "           4       0.57      0.74      0.65        31\n",
      "           5       0.64      0.61      0.62        49\n",
      "\n",
      "    accuracy                           0.62       212\n",
      "   macro avg       0.62      0.62      0.62       212\n",
      "weighted avg       0.62      0.62      0.62       212\n"
     ]
    }
   ],
   "source": [
    "import catboost as cb\n",
    "\n",
    "model = cb.CatBoostClassifier(iterations=500, task_type='GPU')\n",
    "model.fit(X_train, y_train)\n",
    "# Generate classification report\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T00:01:44.601009500Z",
     "start_time": "2024-02-22T00:01:36.225891400Z"
    }
   },
   "id": "c14f291177ba9793",
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "source": [
    "Very interesting to note is that the Catboost model gave the lowest accuracy of 0.62,\n",
    "which is lower than the other models we have tried so far, \n",
    "meaning that the more complex models do not always outperform the simpler ones."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66b66ab411e286e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network\n",
    "\n",
    "While neural networks need a lot of data to perform well, let's try a simple neural network model to see how it performs.\n",
    "\n",
    "We will use the `keras` library to build the neural network model.\n",
    "\n",
    "Let's start by making the train and test splits from above into TensorFlow datasets.\n",
    "\n",
    "We will also define a validation dataset from the training data to monitor the model's performance during training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be15e3c101722f61"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 600  # Adjust based on your dataset size\n",
    "VALIDATION_SPLIT = 0.2  # Fraction of training data to use for validation\n",
    "\n",
    "# Calculate the number of validation samples\n",
    "n_validation_samples = int(len(X_train) * VALIDATION_SPLIT)\n",
    "\n",
    "# Create and process the validation dataset\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_train[:n_validation_samples], y_train[:n_validation_samples]))\n",
    "val_ds = val_ds.batch(BATCH_SIZE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Update the training dataset to exclude the validation data and process it\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train[n_validation_samples:], y_train[n_validation_samples:]))\n",
    "train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Create and process the test dataset\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_ds = test_ds.batch(BATCH_SIZE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T18:14:23.169981800Z",
     "start_time": "2024-02-23T18:14:17.071881900Z"
    }
   },
   "id": "fac050077fcb120",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that done, let's have a look at the first batch of the training dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1bc0a932557566"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [[ 0.00471497 -0.00479507 -0.01774597 ...  0.03649902 -0.06439209\n",
      "   0.01343536]\n",
      " [-0.02244568 -0.00348091 -0.03799438 ...  0.01768494 -0.03353882\n",
      "   0.00830078]\n",
      " [ 0.0713501  -0.02305603 -0.02178955 ...  0.03433228 -0.02548218\n",
      "  -0.00827026]\n",
      " ...\n",
      " [ 0.02381897  0.0345459  -0.04190063 ...  0.04336548 -0.03881836\n",
      "  -0.02015686]\n",
      " [ 0.02702332 -0.01013947 -0.02232361 ...  0.04812622 -0.07067871\n",
      "   0.01161194]\n",
      " [-0.01200104 -0.00301361 -0.03564453 ... -0.00743866 -0.04656982\n",
      "  -0.04006958]], Target: [1 1 3 5 3 5 4 4 0 3 4 4 4 4 4 2 4 1 1 1 5 4 3 4 4 2 1 2 0 3 0 5]\n"
     ]
    }
   ],
   "source": [
    "for feat, targ in train_ds.take(1):\n",
    "    print('Features: {}, Target: {}'.format(feat, targ))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T18:14:23.236576500Z",
     "start_time": "2024-02-23T18:14:23.169981800Z"
    }
   },
   "id": "28b6cd6e52c06ff9",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the training dataset is structured as expected, with the features and the targets batched together with a size of 32."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2200f63d62e00072"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's define an early stopping callback to stop training the model when the validation loss does not improve after a certain number of epochs. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55faaf898903bf36"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    patience=30,  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T18:14:23.291233700Z",
     "start_time": "2024-02-23T18:14:23.231576500Z"
    }
   },
   "id": "a2a85456f6f0206a",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another useful callback method in TensorFlow is the ModelCheckpoint callback. This callback saves the model at regular intervals during training, allowing you to retain and load the best version of your model based on a certain monitored metric, such as validation loss or accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58e3c995a8f8d943"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model_{epoch:02d}-{val_loss:.2f}.keras',  # Change the extension to '.keras'\n",
    "    save_weights_only=False,  # Set to True to save only weights, False to save the entire model\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    mode='min',  # The model is saved when the monitored metric stops decreasing\n",
    "    save_best_only=True,  # Only the best model is saved\n",
    "    verbose=1  # Verbosity mode, 1 or 0\n",
    ")\n",
    "\n",
    "CALLBACKS = [\n",
    "    tfdocs.modeling.EpochDots(),\n",
    "    early_stopping_callback,\n",
    "    model_checkpoint_callback\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T18:14:23.500459900Z",
     "start_time": "2024-02-23T18:14:23.292234Z"
    }
   },
   "id": "e99e3b0058d27c4b",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly, let's define a scheduler callback to adjust the learning rate during training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e70949e4096c0a1f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "STEPS_PER_EPOCH = X_train.shape[0] // BATCH_SIZE\n",
    "lr_scheduler = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    initial_learning_rate=1e-3,  # Initial learning rate\n",
    "    decay_steps=STEPS_PER_EPOCH * 1000,  # Decay steps\n",
    "    decay_rate=0.9,  # Decay rate\n",
    "    staircase=True  # If True, decay the learning rate at discrete intervals\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T18:14:23.507317100Z",
     "start_time": "2024-02-23T18:14:23.501460600Z"
    }
   },
   "id": "b24cce175aac2630",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "With all the callbacks and the learning rate scheduler defined, let's now build the neural network model.\n",
    "\n",
    "We will use a simple neural network with two hidden layers, each with 64 units, and a ReLU activation function.\n",
    "\n",
    "Also, we will use the Adam optimizer and the SparseCategoricalCrossentropy loss function, as well as the accuracy metric.\n",
    "\n",
    "Lastly, we will add WeightRegularization to the model as well as DropoutLayers to prevent overfitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "396c45887ea4ddcd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import keras \n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "from keras import regularizers\n",
    "\n",
    "weight_regularizer = regularizers.l2(0.0001)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=weight_regularizer),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(16, activation='relu', kernel_regularizer=weight_regularizer),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(8, activation='relu', kernel_regularizer=weight_regularizer),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(6, activation='softmax')\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T18:16:10.715661200Z",
     "start_time": "2024-02-23T18:16:10.681441600Z"
    }
   },
   "id": "ed010b4c406e22d7",
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the model created, let's compile it using the Adam optimizer, the SparseCategoricalCrossentropy loss function, and the accuracy metric."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9600efc4cd8cdafb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=(['accuracy',\n",
    "                        tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                            from_logits=False, name='sparse_categorical_crossentropy')]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T18:16:10.943114400Z",
     "start_time": "2024-02-23T18:16:10.937256100Z"
    }
   },
   "id": "3dc26bb0fade8741",
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see now the model summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d19eb4b55cb1da4d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 32)                24608     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 6)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25326 (98.93 KB)\n",
      "Trainable params: 25326 (98.93 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T18:16:11.257051600Z",
     "start_time": "2024-02-23T18:16:11.232573700Z"
    }
   },
   "id": "7794956153aea89d",
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the model has 25326 parameters to train, which are quite a lot for the small dataset we have.\n",
    "However, with the Dropout layers and the WeightRegularization, we hope to prevent excessive overfitting.\n",
    "\n",
    "Let's now train the model using the training dataset and the validation dataset we created earlier."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "689f9ac49e7e4324"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1/22 [>.............................] - ETA: 9s - loss: 1.8004 - accuracy: 0.2188 - sparse_categorical_crossentropy: 1.7912\n",
      "Epoch: 0, accuracy:0.1879,  loss:1.7991,  sparse_categorical_crossentropy:1.7900,  val_accuracy:0.1538,  val_loss:1.7973,  val_sparse_categorical_crossentropy:1.7876,  \n",
      ".\n",
      "Epoch 1: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 1s 5ms/step - loss: 1.7991 - accuracy: 0.1879 - sparse_categorical_crossentropy: 1.7900 - val_loss: 1.7973 - val_accuracy: 0.1538 - val_sparse_categorical_crossentropy: 1.7876\n",
      "Epoch 2/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7836 - accuracy: 0.2500 - sparse_categorical_crossentropy: 1.7748.\n",
      "Epoch 2: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.7898 - accuracy: 0.2041 - sparse_categorical_crossentropy: 1.7827 - val_loss: 1.7882 - val_accuracy: 0.2367 - val_sparse_categorical_crossentropy: 1.7772\n",
      "Epoch 3/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7789 - accuracy: 0.3125 - sparse_categorical_crossentropy: 1.7704.\n",
      "Epoch 3: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.7759 - accuracy: 0.2530 - sparse_categorical_crossentropy: 1.7644 - val_loss: 1.7743 - val_accuracy: 0.2840 - val_sparse_categorical_crossentropy: 1.7614\n",
      "Epoch 4/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7574 - accuracy: 0.3438 - sparse_categorical_crossentropy: 1.7492.\n",
      "Epoch 4: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.7515 - accuracy: 0.2944 - sparse_categorical_crossentropy: 1.7431 - val_loss: 1.7548 - val_accuracy: 0.1953 - val_sparse_categorical_crossentropy: 1.7400\n",
      "Epoch 5/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7759 - accuracy: 0.2500 - sparse_categorical_crossentropy: 1.7677.\n",
      "Epoch 5: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.7311 - accuracy: 0.2899 - sparse_categorical_crossentropy: 1.7208 - val_loss: 1.7363 - val_accuracy: 0.2367 - val_sparse_categorical_crossentropy: 1.7199\n",
      "Epoch 6/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7087 - accuracy: 0.3750 - sparse_categorical_crossentropy: 1.7006.\n",
      "Epoch 6: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.7011 - accuracy: 0.3107 - sparse_categorical_crossentropy: 1.6877 - val_loss: 1.7108 - val_accuracy: 0.2781 - val_sparse_categorical_crossentropy: 1.6912\n",
      "Epoch 7/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7011 - accuracy: 0.2500 - sparse_categorical_crossentropy: 1.6930.\n",
      "Epoch 7: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.6753 - accuracy: 0.3269 - sparse_categorical_crossentropy: 1.6629 - val_loss: 1.6842 - val_accuracy: 0.3254 - val_sparse_categorical_crossentropy: 1.6608\n",
      "Epoch 8/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.5973 - accuracy: 0.3750 - sparse_categorical_crossentropy: 1.5890.\n",
      "Epoch 8: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.6345 - accuracy: 0.3314 - sparse_categorical_crossentropy: 1.6361 - val_loss: 1.6602 - val_accuracy: 0.3136 - val_sparse_categorical_crossentropy: 1.6314\n",
      "Epoch 9/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.6391 - accuracy: 0.3125 - sparse_categorical_crossentropy: 1.6307.\n",
      "Epoch 9: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5923 - accuracy: 0.3624 - sparse_categorical_crossentropy: 1.5791 - val_loss: 1.6247 - val_accuracy: 0.3254 - val_sparse_categorical_crossentropy: 1.5946\n",
      "Epoch 10/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.5239 - accuracy: 0.3438 - sparse_categorical_crossentropy: 1.5153.\n",
      "Epoch 10: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5709 - accuracy: 0.3550 - sparse_categorical_crossentropy: 1.5563 - val_loss: 1.6013 - val_accuracy: 0.3432 - val_sparse_categorical_crossentropy: 1.5698\n",
      "Epoch 11/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.4068 - accuracy: 0.4688 - sparse_categorical_crossentropy: 1.3979.\n",
      "Epoch 11: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5436 - accuracy: 0.3920 - sparse_categorical_crossentropy: 1.5243 - val_loss: 1.6006 - val_accuracy: 0.3077 - val_sparse_categorical_crossentropy: 1.5587\n",
      "Epoch 12/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.5089 - accuracy: 0.4062 - sparse_categorical_crossentropy: 1.4999.\n",
      "Epoch 12: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5291 - accuracy: 0.3831 - sparse_categorical_crossentropy: 1.5297 - val_loss: 1.5699 - val_accuracy: 0.3254 - val_sparse_categorical_crossentropy: 1.5272\n",
      "Epoch 13/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.5174 - accuracy: 0.2812 - sparse_categorical_crossentropy: 1.5081.\n",
      "Epoch 13: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5169 - accuracy: 0.3905 - sparse_categorical_crossentropy: 1.5111 - val_loss: 1.5560 - val_accuracy: 0.3787 - val_sparse_categorical_crossentropy: 1.5174\n",
      "Epoch 14/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.3875 - accuracy: 0.5312 - sparse_categorical_crossentropy: 1.3779.\n",
      "Epoch 14: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4768 - accuracy: 0.4201 - sparse_categorical_crossentropy: 1.4716 - val_loss: 1.5373 - val_accuracy: 0.3728 - val_sparse_categorical_crossentropy: 1.4948\n",
      "Epoch 15/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.4499 - accuracy: 0.4375 - sparse_categorical_crossentropy: 1.4401.\n",
      "Epoch 15: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4800 - accuracy: 0.4038 - sparse_categorical_crossentropy: 1.4730 - val_loss: 1.5242 - val_accuracy: 0.3669 - val_sparse_categorical_crossentropy: 1.4795\n",
      "Epoch 16/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.4798 - accuracy: 0.3750 - sparse_categorical_crossentropy: 1.4698.\n",
      "Epoch 16: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4316 - accuracy: 0.4497 - sparse_categorical_crossentropy: 1.4452 - val_loss: 1.5043 - val_accuracy: 0.4024 - val_sparse_categorical_crossentropy: 1.4617\n",
      "Epoch 17/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.4056 - accuracy: 0.4688 - sparse_categorical_crossentropy: 1.3953.\n",
      "Epoch 17: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4363 - accuracy: 0.4216 - sparse_categorical_crossentropy: 1.4296 - val_loss: 1.4975 - val_accuracy: 0.4260 - val_sparse_categorical_crossentropy: 1.4566\n",
      "Epoch 18/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.3777 - accuracy: 0.4375 - sparse_categorical_crossentropy: 1.3671.\n",
      "Epoch 18: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3943 - accuracy: 0.4689 - sparse_categorical_crossentropy: 1.3784 - val_loss: 1.4901 - val_accuracy: 0.4142 - val_sparse_categorical_crossentropy: 1.4452\n",
      "Epoch 19/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.5015 - accuracy: 0.3750 - sparse_categorical_crossentropy: 1.4907.\n",
      "Epoch 19: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3650 - accuracy: 0.4896 - sparse_categorical_crossentropy: 1.3405 - val_loss: 1.4839 - val_accuracy: 0.3964 - val_sparse_categorical_crossentropy: 1.4335\n",
      "Epoch 20/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1725 - accuracy: 0.5938 - sparse_categorical_crossentropy: 1.1614.\n",
      "Epoch 20: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3516 - accuracy: 0.4896 - sparse_categorical_crossentropy: 1.3389 - val_loss: 1.4564 - val_accuracy: 0.4260 - val_sparse_categorical_crossentropy: 1.4061\n",
      "Epoch 21/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.3205 - accuracy: 0.5625 - sparse_categorical_crossentropy: 1.3091.\n",
      "Epoch 21: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3487 - accuracy: 0.4941 - sparse_categorical_crossentropy: 1.3407 - val_loss: 1.4415 - val_accuracy: 0.4556 - val_sparse_categorical_crossentropy: 1.3874\n",
      "Epoch 22/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1522 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.1406.\n",
      "Epoch 22: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3092 - accuracy: 0.4941 - sparse_categorical_crossentropy: 1.2943 - val_loss: 1.4460 - val_accuracy: 0.4379 - val_sparse_categorical_crossentropy: 1.3957\n",
      "Epoch 23/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1129 - accuracy: 0.6562 - sparse_categorical_crossentropy: 1.1009.\n",
      "Epoch 23: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.2836 - accuracy: 0.5104 - sparse_categorical_crossentropy: 1.2682 - val_loss: 1.4102 - val_accuracy: 0.4911 - val_sparse_categorical_crossentropy: 1.3564\n",
      "Epoch 24/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2195 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.2073.\n",
      "Epoch 24: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3014 - accuracy: 0.5044 - sparse_categorical_crossentropy: 1.2906 - val_loss: 1.4065 - val_accuracy: 0.4615 - val_sparse_categorical_crossentropy: 1.3500\n",
      "Epoch 25/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2099 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.1973.\n",
      "Epoch 25: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.2677 - accuracy: 0.4941 - sparse_categorical_crossentropy: 1.2490 - val_loss: 1.3990 - val_accuracy: 0.4852 - val_sparse_categorical_crossentropy: 1.3440\n",
      "Epoch 26/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0629 - accuracy: 0.5625 - sparse_categorical_crossentropy: 1.0500.\n",
      "Epoch 26: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.2476 - accuracy: 0.5429 - sparse_categorical_crossentropy: 1.2303 - val_loss: 1.3801 - val_accuracy: 0.4911 - val_sparse_categorical_crossentropy: 1.3163\n",
      "Epoch 27/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2265 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.2133.\n",
      "Epoch 27: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2132 - accuracy: 0.5533 - sparse_categorical_crossentropy: 1.2053 - val_loss: 1.3954 - val_accuracy: 0.4911 - val_sparse_categorical_crossentropy: 1.3333\n",
      "Epoch 28/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9873 - accuracy: 0.6562 - sparse_categorical_crossentropy: 0.9738.\n",
      "Epoch 28: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1812 - accuracy: 0.5399 - sparse_categorical_crossentropy: 1.1822 - val_loss: 1.3795 - val_accuracy: 0.4911 - val_sparse_categorical_crossentropy: 1.3099\n",
      "Epoch 29/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.3047 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.2909.\n",
      "Epoch 29: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.2065 - accuracy: 0.5459 - sparse_categorical_crossentropy: 1.1950 - val_loss: 1.3663 - val_accuracy: 0.4911 - val_sparse_categorical_crossentropy: 1.3057\n",
      "Epoch 30/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1968 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.1828.\n",
      "Epoch 30: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1679 - accuracy: 0.5518 - sparse_categorical_crossentropy: 1.1493 - val_loss: 1.3584 - val_accuracy: 0.5030 - val_sparse_categorical_crossentropy: 1.2869\n",
      "Epoch 31/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.3004 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.2861.\n",
      "Epoch 31: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1719 - accuracy: 0.5503 - sparse_categorical_crossentropy: 1.1647 - val_loss: 1.3517 - val_accuracy: 0.5089 - val_sparse_categorical_crossentropy: 1.2810\n",
      "Epoch 32/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0629 - accuracy: 0.7188 - sparse_categorical_crossentropy: 1.0483.\n",
      "Epoch 32: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0836 - accuracy: 0.6272 - sparse_categorical_crossentropy: 1.0460 - val_loss: 1.3530 - val_accuracy: 0.5030 - val_sparse_categorical_crossentropy: 1.2773\n",
      "Epoch 33/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0545 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.0395.\n",
      "Epoch 33: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.1408 - accuracy: 0.5488 - sparse_categorical_crossentropy: 1.1080 - val_loss: 1.3566 - val_accuracy: 0.5207 - val_sparse_categorical_crossentropy: 1.2818\n",
      "Epoch 34/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0355 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.0202.\n",
      "Epoch 34: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0893 - accuracy: 0.5814 - sparse_categorical_crossentropy: 1.0643 - val_loss: 1.3290 - val_accuracy: 0.5030 - val_sparse_categorical_crossentropy: 1.2604\n",
      "Epoch 35/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2317 - accuracy: 0.5938 - sparse_categorical_crossentropy: 1.2162.\n",
      "Epoch 35: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1099 - accuracy: 0.5754 - sparse_categorical_crossentropy: 1.0919 - val_loss: 1.3517 - val_accuracy: 0.5207 - val_sparse_categorical_crossentropy: 1.2738\n",
      "Epoch 36/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0690 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.0532.\n",
      "Epoch 36: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0564 - accuracy: 0.6198 - sparse_categorical_crossentropy: 1.1177 - val_loss: 1.3411 - val_accuracy: 0.5089 - val_sparse_categorical_crossentropy: 1.2624\n",
      "Epoch 37/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0249 - accuracy: 0.5625 - sparse_categorical_crossentropy: 1.0088.\n",
      "Epoch 37: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0810 - accuracy: 0.6124 - sparse_categorical_crossentropy: 1.0667 - val_loss: 1.3372 - val_accuracy: 0.5207 - val_sparse_categorical_crossentropy: 1.2633\n",
      "Epoch 38/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8444 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.8281.\n",
      "Epoch 38: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0455 - accuracy: 0.6317 - sparse_categorical_crossentropy: 1.0060 - val_loss: 1.3414 - val_accuracy: 0.5503 - val_sparse_categorical_crossentropy: 1.2654\n",
      "Epoch 39/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.3137 - accuracy: 0.5312 - sparse_categorical_crossentropy: 1.2971.\n",
      "Epoch 39: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0388 - accuracy: 0.6124 - sparse_categorical_crossentropy: 1.0225 - val_loss: 1.3325 - val_accuracy: 0.5325 - val_sparse_categorical_crossentropy: 1.2608\n",
      "Epoch 40/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9647 - accuracy: 0.5312 - sparse_categorical_crossentropy: 0.9478.\n",
      "Epoch 40: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0186 - accuracy: 0.6272 - sparse_categorical_crossentropy: 1.0214 - val_loss: 1.3272 - val_accuracy: 0.5325 - val_sparse_categorical_crossentropy: 1.2507\n",
      "Epoch 41/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8313 - accuracy: 0.5938 - sparse_categorical_crossentropy: 0.8142.\n",
      "Epoch 41: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0074 - accuracy: 0.6346 - sparse_categorical_crossentropy: 0.9823 - val_loss: 1.3424 - val_accuracy: 0.5385 - val_sparse_categorical_crossentropy: 1.2637\n",
      "Epoch 42/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8991 - accuracy: 0.7188 - sparse_categorical_crossentropy: 0.8816.\n",
      "Epoch 42: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9595 - accuracy: 0.6420 - sparse_categorical_crossentropy: 0.9426 - val_loss: 1.3207 - val_accuracy: 0.5385 - val_sparse_categorical_crossentropy: 1.2403\n",
      "Epoch 43/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0924 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.0746.\n",
      "Epoch 43: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0072 - accuracy: 0.6272 - sparse_categorical_crossentropy: 0.9837 - val_loss: 1.3749 - val_accuracy: 0.5266 - val_sparse_categorical_crossentropy: 1.3037\n",
      "Epoch 44/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0072 - accuracy: 0.5938 - sparse_categorical_crossentropy: 0.9891.\n",
      "Epoch 44: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9726 - accuracy: 0.6716 - sparse_categorical_crossentropy: 0.9721 - val_loss: 1.3242 - val_accuracy: 0.5266 - val_sparse_categorical_crossentropy: 1.2418\n",
      "Epoch 45/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8501 - accuracy: 0.7188 - sparse_categorical_crossentropy: 0.8317.\n",
      "Epoch 45: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9761 - accuracy: 0.6494 - sparse_categorical_crossentropy: 0.9477 - val_loss: 1.3276 - val_accuracy: 0.5562 - val_sparse_categorical_crossentropy: 1.2465\n",
      "Epoch 46/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8703 - accuracy: 0.6250 - sparse_categorical_crossentropy: 0.8517.\n",
      "Epoch 46: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9629 - accuracy: 0.6642 - sparse_categorical_crossentropy: 0.9253 - val_loss: 1.4006 - val_accuracy: 0.5385 - val_sparse_categorical_crossentropy: 1.3255\n",
      "Epoch 47/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0937 - accuracy: 0.5625 - sparse_categorical_crossentropy: 1.0748.\n",
      "Epoch 47: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9163 - accuracy: 0.6657 - sparse_categorical_crossentropy: 0.8951 - val_loss: 1.3722 - val_accuracy: 0.5148 - val_sparse_categorical_crossentropy: 1.3120\n",
      "Epoch 48/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1376 - accuracy: 0.4688 - sparse_categorical_crossentropy: 1.1185.\n",
      "Epoch 48: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9081 - accuracy: 0.6568 - sparse_categorical_crossentropy: 0.9008 - val_loss: 1.4146 - val_accuracy: 0.5207 - val_sparse_categorical_crossentropy: 1.3513\n",
      "Epoch 49/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6030 - accuracy: 0.7812 - sparse_categorical_crossentropy: 0.5836.\n",
      "Epoch 49: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8855 - accuracy: 0.6834 - sparse_categorical_crossentropy: 0.8566 - val_loss: 1.3813 - val_accuracy: 0.5562 - val_sparse_categorical_crossentropy: 1.3074\n",
      "Epoch 50/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9827 - accuracy: 0.6875 - sparse_categorical_crossentropy: 0.9631.\n",
      "Epoch 50: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9330 - accuracy: 0.6627 - sparse_categorical_crossentropy: 0.9358 - val_loss: 1.3685 - val_accuracy: 0.5266 - val_sparse_categorical_crossentropy: 1.2940\n",
      "Epoch 51/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9062 - accuracy: 0.6875 - sparse_categorical_crossentropy: 0.8863.\n",
      "Epoch 51: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8903 - accuracy: 0.6805 - sparse_categorical_crossentropy: 0.8754 - val_loss: 1.3808 - val_accuracy: 0.5266 - val_sparse_categorical_crossentropy: 1.3197\n",
      "Epoch 52/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8284 - accuracy: 0.6875 - sparse_categorical_crossentropy: 0.8084.\n",
      "Epoch 52: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8794 - accuracy: 0.6760 - sparse_categorical_crossentropy: 0.8431 - val_loss: 1.3701 - val_accuracy: 0.5266 - val_sparse_categorical_crossentropy: 1.2879\n",
      "Epoch 53/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0329 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.0125.\n",
      "Epoch 53: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8549 - accuracy: 0.7012 - sparse_categorical_crossentropy: 0.8192 - val_loss: 1.4328 - val_accuracy: 0.5207 - val_sparse_categorical_crossentropy: 1.3695\n",
      "Epoch 54/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7045 - accuracy: 0.8125 - sparse_categorical_crossentropy: 0.6839.\n",
      "Epoch 54: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8519 - accuracy: 0.6716 - sparse_categorical_crossentropy: 0.8134 - val_loss: 1.3710 - val_accuracy: 0.5503 - val_sparse_categorical_crossentropy: 1.3017\n",
      "Epoch 55/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7369 - accuracy: 0.8125 - sparse_categorical_crossentropy: 0.7161.\n",
      "Epoch 55: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8177 - accuracy: 0.7367 - sparse_categorical_crossentropy: 0.7919 - val_loss: 1.3916 - val_accuracy: 0.5562 - val_sparse_categorical_crossentropy: 1.3093\n",
      "Epoch 56/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7622 - accuracy: 0.7812 - sparse_categorical_crossentropy: 0.7411.\n",
      "Epoch 56: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8219 - accuracy: 0.6820 - sparse_categorical_crossentropy: 0.7955 - val_loss: 1.4422 - val_accuracy: 0.5266 - val_sparse_categorical_crossentropy: 1.3747\n",
      "Epoch 57/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7088 - accuracy: 0.7188 - sparse_categorical_crossentropy: 0.6875.\n",
      "Epoch 57: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8802 - accuracy: 0.6657 - sparse_categorical_crossentropy: 0.8518 - val_loss: 1.4109 - val_accuracy: 0.5385 - val_sparse_categorical_crossentropy: 1.3433\n",
      "Epoch 58/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0135 - accuracy: 0.5938 - sparse_categorical_crossentropy: 0.9919.\n",
      "Epoch 58: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8441 - accuracy: 0.7086 - sparse_categorical_crossentropy: 0.8017 - val_loss: 1.3860 - val_accuracy: 0.5621 - val_sparse_categorical_crossentropy: 1.3054\n",
      "Epoch 59/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9339 - accuracy: 0.5938 - sparse_categorical_crossentropy: 0.9121.\n",
      "Epoch 59: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8160 - accuracy: 0.7175 - sparse_categorical_crossentropy: 0.7857 - val_loss: 1.4363 - val_accuracy: 0.5503 - val_sparse_categorical_crossentropy: 1.3666\n",
      "Epoch 60/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9268 - accuracy: 0.6250 - sparse_categorical_crossentropy: 0.9048.\n",
      "Epoch 60: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8497 - accuracy: 0.6805 - sparse_categorical_crossentropy: 0.8209 - val_loss: 1.4669 - val_accuracy: 0.5503 - val_sparse_categorical_crossentropy: 1.4060\n",
      "Epoch 61/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8287 - accuracy: 0.6875 - sparse_categorical_crossentropy: 0.8065.\n",
      "Epoch 61: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7576 - accuracy: 0.7322 - sparse_categorical_crossentropy: 0.7268 - val_loss: 1.4517 - val_accuracy: 0.5562 - val_sparse_categorical_crossentropy: 1.3724\n",
      "Epoch 62/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6444 - accuracy: 0.7812 - sparse_categorical_crossentropy: 0.6219.\n",
      "Epoch 62: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7680 - accuracy: 0.7145 - sparse_categorical_crossentropy: 0.7959 - val_loss: 1.3983 - val_accuracy: 0.5621 - val_sparse_categorical_crossentropy: 1.3106\n",
      "Epoch 63/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6881 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.6654.\n",
      "Epoch 63: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7802 - accuracy: 0.7308 - sparse_categorical_crossentropy: 0.7495 - val_loss: 1.4042 - val_accuracy: 0.5680 - val_sparse_categorical_crossentropy: 1.3238\n",
      "Epoch 64/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8616 - accuracy: 0.7188 - sparse_categorical_crossentropy: 0.8386.\n",
      "Epoch 64: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7629 - accuracy: 0.7130 - sparse_categorical_crossentropy: 0.7429 - val_loss: 1.4381 - val_accuracy: 0.5325 - val_sparse_categorical_crossentropy: 1.3545\n",
      "Epoch 65/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0084 - accuracy: 0.5000 - sparse_categorical_crossentropy: 0.9851.\n",
      "Epoch 65: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7342 - accuracy: 0.7426 - sparse_categorical_crossentropy: 0.7353 - val_loss: 1.4328 - val_accuracy: 0.5385 - val_sparse_categorical_crossentropy: 1.3586\n",
      "Epoch 66/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8126 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.7892.\n",
      "Epoch 66: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7767 - accuracy: 0.7308 - sparse_categorical_crossentropy: 0.7530 - val_loss: 1.5036 - val_accuracy: 0.5621 - val_sparse_categorical_crossentropy: 1.4299\n",
      "Epoch 67/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7103 - accuracy: 0.7188 - sparse_categorical_crossentropy: 0.6866.\n",
      "Epoch 67: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7715 - accuracy: 0.7115 - sparse_categorical_crossentropy: 0.7437 - val_loss: 1.4475 - val_accuracy: 0.5385 - val_sparse_categorical_crossentropy: 1.3712\n",
      "Epoch 68/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6006 - accuracy: 0.8438 - sparse_categorical_crossentropy: 0.5767.\n",
      "Epoch 68: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7243 - accuracy: 0.7485 - sparse_categorical_crossentropy: 0.7187 - val_loss: 1.4706 - val_accuracy: 0.5385 - val_sparse_categorical_crossentropy: 1.3808\n",
      "Epoch 69/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6744 - accuracy: 0.7812 - sparse_categorical_crossentropy: 0.6503.\n",
      "Epoch 69: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7240 - accuracy: 0.7160 - sparse_categorical_crossentropy: 0.7100 - val_loss: 1.5006 - val_accuracy: 0.5385 - val_sparse_categorical_crossentropy: 1.4172\n",
      "Epoch 70/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7906 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.7662.\n",
      "Epoch 70: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7254 - accuracy: 0.7574 - sparse_categorical_crossentropy: 0.6859 - val_loss: 1.4833 - val_accuracy: 0.5562 - val_sparse_categorical_crossentropy: 1.3986\n",
      "Epoch 71/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9901 - accuracy: 0.6875 - sparse_categorical_crossentropy: 0.9656.\n",
      "Epoch 71: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6794 - accuracy: 0.7456 - sparse_categorical_crossentropy: 0.6394 - val_loss: 1.5431 - val_accuracy: 0.5503 - val_sparse_categorical_crossentropy: 1.4649\n",
      "Epoch 72/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0711 - accuracy: 0.6562 - sparse_categorical_crossentropy: 1.0463.\n",
      "Epoch 72: val_loss did not improve from 1.30740\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7333 - accuracy: 0.7204 - sparse_categorical_crossentropy: 0.7113 - val_loss: 1.5530 - val_accuracy: 0.5562 - val_sparse_categorical_crossentropy: 1.4853\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=100, batch_size=BATCH_SIZE, validation_data=val_ds, callbacks=CALLBACKS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T18:16:14.768220400Z",
     "start_time": "2024-02-23T18:16:11.527556900Z"
    }
   },
   "id": "18398cf390d74222",
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now using tfdocs.plots.HistoryPlotter, we can visualize the training and validation loss and accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc642328f72f6966"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "size_histories = {}\n",
    "size_histories['Baseline'] = history\n",
    "plotter = tfdocs.plots.HistoryPlotter(metric='sparse_categorical_crossentropy', smoothing_std=10)\n",
    "plotter.plot(size_histories)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T18:16:14.844827700Z",
     "start_time": "2024-02-23T18:16:14.768220400Z"
    }
   },
   "id": "a7a2ea0769fdc279",
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the model the validation loss becomes stagnant after  about 40 epochs.\n",
    "\n",
    "Let's now evaluate the model on the test dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a97fe8c00c1e28e0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 833us/step - loss: 1.2012 - accuracy: 0.6274 - sparse_categorical_crossentropy: 1.1707\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1.201154112815857, 0.6273584961891174, 1.17070472240448]"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds, verbose=1, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T18:16:14.907143400Z",
     "start_time": "2024-02-23T18:16:14.844827700Z"
    }
   },
   "id": "d52c2208aed74b8b",
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also generate a classification report for the model along with the confusion matrix."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb4f229959c3287d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 500us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.77      0.65        44\n",
      "           1       0.71      0.57      0.63        30\n",
      "           2       0.74      0.65      0.69        31\n",
      "           3       0.67      0.30      0.41        27\n",
      "           4       0.57      0.77      0.66        31\n",
      "           5       0.65      0.61      0.63        49\n",
      "\n",
      "    accuracy                           0.63       212\n",
      "   macro avg       0.65      0.61      0.61       212\n",
      "weighted avg       0.64      0.63      0.62       212\n",
      "\n",
      "[[34  1  1  1  2  5]\n",
      " [ 0 17  5  2  4  2]\n",
      " [ 2  3 20  0  4  2]\n",
      " [13  0  0  8  2  4]\n",
      " [ 1  2  0  1 24  3]\n",
      " [11  1  1  0  6 30]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(test_ds)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T18:20:28.711830400Z",
     "start_time": "2024-02-23T18:20:28.680410200Z"
    }
   },
   "id": "73a947990a3f8705",
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the model has an accuracy of 0.63, which is not bad considering the small dataset we have.\n",
    "Notable are the precision and recall scores for class 3, which are 0.67 and 0.30, respectively, showing that the model is not performing well on this class\n",
    "in comparison to the other classes. This class is about 'WORDS RELATING TO THE INTELLECTUAL FACULTIES', whose words may be more challenging to classify\n",
    "due to the philosophical nature of the words.\n",
    "\n",
    "However, we can see that the model is overfitting the training data, as the validation loss becomes higher than the training loss quickly,\n",
    "which makes sense given the small dataset we have.\n",
    "\n",
    "Ironically, the simpler models such as logistic regression and SGD outperformed the neural network model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ab41048404b3e7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Two-Level Classification\n",
    "\n",
    "As we saw from before, we got the best results using the SGD Classifier.\n",
    "\n",
    "Let's now try to perform a two-level classification, where we first classify the class of the word and then classify the division/section of the word based on the class it belongs to.\n",
    "\n",
    "Firstly, we will create a new column in the dataframe named division/section, \n",
    "where it has as value the division if the division is not 4 (meaning `N/A` in our case),\n",
    " otherwise it has the value of the section.\n",
    "\n",
    "Then we will split the data into training and testing sets, using 80% of the data for training and 20% for testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cece51585462464"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# For the cls_df dataframe create a new column named division/section where it has as value the division if the division is not 4, otherwise it has the value of the section\n",
    "cls_df['division/section'] = cls_df['division']\n",
    "\n",
    "# 4 Means No Division\n",
    "cls_df.loc[cls_df['division'] == 4, 'division/section'] = cls_df['section']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:33:55.287154400Z",
     "start_time": "2024-02-23T23:33:55.269341Z"
    }
   },
   "id": "a7f158c0df67a4a0",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's split the data into training and testing sets, using 80% of the data for training and 20% for testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2411460d85eb8e55"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get class and division/section\n",
    "y = cls_df[['class', 'division/section']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:33:55.609492100Z",
     "start_time": "2024-02-23T23:33:55.603533400Z"
    }
   },
   "id": "85fde21674eaec17",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having done that we can now approach the two-level classification using the SGD Classifier as before, but this\n",
    "time we will use the `MultiOutputClassifier` from sklearn to perform the multi-output classification.\n",
    "However, there is a drawback to this approach, because at present, no metric in sklearn.metrics supports the multiclass-multioutput classification task.\n",
    "This means that we will have to create our own custom accuracy metric for the multi-output classification."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25d67e79115b7dd8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-output Accuracy: 0.4386792452830189\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Create a multi-output classifier\n",
    "multi_output_model = MultiOutputClassifier(\n",
    "    sgd_model, n_jobs=-1)\n",
    "\n",
    "# Train the multi-output classifier\n",
    "multi_output_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = multi_output_model.predict(X_test)\n",
    "\n",
    "# Custom accuracy for multi-output\n",
    "def multioutput_accuracy(y_true, y_pred):\n",
    "    correct = np.all(y_true == y_pred, axis=1)\n",
    "    return np.mean(correct)\n",
    "\n",
    "\n",
    "accuracy = multioutput_accuracy(y_test, y_pred)\n",
    "print(f\"Multi-output Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:33:59.683944400Z",
     "start_time": "2024-02-23T23:33:56.308865800Z"
    }
   },
   "id": "79b2a49fecb749be",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the multi-output accuracy is 0.438 which is not bad considering the small dataset we have,\n",
    "however, it is not as good as the single-output classification we performed earlier.\n",
    "Also, the custom accuracy metric we created may not be the best way to evaluate the multi-output classification.\n",
    "\n",
    "For that reason, let's try and train separate models for the class and the division/section and see how they perform.\n",
    "\n",
    "We will train a separate model (or models) for predicting the division/section within each class. This model will be trained only on data from a specific class. For example, you might have one model for each class, or a single model that uses the class prediction as an additional input."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7326f9175f93ceb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Accuracy: 0.6410256410256411\n",
      "Class 1 Accuracy: 0.7666666666666667\n",
      "Class 2 Accuracy: 0.7586206896551724\n",
      "Class 3 Accuracy: 0.696969696969697\n",
      "Class 4 Accuracy: 0.6756756756756757\n",
      "Class 5 Accuracy: 0.7608695652173914\n",
      "\n",
      "Average Accuracy: 0.7166379892017073\n"
     ]
    }
   ],
   "source": [
    "# For each class, train a separate model to predict the division/section\n",
    "class_models = {}\n",
    "accuracies = []\n",
    "for i in range(6):\n",
    "    # Get the data for the class\n",
    "    class_data = cls_df[cls_df['class'] == i]\n",
    "    X_C = np.vstack(class_data['embedding'])\n",
    "    y_C = class_data['division/section']\n",
    "    # Split the data into training and testing sets\n",
    "    X_C_train, X_C_test, y_C_train, y_C_test = train_test_split(X_C, y_C, test_size=0.2, random_state=62)\n",
    "\n",
    "    # Train the model\n",
    "    model = sgd_model\n",
    "    model.fit(X_C_train, y_C_train)\n",
    "\n",
    "    # Generate classification report\n",
    "    y_C_pred = model.predict(X_C_test)\n",
    "    print(f\"Class {i} Accuracy: {metrics.accuracy_score(y_C_test, y_C_pred)}\")\n",
    "    accuracies.append(metrics.accuracy_score(y_C_test, y_C_pred))\n",
    "    class_models[i] = model\n",
    "\n",
    "print()\n",
    "print(f\"Average Accuracy: {np.mean(accuracies)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:36:04.021641700Z",
     "start_time": "2024-02-23T23:36:03.764498700Z"
    }
   },
   "id": "200c547705fbd699",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now having the separate models for each class, we can now chain them together to predict the division/section based on the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec307bba8ff898af"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Chain the models together to predict the division/section based on the class\n",
    "def predict_word_class_and_division(df):\n",
    "    predictions = []\n",
    "\n",
    "    for word in df[word_column]:\n",
    "        # Predict the class\n",
    "        predicted_class = sgd_model.predict(word)[0]  # Assuming the output is a list\n",
    "\n",
    "        # Predict the division/section within the predicted class\n",
    "        predicted_division = class_models[predicted_class].predict([word])[0]  # Assuming the output is a list\n",
    "\n",
    "        predictions.append({'class': predicted_class, 'division/section': predicted_division})\n",
    "\n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "y_pred = predict_word_class_and_division(X_test)\n",
    "\n",
    "# Custom accuracy for multi-output\n",
    "def multioutput_accuracy(y_true, y_pred):\n",
    "    correct = np.all(y_true == y_pred, axis=1)\n",
    "    return np.mean(correct)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4007c2ce0b400bf8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
