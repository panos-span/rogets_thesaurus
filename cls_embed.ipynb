{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Classification\n",
    "\n",
    "Let's now retrieve the classification embeddings from the chromadb vector database and perform classification on them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb007503dc588dee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = client.get_collection(\"nomic_classification_v1\")"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:20:31.456795800Z",
     "start_time": "2024-02-24T00:20:30.346782100Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us get the words along with their classes and division and sections from the json file we created earlier."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84452ecc48ce9511"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"hierarchy.json\", \"r\") as f:\n",
    "    categories = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:20:31.464161900Z",
     "start_time": "2024-02-24T00:20:31.457796500Z"
    }
   },
   "id": "6185d3b32070a313",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's retrieve the embeddings and the words along with the class, division, and section from the chromadb and create a dataframe with them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11d5ef59ed70ec91"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                   word                                          embedding  \\\n0             Existence  [-0.029754638671875, -0.0263671875, -0.0127792...   \n1           Inexistence  [-0.01029205322265625, 0.019256591796875, -0.0...   \n2         Consanguinity  [0.0386962890625, -0.01026153564453125, -0.013...   \n3            Trisection  [0.007350921630859375, -0.001903533935546875, ...   \n4             Innocence  [0.0338134765625, 0.0139923095703125, -0.03097...   \n...                 ...                                                ...   \n1052              Knave  [0.0032215118408203125, 0.01201629638671875, -...   \n1053  Disinterestedness  [-0.01776123046875, 0.020965576171875, -0.0221...   \n1054        Selfishness  [0.01071929931640625, 0.049468994140625, -0.03...   \n1055             Virtue  [0.0201416015625, 0.006626129150390625, -0.035...   \n1056               Vice  [0.0035228729248046875, -0.028533935546875, -0...   \n\n                                                class division  \\\n0                 WORDS EXPRESSING ABSTRACT RELATIONS      N/A   \n1                 WORDS EXPRESSING ABSTRACT RELATIONS      N/A   \n2                 WORDS EXPRESSING ABSTRACT RELATIONS      N/A   \n3                 WORDS EXPRESSING ABSTRACT RELATIONS      N/A   \n4     WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n...                                               ...      ...   \n1052  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n1053  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n1054  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n1055  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n1056  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n\n               section  \n0            EXISTENCE  \n1            EXISTENCE  \n2             RELATION  \n3               NUMBER  \n4     MORAL AFFECTIONS  \n...                ...  \n1052  MORAL AFFECTIONS  \n1053  MORAL AFFECTIONS  \n1054  MORAL AFFECTIONS  \n1055  MORAL AFFECTIONS  \n1056  MORAL AFFECTIONS  \n\n[1057 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>embedding</th>\n      <th>class</th>\n      <th>division</th>\n      <th>section</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Existence</td>\n      <td>[-0.029754638671875, -0.0263671875, -0.0127792...</td>\n      <td>WORDS EXPRESSING ABSTRACT RELATIONS</td>\n      <td>N/A</td>\n      <td>EXISTENCE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Inexistence</td>\n      <td>[-0.01029205322265625, 0.019256591796875, -0.0...</td>\n      <td>WORDS EXPRESSING ABSTRACT RELATIONS</td>\n      <td>N/A</td>\n      <td>EXISTENCE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Consanguinity</td>\n      <td>[0.0386962890625, -0.01026153564453125, -0.013...</td>\n      <td>WORDS EXPRESSING ABSTRACT RELATIONS</td>\n      <td>N/A</td>\n      <td>RELATION</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Trisection</td>\n      <td>[0.007350921630859375, -0.001903533935546875, ...</td>\n      <td>WORDS EXPRESSING ABSTRACT RELATIONS</td>\n      <td>N/A</td>\n      <td>NUMBER</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Innocence</td>\n      <td>[0.0338134765625, 0.0139923095703125, -0.03097...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1052</th>\n      <td>Knave</td>\n      <td>[0.0032215118408203125, 0.01201629638671875, -...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>1053</th>\n      <td>Disinterestedness</td>\n      <td>[-0.01776123046875, 0.020965576171875, -0.0221...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>1054</th>\n      <td>Selfishness</td>\n      <td>[0.01071929931640625, 0.049468994140625, -0.03...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>1055</th>\n      <td>Virtue</td>\n      <td>[0.0201416015625, 0.006626129150390625, -0.035...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>1056</th>\n      <td>Vice</td>\n      <td>[0.0035228729248046875, -0.028533935546875, -0...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n  </tbody>\n</table>\n<p>1057 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Retrieve the embeddings, words, and metadata\n",
    "words = collection.get(include=[\"embeddings\", \"documents\"])['documents']\n",
    "embeddings = collection.get(include=[\"embeddings\", \"documents\"])['embeddings']\n",
    "metadata = collection.get(include=[\"embeddings\", \"documents\", \"metadatas\"])['metadatas']\n",
    "\n",
    "# Create a dataframe with the embeddings and the words\n",
    "cls_df = pd.DataFrame({'word': words, 'embedding': embeddings})\n",
    "\n",
    "# Add the class, division, and section from metadata to the dataframe\n",
    "cls_df['class'] = [md['class'] for md in metadata]\n",
    "cls_df['division'] = [md['division'] for md in metadata]\n",
    "cls_df['section'] = [md['section'] for md in metadata]\n",
    "\n",
    "cls_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:20:32.120252600Z",
     "start_time": "2024-02-24T00:20:31.461160300Z"
    }
   },
   "id": "b09af619c5292133",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's prepare the data for classification by converting the class, division, and section to numerical values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3501a068561a6136"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                   word                                          embedding  \\\n0             Existence  [-0.029754638671875, -0.0263671875, -0.0127792...   \n1           Inexistence  [-0.01029205322265625, 0.019256591796875, -0.0...   \n2         Consanguinity  [0.0386962890625, -0.01026153564453125, -0.013...   \n3            Trisection  [0.007350921630859375, -0.001903533935546875, ...   \n4             Innocence  [0.0338134765625, 0.0139923095703125, -0.03097...   \n...                 ...                                                ...   \n1052              Knave  [0.0032215118408203125, 0.01201629638671875, -...   \n1053  Disinterestedness  [-0.01776123046875, 0.020965576171875, -0.0221...   \n1054        Selfishness  [0.01071929931640625, 0.049468994140625, -0.03...   \n1055             Virtue  [0.0201416015625, 0.006626129150390625, -0.035...   \n1056               Vice  [0.0035228729248046875, -0.028533935546875, -0...   \n\n      class  division  section  \n0         0         4        7  \n1         0         4        7  \n2         0         4       29  \n3         0         4       19  \n4         4         4       16  \n...     ...       ...      ...  \n1052      4         4       16  \n1053      4         4       16  \n1054      4         4       16  \n1055      4         4       16  \n1056      4         4       16  \n\n[1057 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>embedding</th>\n      <th>class</th>\n      <th>division</th>\n      <th>section</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Existence</td>\n      <td>[-0.029754638671875, -0.0263671875, -0.0127792...</td>\n      <td>0</td>\n      <td>4</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Inexistence</td>\n      <td>[-0.01029205322265625, 0.019256591796875, -0.0...</td>\n      <td>0</td>\n      <td>4</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Consanguinity</td>\n      <td>[0.0386962890625, -0.01026153564453125, -0.013...</td>\n      <td>0</td>\n      <td>4</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Trisection</td>\n      <td>[0.007350921630859375, -0.001903533935546875, ...</td>\n      <td>0</td>\n      <td>4</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Innocence</td>\n      <td>[0.0338134765625, 0.0139923095703125, -0.03097...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1052</th>\n      <td>Knave</td>\n      <td>[0.0032215118408203125, 0.01201629638671875, -...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1053</th>\n      <td>Disinterestedness</td>\n      <td>[-0.01776123046875, 0.020965576171875, -0.0221...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1054</th>\n      <td>Selfishness</td>\n      <td>[0.01071929931640625, 0.049468994140625, -0.03...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1055</th>\n      <td>Virtue</td>\n      <td>[0.0201416015625, 0.006626129150390625, -0.035...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1056</th>\n      <td>Vice</td>\n      <td>[0.0035228729248046875, -0.028533935546875, -0...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n<p>1057 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all classes and divisions/sections as numerical values\n",
    "cls_df['class'] = pd.Categorical(cls_df['class'])\n",
    "cls_df['division'] = pd.Categorical(cls_df['division'])\n",
    "cls_df['section'] = pd.Categorical(cls_df['section'])\n",
    "\n",
    "cls_df['class'] = cls_df['class'].cat.codes\n",
    "cls_df['division'] = cls_df['division'].cat.codes\n",
    "cls_df['section'] = cls_df['section'].cat.codes\n",
    "cls_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:20:32.121252700Z",
     "start_time": "2024-02-24T00:20:32.086160500Z"
    }
   },
   "id": "516a1edf25fbdd15",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also prepare the embeddings for classification by converting them to a numpy array."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "891ef0bbf5ddeecc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.vstack(cls_df['embedding'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:20:32.168254800Z",
     "start_time": "2024-02-24T00:20:32.102251400Z"
    }
   },
   "id": "1d2b8c0fca50cf83",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class logistic regression\n",
    "\n",
    "Let us start by training a multi-class logistic regression model on the embeddings and the class.\n",
    "\n",
    "Before we do that, however, let's split the data into training and testing sets, using 80% of the data for training and 20% for testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0da1d957a8e1533"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = cls_df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:20:33.268251900Z",
     "start_time": "2024-02-24T00:20:32.130252600Z"
    }
   },
   "id": "e541551066686048",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's train a logistic regression model on the training data and evaluate it on the testing data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b8a7c1f9edc9e63"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=1000, multi_class='ovr', n_jobs=-1, random_state=42)",
      "text/html": "<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"â–¸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"â–¾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;ovr&#x27;, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;ovr&#x27;, n_jobs=-1, random_state=42)</pre></div> </div></div></div></div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, multi_class='ovr', n_jobs=-1, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:20:37.352335600Z",
     "start_time": "2024-02-24T00:20:33.731770100Z"
    }
   },
   "id": "2a20c0e4ce5e28ab",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68        44\n",
      "           1       0.62      0.50      0.56        30\n",
      "           2       0.67      0.45      0.54        31\n",
      "           3       0.77      0.74      0.75        27\n",
      "           4       0.63      0.77      0.70        31\n",
      "           5       0.58      0.69      0.63        49\n",
      "\n",
      "    accuracy                           0.65       212\n",
      "   macro avg       0.66      0.64      0.64       212\n",
      "weighted avg       0.65      0.65      0.64       212\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Generate classification report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:20:37.353335900Z",
     "start_time": "2024-02-24T00:20:37.322596800Z"
    }
   },
   "id": "8caa5ddbc9c9df03",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having trained and evaluated the model, we can see that the model has an accuracy of 0.64, \n",
    "which is not bad considering the previous results we got from the clustering of the embeddings.\n",
    "\n",
    "However, let's use a hyperparameter tuning library such as `optuna` to find the best hyperparameters for the model.\n",
    "\n",
    "We will use the `cross_val_score` function from `sklearn` to evaluate the model using cross-validation and the `optuna` library to find the best hyperparameters for the model based on the maximization of the accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f52da790af8442a3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-24 02:12:50,317] A new study created in memory with name: no-name-69d8a854-349f-485b-a37a-5ba3d4228075\n",
      "[W 2024-02-24 02:12:52,534] Trial 0 failed with parameters: {'multi_class': 'ovr', 'C': 0.002435712112797069} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Temp\\ipykernel_31096\\3990631084.py\", line 13, in objective\n",
      "    score = cross_val_score(model, X, y, scoring='accuracy')\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 714, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 425, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1296, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1952, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1595, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1707, in _retrieve\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n",
      "[W 2024-02-24 02:12:52,558] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 19\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Create a study object and specify the optimization direction\u001B[39;00m\n\u001B[0;32m     18\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 19\u001B[0m \u001B[43mstudy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# You can adjust the number of trials\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Output the best hyperparameters\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBest trial:\u001B[39m\u001B[38;5;124m'\u001B[39m, study\u001B[38;5;241m.\u001B[39mbest_trial\u001B[38;5;241m.\u001B[39mparams)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:451\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[0;32m    349\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    350\u001B[0m     func: ObjectiveFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    357\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    358\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    359\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[0;32m    360\u001B[0m \n\u001B[0;32m    361\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    449\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 451\u001B[0m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    452\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    453\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    454\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    456\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    458\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001B[0m, in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 66\u001B[0m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     79\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    160\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 163\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    244\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    247\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[0;32m    250\u001B[0m ):\n\u001B[1;32m--> 251\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[0;32m    252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[0;32m    199\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 200\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    202\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[0;32m    203\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[1;32mIn[16], line 13\u001B[0m, in \u001B[0;36mobjective\u001B[1;34m(trial)\u001B[0m\n\u001B[0;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m LogisticRegression(C\u001B[38;5;241m=\u001B[39mC, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, multi_class\u001B[38;5;241m=\u001B[39mmulti_class , n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Perform cross-validation and return the mean score\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m score \u001B[38;5;241m=\u001B[39m \u001B[43mcross_val_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43maccuracy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m score\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:714\u001B[0m, in \u001B[0;36mcross_val_score\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001B[0m\n\u001B[0;32m    711\u001B[0m \u001B[38;5;66;03m# To ensure multimetric format is not supported\u001B[39;00m\n\u001B[0;32m    712\u001B[0m scorer \u001B[38;5;241m=\u001B[39m check_scoring(estimator, scoring\u001B[38;5;241m=\u001B[39mscoring)\n\u001B[1;32m--> 714\u001B[0m cv_results \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    715\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    716\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    717\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    718\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    719\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    720\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    721\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    722\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    723\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    724\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    725\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    726\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    727\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    728\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cv_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_score\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425\u001B[0m, in \u001B[0;36mcross_validate\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[0m\n\u001B[0;32m    422\u001B[0m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[0;32m    423\u001B[0m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[0;32m    424\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m--> 425\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    426\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    427\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    428\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    429\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscorer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscorers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    435\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    436\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscore_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscorer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    437\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_times\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_estimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_estimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[43m        \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    442\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\n\u001B[0;32m    443\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    445\u001B[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001B[0;32m    447\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[0;32m    448\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[0;32m    449\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     66\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1790\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1792\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1793\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1794\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    127\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:890\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[0;32m    888\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    889\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 890\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    892\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    893\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[0;32m    894\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1351\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1344\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1346\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1347\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1348\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1349\u001B[0m     )\n\u001B[0;32m   1350\u001B[0m ):\n\u001B[1;32m-> 1351\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296\u001B[0m, in \u001B[0;36mLogisticRegression.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m   1293\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1294\u001B[0m     n_threads \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1296\u001B[0m fold_coefs_ \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprefer\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1297\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1298\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1299\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1300\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpos_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1301\u001B[0m \u001B[43m        \u001B[49m\u001B[43mCs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mC_\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1302\u001B[0m \u001B[43m        \u001B[49m\u001B[43ml1_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ml1_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1303\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_intercept\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_intercept\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1304\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1305\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1306\u001B[0m \u001B[43m        \u001B[49m\u001B[43msolver\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msolver\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1307\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmulti_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmulti_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1308\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1309\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1310\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcoef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwarm_start_coef_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpenalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpenalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_squared_sum\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_squared_sum\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1315\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1317\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1318\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclass_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarm_start_coef_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mclasses_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarm_start_coef\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1319\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1321\u001B[0m fold_coefs_, _, n_iter_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mfold_coefs_)\n\u001B[0;32m   1322\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_iter_ \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(n_iter_, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint32)[:, \u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     66\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1946\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   1947\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   1948\u001B[0m \u001B[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001B[39;00m\n\u001B[0;32m   1949\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   1950\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1952\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1592\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1594\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1595\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1597\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1598\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1599\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1600\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1601\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1707\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1702\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1703\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[0;32m   1704\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1705\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[0;32m   1706\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[1;32m-> 1707\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1708\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1710\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[0;32m   1711\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[0;32m   1712\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    multi_class = trial.suggest_categorical('multi_class', ['ovr', 'multinomial'])\n",
    "    C = trial.suggest_float('C', 1e-4, 1e4, log=True)\n",
    "\n",
    "    # Create logistic regression model with suggested hyperparameters\n",
    "    model = LogisticRegression(C=C, random_state=22, max_iter=1000, multi_class=multi_class , n_jobs=-1)\n",
    "    # Perform cross-validation and return the mean score\n",
    "    score = cross_val_score(model, X, y, scoring='accuracy')\n",
    "    return score.mean()\n",
    "\n",
    "\n",
    "# Create a study object and specify the optimization direction\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print('Best trial:', study.best_trial.params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:12:53.170361100Z",
     "start_time": "2024-02-24T00:12:49.842999700Z"
    }
   },
   "id": "9e76682b23c63239",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class SVM\n",
    "\n",
    "Let's now train a multi-class SVM model on the embeddings and the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e86e3dc9de132389"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "SVC(C=0.7524791632787694, kernel='linear', random_state=22)",
      "text/html": "<style>#sk-container-id-6 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-6 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-6 pre {\n  padding: 0;\n}\n\n#sk-container-id-6 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-6 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-6 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-6 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-6 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-6 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-6 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-6 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-6 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-6 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-6 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-6 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-6 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"â–¸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-6 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-6 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-6 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"â–¾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n#sk-container-id-6 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-6 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-6 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-6 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-6 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-6 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-6 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-6 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-6 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-6 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=0.7524791632787694, kernel=&#x27;linear&#x27;, random_state=22)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=0.7524791632787694, kernel=&#x27;linear&#x27;, random_state=22)</pre></div> </div></div></div></div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', random_state=22)\n",
    "svm_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:17:06.711543300Z",
     "start_time": "2024-02-24T00:17:06.630847500Z"
    }
   },
   "id": "2c66c7edab0d64",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.69      0.71        35\n",
      "           1       0.72      0.66      0.69        35\n",
      "           2       0.68      0.46      0.55        28\n",
      "           3       0.69      0.69      0.69        35\n",
      "           4       0.71      0.69      0.70        36\n",
      "           5       0.55      0.74      0.63        43\n",
      "\n",
      "    accuracy                           0.67       212\n",
      "   macro avg       0.68      0.66      0.66       212\n",
      "weighted avg       0.68      0.67      0.66       212\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:17:06.834200100Z",
     "start_time": "2024-02-24T00:17:06.808759Z"
    }
   },
   "id": "62835de5722c9451",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use the `optuna` library to find the best hyperparameters for the model based on the maximization of the accuracy.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a87564b264fa6a83"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-24 02:14:01,506] A new study created in memory with name: no-name-38251ba7-1be7-47a7-8447-1a6506656109\n",
      "[I 2024-02-24 02:14:01,984] Trial 0 finished with value: 0.21570240543682373 and parameters: {'C': 7.346606385706798e-05, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 0 with value: 0.21570240543682373.\n",
      "[I 2024-02-24 02:14:02,383] Trial 1 finished with value: 0.5298399356165608 and parameters: {'C': 3.112791470896396, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:02,752] Trial 2 finished with value: 0.4626263077886078 and parameters: {'C': 19925.399492522036, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:03,368] Trial 3 finished with value: 0.21570240543682373 and parameters: {'C': 2.6544817175931484e-05, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:03,985] Trial 4 finished with value: 0.21570240543682373 and parameters: {'C': 0.0010931590179148747, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:04,535] Trial 5 finished with value: 0.4418179379415184 and parameters: {'C': 0.2618748005711188, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:05,081] Trial 6 finished with value: 0.21570240543682373 and parameters: {'C': 0.0016734952877087974, 'kernel': 'poly', 'gamma': 'auto', 'degree': 10}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:05,677] Trial 7 finished with value: 0.21570240543682373 and parameters: {'C': 0.024678213832132836, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:06,575] Trial 8 finished with value: 0.5109049450058124 and parameters: {'C': 3382.900960405658, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:07,282] Trial 9 finished with value: 0.21570240543682373 and parameters: {'C': 0.2914004942967366, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:08,025] Trial 10 finished with value: 0.5109049450058124 and parameters: {'C': 158.62626562796882, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:08,854] Trial 11 finished with value: 0.5109049450058124 and parameters: {'C': 243.042782155625, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:09,779] Trial 12 finished with value: 0.5109049450058124 and parameters: {'C': 82.46745502159425, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:10,290] Trial 13 finished with value: 0.4626263077886078 and parameters: {'C': 60674.23868770223, 'kernel': 'poly', 'gamma': 'scale', 'degree': 1}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:10,633] Trial 14 finished with value: 0.49482696950728783 and parameters: {'C': 11.465461833486582, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:11,424] Trial 15 finished with value: 0.5109049450058124 and parameters: {'C': 1041.2310585659307, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:11,771] Trial 16 finished with value: 0.5042967003487436 and parameters: {'C': 9.537121569869601, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:12,541] Trial 17 finished with value: 0.5109049450058124 and parameters: {'C': 7503.777002531191, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 1 with value: 0.5298399356165608.\n",
      "[I 2024-02-24 02:14:13,137] Trial 18 finished with value: 0.5336314048108737 and parameters: {'C': 1.9728533582587526, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 18 with value: 0.5336314048108737.\n",
      "[I 2024-02-24 02:14:13,795] Trial 19 finished with value: 0.5355226683358669 and parameters: {'C': 3.023913918769061, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:14,390] Trial 20 finished with value: 0.21570240543682373 and parameters: {'C': 0.024904514496397377, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:15,012] Trial 21 finished with value: 0.5355226683358669 and parameters: {'C': 4.614991357064005, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:15,639] Trial 22 finished with value: 0.5289010104623089 and parameters: {'C': 0.5732884490434929, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:16,402] Trial 23 finished with value: 0.5355226683358669 and parameters: {'C': 17.84496562628588, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:17,194] Trial 24 finished with value: 0.5213270142180095 and parameters: {'C': 23.645512656614542, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:17,790] Trial 25 finished with value: 0.21570240543682373 and parameters: {'C': 0.09760123070904306, 'kernel': 'poly', 'gamma': 'scale', 'degree': 8}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:18,480] Trial 26 finished with value: 0.5213270142180095 and parameters: {'C': 509.85438950811795, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:19,219] Trial 27 finished with value: 0.5307922739873021 and parameters: {'C': 35.76338410722615, 'kernel': 'poly', 'gamma': 'scale', 'degree': 8}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:19,832] Trial 28 finished with value: 0.5260574085665743 and parameters: {'C': 3.2324271922395726, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:20,380] Trial 29 finished with value: 0.21570240543682373 and parameters: {'C': 0.002621909168361747, 'kernel': 'poly', 'gamma': 'auto', 'degree': 8}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:20,971] Trial 30 finished with value: 0.21570240543682373 and parameters: {'C': 0.06635143962780242, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:21,622] Trial 31 finished with value: 0.5355226683358669 and parameters: {'C': 6.5915162663526825, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:22,285] Trial 32 finished with value: 0.534583743181615 and parameters: {'C': 3.462183407514682, 'kernel': 'poly', 'gamma': 'scale', 'degree': 7}. Best is trial 19 with value: 0.5355226683358669.\n",
      "[I 2024-02-24 02:14:22,891] Trial 33 finished with value: 0.5374094607886971 and parameters: {'C': 0.9026372397485738, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:23,614] Trial 34 finished with value: 0.5336224626665474 and parameters: {'C': 0.5312726272712668, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:24,004] Trial 35 finished with value: 0.4626263077886078 and parameters: {'C': 54.11290893785433, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:24,616] Trial 36 finished with value: 0.21570240543682373 and parameters: {'C': 0.9315320838386998, 'kernel': 'poly', 'gamma': 'auto', 'degree': 3}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:25,181] Trial 37 finished with value: 0.21570240543682373 and parameters: {'C': 0.00021208867109387075, 'kernel': 'poly', 'gamma': 'scale', 'degree': 7}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:25,701] Trial 38 finished with value: 0.21570240543682373 and parameters: {'C': 0.0068736336994536795, 'kernel': 'poly', 'gamma': 'auto', 'degree': 7}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:26,098] Trial 39 finished with value: 0.5326880085844585 and parameters: {'C': 1.424011940705963, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:26,649] Trial 40 finished with value: 0.21570240543682373 and parameters: {'C': 0.14252336973024157, 'kernel': 'poly', 'gamma': 'auto', 'degree': 5}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:27,613] Trial 41 finished with value: 0.5355226683358669 and parameters: {'C': 7.711318939134139, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:28,289] Trial 42 finished with value: 0.534583743181615 and parameters: {'C': 6.167424147060117, 'kernel': 'poly', 'gamma': 'scale', 'degree': 7}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:28,900] Trial 43 finished with value: 0.5336314048108737 and parameters: {'C': 1.4785914758203473, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:29,266] Trial 44 finished with value: 0.4664446034158992 and parameters: {'C': 20.805209413340524, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:30,059] Trial 45 finished with value: 0.5355226683358669 and parameters: {'C': 217.90789016283912, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:30,651] Trial 46 finished with value: 0.4229187159080747 and parameters: {'C': 1413.5227870445026, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:31,172] Trial 47 finished with value: 0.5336224626665474 and parameters: {'C': 0.5194985462632763, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:31,781] Trial 48 finished with value: 0.534583743181615 and parameters: {'C': 75.57955392671461, 'kernel': 'poly', 'gamma': 'scale', 'degree': 7}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:32,335] Trial 49 finished with value: 0.21570240543682373 and parameters: {'C': 0.0365429641014625, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:33,068] Trial 50 finished with value: 0.5023786103907717 and parameters: {'C': 0.27757726970235086, 'kernel': 'poly', 'gamma': 'scale', 'degree': 3}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:33,714] Trial 51 finished with value: 0.5355226683358669 and parameters: {'C': 6.848269747924963, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:34,418] Trial 52 finished with value: 0.5355226683358669 and parameters: {'C': 16.80561387045163, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:35,064] Trial 53 finished with value: 0.5118528123043906 and parameters: {'C': 3.9086699080565106, 'kernel': 'poly', 'gamma': 'scale', 'degree': 10}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:35,888] Trial 54 finished with value: 0.5099570777072342 and parameters: {'C': 10.447532038358121, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:36,503] Trial 55 finished with value: 0.5355226683358669 and parameters: {'C': 156.0361326224669, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:37,079] Trial 56 finished with value: 0.5288920683179826 and parameters: {'C': 31.65481865104537, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:37,465] Trial 57 finished with value: 0.5317401412858803 and parameters: {'C': 1.44795799847154, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:37,993] Trial 58 finished with value: 0.22231512116605562 and parameters: {'C': 0.1829106060369741, 'kernel': 'poly', 'gamma': 'scale', 'degree': 7}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:38,627] Trial 59 finished with value: 0.5109049450058124 and parameters: {'C': 106.1984205669056, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:38,989] Trial 60 finished with value: 0.5222659393722615 and parameters: {'C': 3.674873348824021, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:39,535] Trial 61 finished with value: 0.5355226683358669 and parameters: {'C': 262.2828930075466, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:40,070] Trial 62 finished with value: 0.5355226683358669 and parameters: {'C': 4748.848370190109, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:40,626] Trial 63 finished with value: 0.5355226683358669 and parameters: {'C': 535.2470686029187, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:41,162] Trial 64 finished with value: 0.5355226683358669 and parameters: {'C': 36.45428192595595, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:41,697] Trial 65 finished with value: 0.5288920683179826 and parameters: {'C': 10.483774575544533, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:42,140] Trial 66 finished with value: 0.535518197263704 and parameters: {'C': 0.7336524246176801, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:42,604] Trial 67 finished with value: 0.21570240543682373 and parameters: {'C': 6.087300196171019, 'kernel': 'poly', 'gamma': 'auto', 'degree': 7}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:43,139] Trial 68 finished with value: 0.5336314048108737 and parameters: {'C': 2.0080034187000457, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:43,675] Trial 69 finished with value: 0.5355226683358669 and parameters: {'C': 66.69880425620553, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:44,217] Trial 70 finished with value: 0.5307922739873021 and parameters: {'C': 16.970642609816174, 'kernel': 'poly', 'gamma': 'scale', 'degree': 8}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:44,754] Trial 71 finished with value: 0.5355226683358669 and parameters: {'C': 8.881313740684615, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:45,284] Trial 72 finished with value: 0.5288920683179826 and parameters: {'C': 19798.010409563736, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:45,818] Trial 73 finished with value: 0.5355226683358669 and parameters: {'C': 5.114326804077785, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:46,337] Trial 74 finished with value: 0.5260618796387374 and parameters: {'C': 2.029786383635188, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:46,795] Trial 75 finished with value: 0.5146830009836358 and parameters: {'C': 0.4056469387280418, 'kernel': 'poly', 'gamma': 'scale', 'degree': 1}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:47,439] Trial 76 finished with value: 0.5109049450058124 and parameters: {'C': 44.031801478427845, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:47,798] Trial 77 finished with value: 0.4626263077886078 and parameters: {'C': 191.67466005532665, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:48,328] Trial 78 finished with value: 0.5317267280693911 and parameters: {'C': 1.0795506377525674, 'kernel': 'poly', 'gamma': 'scale', 'degree': 7}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:48,727] Trial 79 finished with value: 0.4219753196816597 and parameters: {'C': 580.1386003515998, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:49,255] Trial 80 finished with value: 0.5326835375122954 and parameters: {'C': 2.3032331872917036, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:49,783] Trial 81 finished with value: 0.5355226683358669 and parameters: {'C': 23.032570493244908, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:50,314] Trial 82 finished with value: 0.5288920683179826 and parameters: {'C': 15.818218195136149, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:50,854] Trial 83 finished with value: 0.534583743181615 and parameters: {'C': 1617.522968963623, 'kernel': 'poly', 'gamma': 'scale', 'degree': 7}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:51,387] Trial 84 finished with value: 0.5355226683358669 and parameters: {'C': 5.617711191753373, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:51,914] Trial 85 finished with value: 0.5288920683179826 and parameters: {'C': 14.529764458039478, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:52,594] Trial 86 finished with value: 0.5355226683358669 and parameters: {'C': 106.36438761922028, 'kernel': 'poly', 'gamma': 'scale', 'degree': 6}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:53,171] Trial 87 finished with value: 0.21570240543682373 and parameters: {'C': 1.6343498784826046e-05, 'kernel': 'poly', 'gamma': 'scale', 'degree': 7}. Best is trial 33 with value: 0.5374094607886971.\n",
      "[I 2024-02-24 02:14:53,742] Trial 88 finished with value: 0.5383483859429491 and parameters: {'C': 0.860546850257075, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 88 with value: 0.5383483859429491.\n",
      "[I 2024-02-24 02:14:54,272] Trial 89 finished with value: 0.21570240543682373 and parameters: {'C': 0.09445466327145915, 'kernel': 'poly', 'gamma': 'auto', 'degree': 4}. Best is trial 88 with value: 0.5383483859429491.\n",
      "[I 2024-02-24 02:14:54,709] Trial 90 finished with value: 0.5440400608065815 and parameters: {'C': 1.015130945730916, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 90 with value: 0.5440400608065815.\n",
      "[I 2024-02-24 02:14:55,138] Trial 91 finished with value: 0.5411964589108468 and parameters: {'C': 0.8853419263104957, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 90 with value: 0.5440400608065815.\n",
      "[I 2024-02-24 02:14:55,694] Trial 92 finished with value: 0.471134758114996 and parameters: {'C': 0.32217222427849246, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 90 with value: 0.5440400608065815.\n",
      "[I 2024-02-24 02:14:56,139] Trial 93 finished with value: 0.5459357954037378 and parameters: {'C': 0.7524791632787694, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 93 with value: 0.5459357954037378.\n",
      "[I 2024-02-24 02:14:56,751] Trial 94 finished with value: 0.5431011356523294 and parameters: {'C': 0.706730055598791, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 93 with value: 0.5459357954037378.\n",
      "[I 2024-02-24 02:14:57,311] Trial 95 finished with value: 0.2904274344987928 and parameters: {'C': 0.17166104470492818, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 93 with value: 0.5459357954037378.\n",
      "[I 2024-02-24 02:14:57,781] Trial 96 finished with value: 0.5411964589108468 and parameters: {'C': 0.8086426643126694, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 93 with value: 0.5459357954037378.\n",
      "[I 2024-02-24 02:14:58,246] Trial 97 finished with value: 0.5421353840650988 and parameters: {'C': 0.8456665785399886, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 93 with value: 0.5459357954037378.\n",
      "[I 2024-02-24 02:14:58,672] Trial 98 finished with value: 0.5421398551372618 and parameters: {'C': 0.8786041415033211, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 93 with value: 0.5459357954037378.\n",
      "[I 2024-02-24 02:14:59,287] Trial 99 finished with value: 0.21570240543682373 and parameters: {'C': 0.037648027213254186, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 93 with value: 0.5459357954037378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'C': 0.7524791632787694, 'kernel': 'linear', 'gamma': 'scale'}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    C = trial.suggest_float('C', 1e-5, 1e5, log=True)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "\n",
    "    # If kernel is 'poly', we also tune degree\n",
    "    if kernel == 'poly':\n",
    "        degree = trial.suggest_int('degree', 1, 10)\n",
    "\n",
    "    # Create and train the SVM model\n",
    "    if kernel == 'poly':\n",
    "        model = SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, random_state=42)\n",
    "    else:\n",
    "        model = SVC(C=C, kernel=kernel, gamma=gamma, random_state=22)\n",
    "\n",
    "    score = cross_val_score(model, X, y, scoring='accuracy')\n",
    "    return score.mean()\n",
    "\n",
    "\n",
    "# Create a study object and specify the optimization direction\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print('Best trial:', study.best_trial.params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:14:59.318273900Z",
     "start_time": "2024-02-24T00:14:01.506329200Z"
    }
   },
   "id": "3352fe02bac0f7c4",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class SGD\n",
    "\n",
    "Let's now train a multi-class SGD model on the embeddings and the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78bd39336c8fa4c1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "SGDClassifier(alpha=2.6545371602330262e-06, eta0=0.007698135990041223,\n              learning_rate='constant', loss='squared_hinge', n_jobs=-1,\n              penalty='l1', random_state=62)",
      "text/html": "<style>#sk-container-id-9 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-9 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-9 pre {\n  padding: 0;\n}\n\n#sk-container-id-9 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-9 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-9 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-9 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-9 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-9 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-9 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-9 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-9 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-9 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-9 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-9 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-9 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-9 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"â–¸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-9 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-9 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-9 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-9 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"â–¾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-9 div.sk-label label.sk-toggleable__label,\n#sk-container-id-9 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-9 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-9 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-9 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-9 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-9 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-9 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-9 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-9 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-9 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-9 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(alpha=2.6545371602330262e-06, eta0=0.007698135990041223,\n              learning_rate=&#x27;constant&#x27;, loss=&#x27;squared_hinge&#x27;, n_jobs=-1,\n              penalty=&#x27;l1&#x27;, random_state=62)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SGDClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.SGDClassifier.html\">?<span>Documentation for SGDClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SGDClassifier(alpha=2.6545371602330262e-06, eta0=0.007698135990041223,\n              learning_rate=&#x27;constant&#x27;, loss=&#x27;squared_hinge&#x27;, n_jobs=-1,\n              penalty=&#x27;l1&#x27;, random_state=62)</pre></div> </div></div></div></div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_model = SGDClassifier(loss='squared_hinge', max_iter=1000, n_jobs=-1, random_state=42,\n",
    "                          penalty='l2', alpha=2.6545371602330262e-06,\n",
    "                          learning_rate='adaptive', eta0=0.007698135990041223)\n",
    "sgd_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:19:30.160680600Z",
     "start_time": "2024-02-24T00:19:29.921243600Z"
    }
   },
   "id": "52c78921d9b40eea",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.74      0.71        35\n",
      "           1       0.61      0.63      0.62        35\n",
      "           2       0.63      0.61      0.62        28\n",
      "           3       0.65      0.57      0.61        35\n",
      "           4       0.70      0.58      0.64        36\n",
      "           5       0.58      0.67      0.62        43\n",
      "\n",
      "    accuracy                           0.64       212\n",
      "   macro avg       0.64      0.63      0.64       212\n",
      "weighted avg       0.64      0.64      0.64       212\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report\n",
    "y_pred = sgd_model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:19:30.166882300Z",
     "start_time": "2024-02-24T00:19:30.160680600Z"
    }
   },
   "id": "fe95d6f1c77a888d",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use the `optuna` library to find the best hyperparameters for the model based on the maximization of the accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc1256b67572c199"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-24 02:17:19,710] A new study created in memory with name: no-name-d54b32b9-2b9b-44e5-9e8f-adf87eb8c0ca\n",
      "[I 2024-02-24 02:17:21,536] Trial 0 finished with value: 0.21893491124260356 and parameters: {'loss': 'log_loss', 'penalty': 'l1', 'alpha': 5.397077366142353e-06, 'learning_rate': 'invscaling', 'eta0': 0.012154057015426751}. Best is trial 0 with value: 0.21893491124260356.\n",
      "[I 2024-02-24 02:17:25,654] Trial 1 finished with value: 0.21893491124260356 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 4.125115444756784e-06, 'learning_rate': 'invscaling', 'eta0': 0.00040753222638232397}. Best is trial 0 with value: 0.21893491124260356.\n",
      "[I 2024-02-24 02:17:30,979] Trial 2 finished with value: 0.6343195266272189 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 8.339938338594167e-05, 'learning_rate': 'adaptive', 'eta0': 0.030827234927732625}. Best is trial 2 with value: 0.6343195266272189.\n",
      "[I 2024-02-24 02:17:31,344] Trial 3 finished with value: 0.5289940828402366 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 0.0008793051761092773, 'learning_rate': 'constant', 'eta0': 4.621493431690658e-05}. Best is trial 2 with value: 0.6343195266272189.\n",
      "[I 2024-02-24 02:17:31,492] Trial 4 finished with value: 0.5976331360946746 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 1.4543914303955113e-06, 'learning_rate': 'invscaling', 'eta0': 0.0686440917272869}. Best is trial 2 with value: 0.6343195266272189.\n",
      "[I 2024-02-24 02:17:32,365] Trial 5 finished with value: 0.5408284023668639 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 0.0011979040571914207, 'learning_rate': 'optimal', 'eta0': 5.441068949560297e-05}. Best is trial 2 with value: 0.6343195266272189.\n",
      "[I 2024-02-24 02:17:33,710] Trial 6 finished with value: 0.6035502958579881 and parameters: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 7.754020392244619e-06, 'learning_rate': 'adaptive', 'eta0': 0.0009854259674513918}. Best is trial 2 with value: 0.6343195266272189.\n",
      "[I 2024-02-24 02:17:35,458] Trial 7 finished with value: 0.3479289940828402 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 0.006600075130613508, 'learning_rate': 'adaptive', 'eta0': 0.0008967228192267549}. Best is trial 2 with value: 0.6343195266272189.\n",
      "[I 2024-02-24 02:17:35,642] Trial 8 finished with value: 0.21893491124260356 and parameters: {'loss': 'log_loss', 'penalty': 'l2', 'alpha': 0.015012179872132138, 'learning_rate': 'invscaling', 'eta0': 3.9258916644254854e-05}. Best is trial 2 with value: 0.6343195266272189.\n",
      "[I 2024-02-24 02:17:36,193] Trial 9 finished with value: 0.5940828402366863 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 1.75124035686528e-05, 'learning_rate': 'adaptive', 'eta0': 0.0005321301146962189}. Best is trial 2 with value: 0.6343195266272189.\n",
      "[I 2024-02-24 02:17:40,721] Trial 10 finished with value: 0.6378698224852071 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 4.3548043725952286e-05, 'learning_rate': 'constant', 'eta0': 0.008264085265426907}. Best is trial 10 with value: 0.6378698224852071.\n",
      "[I 2024-02-24 02:17:45,017] Trial 11 finished with value: 0.6355029585798816 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 0.00010872346364688557, 'learning_rate': 'constant', 'eta0': 0.010159928570801766}. Best is trial 10 with value: 0.6378698224852071.\n",
      "[I 2024-02-24 02:17:51,210] Trial 12 finished with value: 0.6402366863905326 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 7.437708477578699e-05, 'learning_rate': 'constant', 'eta0': 0.005549573541449363}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-24 02:17:56,719] Trial 13 finished with value: 0.6402366863905324 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 7.125558555440648e-05, 'learning_rate': 'constant', 'eta0': 0.004735567757223871}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-24 02:18:04,711] Trial 14 finished with value: 0.6284023668639053 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 0.00031786464357312844, 'learning_rate': 'constant', 'eta0': 0.0036874153061294823}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-24 02:18:12,493] Trial 15 finished with value: 0.6047337278106509 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 0.0014124878122787864, 'learning_rate': 'constant', 'eta0': 0.003031949545935699}. Best is trial 12 with value: 0.6402366863905326.\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "[I 2024-02-24 02:18:37,853] Trial 16 finished with value: 0.3289940828402367 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 0.046225104990148015, 'learning_rate': 'optimal', 'eta0': 0.00017644080970614966}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-24 02:18:41,636] Trial 17 finished with value: 0.21893491124260356 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 0.00029898194884213434, 'learning_rate': 'constant', 'eta0': 1.0269951798953467e-05}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-24 02:18:42,368] Trial 18 finished with value: 0.21893491124260356 and parameters: {'loss': 'log_loss', 'penalty': 'l1', 'alpha': 4.2205743098340456e-05, 'learning_rate': 'constant', 'eta0': 0.0027452017301174574}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-24 02:18:46,531] Trial 19 finished with value: 0.6130177514792899 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 0.0002206031318841139, 'learning_rate': 'constant', 'eta0': 0.09239463894916515}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-24 02:18:52,222] Trial 20 finished with value: 0.5881656804733728 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 0.003615080552877013, 'learning_rate': 'optimal', 'eta0': 0.024609224990146915}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-24 02:18:59,653] Trial 21 finished with value: 0.6449704142011835 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 2.6436435375161252e-05, 'learning_rate': 'constant', 'eta0': 0.006488169283859697}. Best is trial 21 with value: 0.6449704142011835.\n",
      "[I 2024-02-24 02:19:05,525] Trial 22 finished with value: 0.6414201183431952 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 1.643424337598053e-05, 'learning_rate': 'constant', 'eta0': 0.005143115292816024}. Best is trial 21 with value: 0.6449704142011835.\n",
      "[I 2024-02-24 02:19:10,748] Trial 23 finished with value: 0.6272189349112426 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 2.2177108443847467e-05, 'learning_rate': 'constant', 'eta0': 0.0022388852594668585}. Best is trial 21 with value: 0.6449704142011835.\n",
      "[W 2024-02-24 02:19:10,840] Trial 24 failed with parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 1.50333898909903e-06, 'learning_rate': 'constant', 'eta0': 0.029123393068772888} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Temp\\ipykernel_17788\\3227327.py\", line 16, in objective\n",
      "    model.fit(X_train, y_train)\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\", line 917, in fit\n",
      "    return self._fit(\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\", line 704, in _fit\n",
      "    self._partial_fit(\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\", line 638, in _partial_fit\n",
      "    self._fit_multiclass(\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\", line 783, in _fit_multiclass\n",
      "    result = Parallel(\n",
      "             ^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\", line 463, in fit_binary\n",
      "    coef, intercept, average_coef, average_intercept, n_iter_ = _plain_sgd(\n",
      "                                                                ^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-02-24 02:19:10,848] Trial 24 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Create a study object and specify the optimization direction\u001B[39;00m\n\u001B[0;32m     24\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 25\u001B[0m \u001B[43mstudy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Adjust the number of trials as needed\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Best hyperparameters\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBest trial:\u001B[39m\u001B[38;5;124m'\u001B[39m, study\u001B[38;5;241m.\u001B[39mbest_trial\u001B[38;5;241m.\u001B[39mparams)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:451\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[0;32m    349\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    350\u001B[0m     func: ObjectiveFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    357\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    358\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    359\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[0;32m    360\u001B[0m \n\u001B[0;32m    361\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    449\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 451\u001B[0m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    452\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    453\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    454\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    456\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    458\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001B[0m, in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 66\u001B[0m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     79\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    160\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 163\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    244\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    247\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[0;32m    250\u001B[0m ):\n\u001B[1;32m--> 251\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[0;32m    252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[0;32m    199\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 200\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    202\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[0;32m    203\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[1;32mIn[22], line 16\u001B[0m, in \u001B[0;36mobjective\u001B[1;34m(trial)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Create and train the SGD Classifier\u001B[39;00m\n\u001B[0;32m     14\u001B[0m model \u001B[38;5;241m=\u001B[39m SGDClassifier(loss\u001B[38;5;241m=\u001B[39mloss, penalty\u001B[38;5;241m=\u001B[39mpenalty, alpha\u001B[38;5;241m=\u001B[39malpha, learning_rate\u001B[38;5;241m=\u001B[39mlearning_rate, eta0\u001B[38;5;241m=\u001B[39meta0,\n\u001B[0;32m     15\u001B[0m                       random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m---> 16\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Perform cross-validation and return the mean score\u001B[39;00m\n\u001B[0;32m     19\u001B[0m scores \u001B[38;5;241m=\u001B[39m cross_val_score(model, X_train, y_train, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)  \u001B[38;5;66;03m# cv=5 for 5-fold cross-validation\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1351\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1344\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1346\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1347\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1348\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1349\u001B[0m     )\n\u001B[0;32m   1350\u001B[0m ):\n\u001B[1;32m-> 1351\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:917\u001B[0m, in \u001B[0;36mBaseSGDClassifier.fit\u001B[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001B[0m\n\u001B[0;32m    888\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fit linear model with Stochastic Gradient Descent.\u001B[39;00m\n\u001B[0;32m    889\u001B[0m \n\u001B[0;32m    890\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    913\u001B[0m \u001B[38;5;124;03m    Returns an instance of self.\u001B[39;00m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    915\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_more_validate_params()\n\u001B[1;32m--> 917\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    919\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    920\u001B[0m \u001B[43m    \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    921\u001B[0m \u001B[43m    \u001B[49m\u001B[43mC\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    922\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    923\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    924\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoef_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoef_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    925\u001B[0m \u001B[43m    \u001B[49m\u001B[43mintercept_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mintercept_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    926\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:704\u001B[0m, in \u001B[0;36mBaseSGDClassifier._fit\u001B[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001B[0m\n\u001B[0;32m    701\u001B[0m \u001B[38;5;66;03m# Clear iteration count for multiple call to fit.\u001B[39;00m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mt_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m\n\u001B[1;32m--> 704\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_partial_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    705\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    706\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    707\u001B[0m \u001B[43m    \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    708\u001B[0m \u001B[43m    \u001B[49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    711\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    712\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclasses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    713\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    714\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoef_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    715\u001B[0m \u001B[43m    \u001B[49m\u001B[43mintercept_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    716\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    718\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    719\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtol \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    720\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtol \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39minf\n\u001B[0;32m    721\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_iter_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_iter\n\u001B[0;32m    722\u001B[0m ):\n\u001B[0;32m    723\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    724\u001B[0m         (\n\u001B[0;32m    725\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMaximum number of iteration reached before \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    729\u001B[0m         ConvergenceWarning,\n\u001B[0;32m    730\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:638\u001B[0m, in \u001B[0;36mBaseSGDClassifier._partial_fit\u001B[1;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001B[0m\n\u001B[0;32m    636\u001B[0m \u001B[38;5;66;03m# delegate to concrete training procedure\u001B[39;00m\n\u001B[0;32m    637\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_classes \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m--> 638\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_multiclass\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    639\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    640\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    641\u001B[0m \u001B[43m        \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    642\u001B[0m \u001B[43m        \u001B[49m\u001B[43mC\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    643\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    644\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    645\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    646\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    647\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m n_classes \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m    648\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_binary(\n\u001B[0;32m    649\u001B[0m         X,\n\u001B[0;32m    650\u001B[0m         y,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    655\u001B[0m         max_iter\u001B[38;5;241m=\u001B[39mmax_iter,\n\u001B[0;32m    656\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:783\u001B[0m, in \u001B[0;36mBaseSGDClassifier._fit_multiclass\u001B[1;34m(self, X, y, alpha, C, learning_rate, sample_weight, max_iter)\u001B[0m\n\u001B[0;32m    781\u001B[0m random_state \u001B[38;5;241m=\u001B[39m check_random_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state)\n\u001B[0;32m    782\u001B[0m seeds \u001B[38;5;241m=\u001B[39m random_state\u001B[38;5;241m.\u001B[39mrandint(MAX_INT, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_))\n\u001B[1;32m--> 783\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    784\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequire\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msharedmem\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m    785\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    786\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfit_binary\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    787\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    788\u001B[0m \u001B[43m        \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    789\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m        \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[43m        \u001B[49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    793\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    794\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_expanded_class_weight\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mseeds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;66;03m# take the maximum of n_iter_ over every binary fit\u001B[39;00m\n\u001B[0;32m    805\u001B[0m n_iter_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     66\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[0;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1790\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1792\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1793\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1794\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    127\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:463\u001B[0m, in \u001B[0;36mfit_binary\u001B[1;34m(est, i, X, y, alpha, C, learning_rate, max_iter, pos_weight, neg_weight, sample_weight, validation_mask, random_state)\u001B[0m\n\u001B[0;32m    460\u001B[0m tol \u001B[38;5;241m=\u001B[39m est\u001B[38;5;241m.\u001B[39mtol \u001B[38;5;28;01mif\u001B[39;00m est\u001B[38;5;241m.\u001B[39mtol \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39minf\n\u001B[0;32m    462\u001B[0m _plain_sgd \u001B[38;5;241m=\u001B[39m _get_plain_sgd_function(input_dtype\u001B[38;5;241m=\u001B[39mcoef\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m--> 463\u001B[0m coef, intercept, average_coef, average_intercept, n_iter_ \u001B[38;5;241m=\u001B[39m \u001B[43m_plain_sgd\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    464\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    465\u001B[0m \u001B[43m    \u001B[49m\u001B[43mintercept\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[43m    \u001B[49m\u001B[43maverage_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[43m    \u001B[49m\u001B[43maverage_intercept\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    468\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_loss_function_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    469\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpenalty_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    470\u001B[0m \u001B[43m    \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    471\u001B[0m \u001B[43m    \u001B[49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    472\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ml1_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    475\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mearly_stopping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    476\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_score_cb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_iter_no_change\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    478\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_intercept\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpos_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    485\u001B[0m \u001B[43m    \u001B[49m\u001B[43mneg_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    486\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    487\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meta0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpower_t\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mt_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m    \u001B[49m\u001B[43mintercept_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    495\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m est\u001B[38;5;241m.\u001B[39maverage:\n\u001B[0;32m    496\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(est\u001B[38;5;241m.\u001B[39mclasses_) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    loss = trial.suggest_categorical('loss', ['hinge', 'log_loss', 'squared_hinge', 'perceptron'])\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2', 'l1', 'elasticnet'])\n",
    "    alpha = trial.suggest_float('alpha', 1e-6, 1e-1, log=True)\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'optimal', 'invscaling', 'adaptive'])\n",
    "    eta0 = trial.suggest_float('eta0', 1e-5, 1e-1, log=True)  # Only relevant for certain learning rates\n",
    "\n",
    "    # Create and train the SGD Classifier\n",
    "    model = SGDClassifier(loss=loss, penalty=penalty, alpha=alpha, learning_rate=learning_rate, eta0=eta0,\n",
    "                          random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Perform cross-validation and return the mean score\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)  # cv=5 for 5-fold cross-validation\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "# Create a study object and specify the optimization direction\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)  # Adjust the number of trials as needed\n",
    "\n",
    "# Best hyperparameters\n",
    "print('Best trial:', study.best_trial.params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:19:11.874211400Z",
     "start_time": "2024-02-24T00:17:18.662871300Z"
    }
   },
   "id": "4477cdb859019617",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "We notice that the SGD model gave the best accuracy of 0.65, which is the highest among the models we have tried so far.\n",
    "\n",
    "So, we will get the best hyperparameters for the model and use them to train the model again, for later use."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f9ee6d9062ffe6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "sgd_model = SGDClassifier(**best_params, random_state=42)\n",
    "sgd_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "114f2bb29b45ae2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class Naive Bayes\n",
    "\n",
    "It is known that Naive Bayes is a simple and effective algorithm for classification for NLP tasks. \n",
    "Let's now train a multi-class Naive Bayes model on the embeddings and the class, to see how it performs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fab4ff2e2e09606"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.70      0.72        44\n",
      "           1       0.55      0.53      0.54        30\n",
      "           2       0.58      0.45      0.51        31\n",
      "           3       0.74      0.85      0.79        27\n",
      "           4       0.51      0.61      0.56        31\n",
      "           5       0.61      0.61      0.61        49\n",
      "\n",
      "    accuracy                           0.63       212\n",
      "   macro avg       0.62      0.63      0.62       212\n",
      "weighted avg       0.63      0.63      0.63       212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T00:02:04.765241900Z",
     "start_time": "2024-02-22T00:02:04.749418400Z"
    }
   },
   "id": "fa13a14098b3636c",
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the Naive Bayes model has an accuracy of 0.63, which does not differ much from the logistic regression model.\n",
    "So there is no need to use `optuna` to find the best hyperparameters for the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e859bc93b6dcd506"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class Random Forest\n",
    "\n",
    "Let's now train a multi-class Random Forest model on the embeddings and the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c9e62a3e6912a48"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.67        44\n",
      "           1       0.61      0.47      0.53        30\n",
      "           2       0.86      0.39      0.53        31\n",
      "           3       0.77      0.63      0.69        27\n",
      "           4       0.56      0.77      0.65        31\n",
      "           5       0.52      0.71      0.60        49\n",
      "\n",
      "    accuracy                           0.62       212\n",
      "   macro avg       0.67      0.61      0.61       212\n",
      "weighted avg       0.65      0.62      0.61       212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=15,\n",
    "                               max_depth=100, min_samples_split=2, min_samples_leaf=1, max_features='sqrt')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T23:59:20.973163800Z",
     "start_time": "2024-02-21T23:59:19.524753300Z"
    }
   },
   "id": "a8fc25a6a41a309b",
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class CatBoost\n",
    "\n",
    "Having tried the above models, let us now proceed to more advanced models such as CatBoost, \n",
    "which is a gradient boosting library that is known to perform well on tabular data, to see how it performs \n",
    "and if it can outperform the other models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28cda8d8bf460600"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.134538\n",
      "0:\tlearn: 1.7228745\ttotal: 25.9ms\tremaining: 10.3s\n",
      "1:\tlearn: 1.6604869\ttotal: 47.1ms\tremaining: 9.37s\n",
      "2:\tlearn: 1.6080402\ttotal: 68.8ms\tremaining: 9.11s\n",
      "3:\tlearn: 1.5585689\ttotal: 89.7ms\tremaining: 8.88s\n",
      "4:\tlearn: 1.5091478\ttotal: 110ms\tremaining: 8.72s\n",
      "5:\tlearn: 1.4662697\ttotal: 131ms\tremaining: 8.62s\n",
      "6:\tlearn: 1.4287979\ttotal: 152ms\tremaining: 8.52s\n",
      "7:\tlearn: 1.3910584\ttotal: 173ms\tremaining: 8.46s\n",
      "8:\tlearn: 1.3552419\ttotal: 194ms\tremaining: 8.41s\n",
      "9:\tlearn: 1.3165946\ttotal: 214ms\tremaining: 8.36s\n",
      "10:\tlearn: 1.2851901\ttotal: 235ms\tremaining: 8.32s\n",
      "11:\tlearn: 1.2504925\ttotal: 257ms\tremaining: 8.31s\n",
      "12:\tlearn: 1.2190597\ttotal: 279ms\tremaining: 8.31s\n",
      "13:\tlearn: 1.1924688\ttotal: 301ms\tremaining: 8.29s\n",
      "14:\tlearn: 1.1640413\ttotal: 323ms\tremaining: 8.3s\n",
      "15:\tlearn: 1.1362085\ttotal: 346ms\tremaining: 8.29s\n",
      "16:\tlearn: 1.1100424\ttotal: 367ms\tremaining: 8.27s\n",
      "17:\tlearn: 1.0878752\ttotal: 388ms\tremaining: 8.22s\n",
      "18:\tlearn: 1.0624523\ttotal: 408ms\tremaining: 8.19s\n",
      "19:\tlearn: 1.0358713\ttotal: 429ms\tremaining: 8.16s\n",
      "20:\tlearn: 1.0105685\ttotal: 451ms\tremaining: 8.14s\n",
      "21:\tlearn: 0.9889355\ttotal: 471ms\tremaining: 8.1s\n",
      "22:\tlearn: 0.9693468\ttotal: 492ms\tremaining: 8.07s\n",
      "23:\tlearn: 0.9472586\ttotal: 513ms\tremaining: 8.04s\n",
      "24:\tlearn: 0.9278074\ttotal: 535ms\tremaining: 8.02s\n",
      "25:\tlearn: 0.9062782\ttotal: 556ms\tremaining: 7.99s\n",
      "26:\tlearn: 0.8873076\ttotal: 576ms\tremaining: 7.96s\n",
      "27:\tlearn: 0.8661164\ttotal: 597ms\tremaining: 7.94s\n",
      "28:\tlearn: 0.8476778\ttotal: 619ms\tremaining: 7.92s\n",
      "29:\tlearn: 0.8313398\ttotal: 640ms\tremaining: 7.89s\n",
      "30:\tlearn: 0.8165745\ttotal: 660ms\tremaining: 7.85s\n",
      "31:\tlearn: 0.8015096\ttotal: 681ms\tremaining: 7.84s\n",
      "32:\tlearn: 0.7840770\ttotal: 704ms\tremaining: 7.82s\n",
      "33:\tlearn: 0.7662083\ttotal: 725ms\tremaining: 7.8s\n",
      "34:\tlearn: 0.7498120\ttotal: 746ms\tremaining: 7.78s\n",
      "35:\tlearn: 0.7370386\ttotal: 766ms\tremaining: 7.74s\n",
      "36:\tlearn: 0.7256692\ttotal: 787ms\tremaining: 7.72s\n",
      "37:\tlearn: 0.7139216\ttotal: 804ms\tremaining: 7.66s\n",
      "38:\tlearn: 0.6983130\ttotal: 826ms\tremaining: 7.64s\n",
      "39:\tlearn: 0.6876474\ttotal: 846ms\tremaining: 7.61s\n",
      "40:\tlearn: 0.6749084\ttotal: 867ms\tremaining: 7.59s\n",
      "41:\tlearn: 0.6631446\ttotal: 887ms\tremaining: 7.56s\n",
      "42:\tlearn: 0.6547389\ttotal: 907ms\tremaining: 7.53s\n",
      "43:\tlearn: 0.6416111\ttotal: 928ms\tremaining: 7.51s\n",
      "44:\tlearn: 0.6300260\ttotal: 950ms\tremaining: 7.49s\n",
      "45:\tlearn: 0.6227455\ttotal: 968ms\tremaining: 7.45s\n",
      "46:\tlearn: 0.6116992\ttotal: 985ms\tremaining: 7.4s\n",
      "47:\tlearn: 0.6017839\ttotal: 1s\tremaining: 7.37s\n",
      "48:\tlearn: 0.5913679\ttotal: 1.03s\tremaining: 7.36s\n",
      "49:\tlearn: 0.5811772\ttotal: 1.05s\tremaining: 7.33s\n",
      "50:\tlearn: 0.5744012\ttotal: 1.06s\tremaining: 7.29s\n",
      "51:\tlearn: 0.5661624\ttotal: 1.09s\tremaining: 7.27s\n",
      "52:\tlearn: 0.5583602\ttotal: 1.1s\tremaining: 7.23s\n",
      "53:\tlearn: 0.5498467\ttotal: 1.12s\tremaining: 7.21s\n",
      "54:\tlearn: 0.5410277\ttotal: 1.15s\tremaining: 7.19s\n",
      "55:\tlearn: 0.5344122\ttotal: 1.17s\tremaining: 7.16s\n",
      "56:\tlearn: 0.5256119\ttotal: 1.19s\tremaining: 7.14s\n",
      "57:\tlearn: 0.5168590\ttotal: 1.21s\tremaining: 7.11s\n",
      "58:\tlearn: 0.5090309\ttotal: 1.22s\tremaining: 7.08s\n",
      "59:\tlearn: 0.5007300\ttotal: 1.24s\tremaining: 7.05s\n",
      "60:\tlearn: 0.4930892\ttotal: 1.26s\tremaining: 7.02s\n",
      "61:\tlearn: 0.4862400\ttotal: 1.28s\tremaining: 7s\n",
      "62:\tlearn: 0.4796660\ttotal: 1.3s\tremaining: 6.97s\n",
      "63:\tlearn: 0.4744049\ttotal: 1.32s\tremaining: 6.93s\n",
      "64:\tlearn: 0.4672496\ttotal: 1.34s\tremaining: 6.91s\n",
      "65:\tlearn: 0.4599536\ttotal: 1.36s\tremaining: 6.9s\n",
      "66:\tlearn: 0.4526283\ttotal: 1.38s\tremaining: 6.88s\n",
      "67:\tlearn: 0.4464831\ttotal: 1.4s\tremaining: 6.85s\n",
      "68:\tlearn: 0.4403173\ttotal: 1.42s\tremaining: 6.83s\n",
      "69:\tlearn: 0.4353264\ttotal: 1.44s\tremaining: 6.79s\n",
      "70:\tlearn: 0.4292748\ttotal: 1.46s\tremaining: 6.77s\n",
      "71:\tlearn: 0.4231607\ttotal: 1.48s\tremaining: 6.74s\n",
      "72:\tlearn: 0.4173319\ttotal: 1.5s\tremaining: 6.71s\n",
      "73:\tlearn: 0.4120874\ttotal: 1.52s\tremaining: 6.69s\n",
      "74:\tlearn: 0.4075504\ttotal: 1.54s\tremaining: 6.66s\n",
      "75:\tlearn: 0.4036470\ttotal: 1.55s\tremaining: 6.63s\n",
      "76:\tlearn: 0.3971942\ttotal: 1.58s\tremaining: 6.61s\n",
      "77:\tlearn: 0.3923635\ttotal: 1.59s\tremaining: 6.59s\n",
      "78:\tlearn: 0.3871207\ttotal: 1.62s\tremaining: 6.57s\n",
      "79:\tlearn: 0.3824068\ttotal: 1.64s\tremaining: 6.54s\n",
      "80:\tlearn: 0.3781196\ttotal: 1.65s\tremaining: 6.52s\n",
      "81:\tlearn: 0.3720005\ttotal: 1.67s\tremaining: 6.49s\n",
      "82:\tlearn: 0.3662377\ttotal: 1.69s\tremaining: 6.47s\n",
      "83:\tlearn: 0.3621305\ttotal: 1.71s\tremaining: 6.44s\n",
      "84:\tlearn: 0.3580423\ttotal: 1.73s\tremaining: 6.41s\n",
      "85:\tlearn: 0.3542212\ttotal: 1.75s\tremaining: 6.39s\n",
      "86:\tlearn: 0.3493992\ttotal: 1.77s\tremaining: 6.37s\n",
      "87:\tlearn: 0.3446388\ttotal: 1.79s\tremaining: 6.35s\n",
      "88:\tlearn: 0.3398332\ttotal: 1.81s\tremaining: 6.33s\n",
      "89:\tlearn: 0.3350392\ttotal: 1.83s\tremaining: 6.31s\n",
      "90:\tlearn: 0.3310729\ttotal: 1.85s\tremaining: 6.29s\n",
      "91:\tlearn: 0.3277042\ttotal: 1.87s\tremaining: 6.25s\n",
      "92:\tlearn: 0.3239233\ttotal: 1.89s\tremaining: 6.23s\n",
      "93:\tlearn: 0.3200518\ttotal: 1.91s\tremaining: 6.21s\n",
      "94:\tlearn: 0.3159525\ttotal: 1.93s\tremaining: 6.19s\n",
      "95:\tlearn: 0.3117227\ttotal: 1.95s\tremaining: 6.17s\n",
      "96:\tlearn: 0.3078341\ttotal: 1.97s\tremaining: 6.14s\n",
      "97:\tlearn: 0.3037615\ttotal: 1.99s\tremaining: 6.13s\n",
      "98:\tlearn: 0.3009603\ttotal: 2.01s\tremaining: 6.11s\n",
      "99:\tlearn: 0.2972008\ttotal: 2.03s\tremaining: 6.08s\n",
      "100:\tlearn: 0.2940256\ttotal: 2.04s\tremaining: 6.05s\n",
      "101:\tlearn: 0.2912744\ttotal: 2.06s\tremaining: 6.03s\n",
      "102:\tlearn: 0.2874840\ttotal: 2.08s\tremaining: 6s\n",
      "103:\tlearn: 0.2851025\ttotal: 2.1s\tremaining: 5.97s\n",
      "104:\tlearn: 0.2817369\ttotal: 2.12s\tremaining: 5.96s\n",
      "105:\tlearn: 0.2787950\ttotal: 2.14s\tremaining: 5.93s\n",
      "106:\tlearn: 0.2753153\ttotal: 2.16s\tremaining: 5.91s\n",
      "107:\tlearn: 0.2726117\ttotal: 2.18s\tremaining: 5.89s\n",
      "108:\tlearn: 0.2700277\ttotal: 2.2s\tremaining: 5.86s\n",
      "109:\tlearn: 0.2671003\ttotal: 2.22s\tremaining: 5.84s\n",
      "110:\tlearn: 0.2641920\ttotal: 2.24s\tremaining: 5.82s\n",
      "111:\tlearn: 0.2622460\ttotal: 2.25s\tremaining: 5.8s\n",
      "112:\tlearn: 0.2597229\ttotal: 2.27s\tremaining: 5.77s\n",
      "113:\tlearn: 0.2567605\ttotal: 2.29s\tremaining: 5.75s\n",
      "114:\tlearn: 0.2538125\ttotal: 2.31s\tremaining: 5.74s\n",
      "115:\tlearn: 0.2512457\ttotal: 2.33s\tremaining: 5.72s\n",
      "116:\tlearn: 0.2493987\ttotal: 2.35s\tremaining: 5.69s\n",
      "117:\tlearn: 0.2470901\ttotal: 2.37s\tremaining: 5.67s\n",
      "118:\tlearn: 0.2436367\ttotal: 2.39s\tremaining: 5.64s\n",
      "119:\tlearn: 0.2412490\ttotal: 2.41s\tremaining: 5.62s\n",
      "120:\tlearn: 0.2382601\ttotal: 2.43s\tremaining: 5.6s\n",
      "121:\tlearn: 0.2358525\ttotal: 2.45s\tremaining: 5.57s\n",
      "122:\tlearn: 0.2335635\ttotal: 2.46s\tremaining: 5.55s\n",
      "123:\tlearn: 0.2313625\ttotal: 2.48s\tremaining: 5.53s\n",
      "124:\tlearn: 0.2298685\ttotal: 2.5s\tremaining: 5.5s\n",
      "125:\tlearn: 0.2276978\ttotal: 2.52s\tremaining: 5.48s\n",
      "126:\tlearn: 0.2262614\ttotal: 2.54s\tremaining: 5.45s\n",
      "127:\tlearn: 0.2237244\ttotal: 2.55s\tremaining: 5.43s\n",
      "128:\tlearn: 0.2216352\ttotal: 2.57s\tremaining: 5.41s\n",
      "129:\tlearn: 0.2196897\ttotal: 2.59s\tremaining: 5.38s\n",
      "130:\tlearn: 0.2182188\ttotal: 2.61s\tremaining: 5.36s\n",
      "131:\tlearn: 0.2159438\ttotal: 2.63s\tremaining: 5.33s\n",
      "132:\tlearn: 0.2135043\ttotal: 2.65s\tremaining: 5.31s\n",
      "133:\tlearn: 0.2115320\ttotal: 2.67s\tremaining: 5.3s\n",
      "134:\tlearn: 0.2094532\ttotal: 2.69s\tremaining: 5.27s\n",
      "135:\tlearn: 0.2079651\ttotal: 2.71s\tremaining: 5.25s\n",
      "136:\tlearn: 0.2062597\ttotal: 2.72s\tremaining: 5.23s\n",
      "137:\tlearn: 0.2040880\ttotal: 2.74s\tremaining: 5.21s\n",
      "138:\tlearn: 0.2021655\ttotal: 2.76s\tremaining: 5.18s\n",
      "139:\tlearn: 0.2006026\ttotal: 2.77s\tremaining: 5.16s\n",
      "140:\tlearn: 0.1985812\ttotal: 2.79s\tremaining: 5.13s\n",
      "141:\tlearn: 0.1965836\ttotal: 2.81s\tremaining: 5.11s\n",
      "142:\tlearn: 0.1948308\ttotal: 2.83s\tremaining: 5.08s\n",
      "143:\tlearn: 0.1927285\ttotal: 2.85s\tremaining: 5.07s\n",
      "144:\tlearn: 0.1907884\ttotal: 2.87s\tremaining: 5.05s\n",
      "145:\tlearn: 0.1892524\ttotal: 2.89s\tremaining: 5.03s\n",
      "146:\tlearn: 0.1876610\ttotal: 2.91s\tremaining: 5.01s\n",
      "147:\tlearn: 0.1858838\ttotal: 2.93s\tremaining: 4.99s\n",
      "148:\tlearn: 0.1840987\ttotal: 2.95s\tremaining: 4.96s\n",
      "149:\tlearn: 0.1826783\ttotal: 2.96s\tremaining: 4.94s\n",
      "150:\tlearn: 0.1809587\ttotal: 2.98s\tremaining: 4.92s\n",
      "151:\tlearn: 0.1793601\ttotal: 3s\tremaining: 4.9s\n",
      "152:\tlearn: 0.1783546\ttotal: 3.02s\tremaining: 4.87s\n",
      "153:\tlearn: 0.1774691\ttotal: 3.04s\tremaining: 4.85s\n",
      "154:\tlearn: 0.1758001\ttotal: 3.05s\tremaining: 4.83s\n",
      "155:\tlearn: 0.1744380\ttotal: 3.07s\tremaining: 4.81s\n",
      "156:\tlearn: 0.1731060\ttotal: 3.09s\tremaining: 4.79s\n",
      "157:\tlearn: 0.1715699\ttotal: 3.11s\tremaining: 4.77s\n",
      "158:\tlearn: 0.1696787\ttotal: 3.13s\tremaining: 4.75s\n",
      "159:\tlearn: 0.1682388\ttotal: 3.15s\tremaining: 4.73s\n",
      "160:\tlearn: 0.1673331\ttotal: 3.17s\tremaining: 4.7s\n",
      "161:\tlearn: 0.1659585\ttotal: 3.19s\tremaining: 4.68s\n",
      "162:\tlearn: 0.1643622\ttotal: 3.2s\tremaining: 4.66s\n",
      "163:\tlearn: 0.1636660\ttotal: 3.22s\tremaining: 4.63s\n",
      "164:\tlearn: 0.1621060\ttotal: 3.24s\tremaining: 4.61s\n",
      "165:\tlearn: 0.1607724\ttotal: 3.26s\tremaining: 4.59s\n",
      "166:\tlearn: 0.1594912\ttotal: 3.27s\tremaining: 4.57s\n",
      "167:\tlearn: 0.1576651\ttotal: 3.3s\tremaining: 4.55s\n",
      "168:\tlearn: 0.1567447\ttotal: 3.31s\tremaining: 4.53s\n",
      "169:\tlearn: 0.1552850\ttotal: 3.33s\tremaining: 4.51s\n",
      "170:\tlearn: 0.1543368\ttotal: 3.35s\tremaining: 4.49s\n",
      "171:\tlearn: 0.1526851\ttotal: 3.37s\tremaining: 4.46s\n",
      "172:\tlearn: 0.1516548\ttotal: 3.39s\tremaining: 4.44s\n",
      "173:\tlearn: 0.1503451\ttotal: 3.4s\tremaining: 4.42s\n",
      "174:\tlearn: 0.1489448\ttotal: 3.42s\tremaining: 4.4s\n",
      "175:\tlearn: 0.1479130\ttotal: 3.44s\tremaining: 4.38s\n",
      "176:\tlearn: 0.1468269\ttotal: 3.46s\tremaining: 4.35s\n",
      "177:\tlearn: 0.1457061\ttotal: 3.48s\tremaining: 4.33s\n",
      "178:\tlearn: 0.1447622\ttotal: 3.49s\tremaining: 4.31s\n",
      "179:\tlearn: 0.1438747\ttotal: 3.51s\tremaining: 4.29s\n",
      "180:\tlearn: 0.1430759\ttotal: 3.53s\tremaining: 4.27s\n",
      "181:\tlearn: 0.1418740\ttotal: 3.55s\tremaining: 4.25s\n",
      "182:\tlearn: 0.1406074\ttotal: 3.57s\tremaining: 4.23s\n",
      "183:\tlearn: 0.1393027\ttotal: 3.59s\tremaining: 4.21s\n",
      "184:\tlearn: 0.1384271\ttotal: 3.61s\tremaining: 4.19s\n",
      "185:\tlearn: 0.1379717\ttotal: 3.62s\tremaining: 4.17s\n",
      "186:\tlearn: 0.1372064\ttotal: 3.64s\tremaining: 4.15s\n",
      "187:\tlearn: 0.1363515\ttotal: 3.66s\tremaining: 4.13s\n",
      "188:\tlearn: 0.1353696\ttotal: 3.68s\tremaining: 4.1s\n",
      "189:\tlearn: 0.1345634\ttotal: 3.69s\tremaining: 4.08s\n",
      "190:\tlearn: 0.1334954\ttotal: 3.71s\tremaining: 4.06s\n",
      "191:\tlearn: 0.1325677\ttotal: 3.73s\tremaining: 4.04s\n",
      "192:\tlearn: 0.1315723\ttotal: 3.75s\tremaining: 4.02s\n",
      "193:\tlearn: 0.1309626\ttotal: 3.76s\tremaining: 4s\n",
      "194:\tlearn: 0.1300535\ttotal: 3.78s\tremaining: 3.98s\n",
      "195:\tlearn: 0.1288905\ttotal: 3.8s\tremaining: 3.96s\n",
      "196:\tlearn: 0.1277924\ttotal: 3.82s\tremaining: 3.94s\n",
      "197:\tlearn: 0.1265409\ttotal: 3.84s\tremaining: 3.92s\n",
      "198:\tlearn: 0.1255739\ttotal: 3.86s\tremaining: 3.9s\n",
      "199:\tlearn: 0.1248684\ttotal: 3.88s\tremaining: 3.88s\n",
      "200:\tlearn: 0.1241837\ttotal: 3.89s\tremaining: 3.86s\n",
      "201:\tlearn: 0.1232042\ttotal: 3.91s\tremaining: 3.83s\n",
      "202:\tlearn: 0.1222603\ttotal: 3.93s\tremaining: 3.81s\n",
      "203:\tlearn: 0.1213459\ttotal: 3.95s\tremaining: 3.79s\n",
      "204:\tlearn: 0.1205817\ttotal: 3.97s\tremaining: 3.78s\n",
      "205:\tlearn: 0.1198251\ttotal: 3.99s\tremaining: 3.76s\n",
      "206:\tlearn: 0.1189689\ttotal: 4.01s\tremaining: 3.74s\n",
      "207:\tlearn: 0.1181024\ttotal: 4.03s\tremaining: 3.72s\n",
      "208:\tlearn: 0.1172190\ttotal: 4.04s\tremaining: 3.69s\n",
      "209:\tlearn: 0.1166187\ttotal: 4.06s\tremaining: 3.67s\n",
      "210:\tlearn: 0.1158746\ttotal: 4.08s\tremaining: 3.65s\n",
      "211:\tlearn: 0.1153167\ttotal: 4.1s\tremaining: 3.63s\n",
      "212:\tlearn: 0.1148056\ttotal: 4.11s\tremaining: 3.61s\n",
      "213:\tlearn: 0.1139654\ttotal: 4.13s\tremaining: 3.59s\n",
      "214:\tlearn: 0.1133325\ttotal: 4.15s\tremaining: 3.57s\n",
      "215:\tlearn: 0.1127448\ttotal: 4.17s\tremaining: 3.55s\n",
      "216:\tlearn: 0.1118552\ttotal: 4.19s\tremaining: 3.53s\n",
      "217:\tlearn: 0.1109735\ttotal: 4.21s\tremaining: 3.51s\n",
      "218:\tlearn: 0.1101663\ttotal: 4.23s\tremaining: 3.49s\n",
      "219:\tlearn: 0.1094057\ttotal: 4.25s\tremaining: 3.47s\n",
      "220:\tlearn: 0.1086842\ttotal: 4.27s\tremaining: 3.46s\n",
      "221:\tlearn: 0.1081635\ttotal: 4.29s\tremaining: 3.44s\n",
      "222:\tlearn: 0.1074615\ttotal: 4.3s\tremaining: 3.42s\n",
      "223:\tlearn: 0.1067836\ttotal: 4.32s\tremaining: 3.4s\n",
      "224:\tlearn: 0.1061558\ttotal: 4.34s\tremaining: 3.38s\n",
      "225:\tlearn: 0.1054296\ttotal: 4.36s\tremaining: 3.36s\n",
      "226:\tlearn: 0.1047350\ttotal: 4.38s\tremaining: 3.34s\n",
      "227:\tlearn: 0.1040358\ttotal: 4.4s\tremaining: 3.32s\n",
      "228:\tlearn: 0.1033799\ttotal: 4.41s\tremaining: 3.3s\n",
      "229:\tlearn: 0.1028751\ttotal: 4.43s\tremaining: 3.28s\n",
      "230:\tlearn: 0.1021686\ttotal: 4.45s\tremaining: 3.26s\n",
      "231:\tlearn: 0.1014326\ttotal: 4.47s\tremaining: 3.23s\n",
      "232:\tlearn: 0.1009049\ttotal: 4.49s\tremaining: 3.21s\n",
      "233:\tlearn: 0.1003008\ttotal: 4.5s\tremaining: 3.19s\n",
      "234:\tlearn: 0.0999129\ttotal: 4.52s\tremaining: 3.17s\n",
      "235:\tlearn: 0.0993210\ttotal: 4.54s\tremaining: 3.15s\n",
      "236:\tlearn: 0.0988379\ttotal: 4.56s\tremaining: 3.13s\n",
      "237:\tlearn: 0.0982119\ttotal: 4.58s\tremaining: 3.11s\n",
      "238:\tlearn: 0.0976786\ttotal: 4.59s\tremaining: 3.1s\n",
      "239:\tlearn: 0.0970612\ttotal: 4.61s\tremaining: 3.08s\n",
      "240:\tlearn: 0.0966120\ttotal: 4.63s\tremaining: 3.06s\n",
      "241:\tlearn: 0.0959420\ttotal: 4.65s\tremaining: 3.04s\n",
      "242:\tlearn: 0.0953684\ttotal: 4.67s\tremaining: 3.02s\n",
      "243:\tlearn: 0.0947231\ttotal: 4.69s\tremaining: 3s\n",
      "244:\tlearn: 0.0942221\ttotal: 4.7s\tremaining: 2.98s\n",
      "245:\tlearn: 0.0936252\ttotal: 4.72s\tremaining: 2.96s\n",
      "246:\tlearn: 0.0930829\ttotal: 4.74s\tremaining: 2.94s\n",
      "247:\tlearn: 0.0926261\ttotal: 4.76s\tremaining: 2.92s\n",
      "248:\tlearn: 0.0920309\ttotal: 4.78s\tremaining: 2.9s\n",
      "249:\tlearn: 0.0913719\ttotal: 4.79s\tremaining: 2.88s\n",
      "250:\tlearn: 0.0908123\ttotal: 4.81s\tremaining: 2.86s\n",
      "251:\tlearn: 0.0901615\ttotal: 4.83s\tremaining: 2.84s\n",
      "252:\tlearn: 0.0895904\ttotal: 4.85s\tremaining: 2.82s\n",
      "253:\tlearn: 0.0889963\ttotal: 4.87s\tremaining: 2.8s\n",
      "254:\tlearn: 0.0884139\ttotal: 4.89s\tremaining: 2.78s\n",
      "255:\tlearn: 0.0880217\ttotal: 4.91s\tremaining: 2.76s\n",
      "256:\tlearn: 0.0875964\ttotal: 4.92s\tremaining: 2.74s\n",
      "257:\tlearn: 0.0870218\ttotal: 4.94s\tremaining: 2.72s\n",
      "258:\tlearn: 0.0865999\ttotal: 4.96s\tremaining: 2.7s\n",
      "259:\tlearn: 0.0860540\ttotal: 4.98s\tremaining: 2.68s\n",
      "260:\tlearn: 0.0853690\ttotal: 5s\tremaining: 2.66s\n",
      "261:\tlearn: 0.0847425\ttotal: 5.02s\tremaining: 2.64s\n",
      "262:\tlearn: 0.0841182\ttotal: 5.03s\tremaining: 2.62s\n",
      "263:\tlearn: 0.0835568\ttotal: 5.05s\tremaining: 2.6s\n",
      "264:\tlearn: 0.0830860\ttotal: 5.08s\tremaining: 2.58s\n",
      "265:\tlearn: 0.0826953\ttotal: 5.09s\tremaining: 2.56s\n",
      "266:\tlearn: 0.0821504\ttotal: 5.11s\tremaining: 2.54s\n",
      "267:\tlearn: 0.0816057\ttotal: 5.13s\tremaining: 2.53s\n",
      "268:\tlearn: 0.0811668\ttotal: 5.15s\tremaining: 2.51s\n",
      "269:\tlearn: 0.0806370\ttotal: 5.17s\tremaining: 2.49s\n",
      "270:\tlearn: 0.0803147\ttotal: 5.18s\tremaining: 2.47s\n",
      "271:\tlearn: 0.0799147\ttotal: 5.2s\tremaining: 2.45s\n",
      "272:\tlearn: 0.0796684\ttotal: 5.22s\tremaining: 2.43s\n",
      "273:\tlearn: 0.0791950\ttotal: 5.24s\tremaining: 2.41s\n",
      "274:\tlearn: 0.0787248\ttotal: 5.26s\tremaining: 2.39s\n",
      "275:\tlearn: 0.0782419\ttotal: 5.28s\tremaining: 2.37s\n",
      "276:\tlearn: 0.0780074\ttotal: 5.29s\tremaining: 2.35s\n",
      "277:\tlearn: 0.0775958\ttotal: 5.31s\tremaining: 2.33s\n",
      "278:\tlearn: 0.0771180\ttotal: 5.33s\tremaining: 2.31s\n",
      "279:\tlearn: 0.0767341\ttotal: 5.35s\tremaining: 2.29s\n",
      "280:\tlearn: 0.0763011\ttotal: 5.37s\tremaining: 2.27s\n",
      "281:\tlearn: 0.0758194\ttotal: 5.38s\tremaining: 2.25s\n",
      "282:\tlearn: 0.0754358\ttotal: 5.4s\tremaining: 2.23s\n",
      "283:\tlearn: 0.0750667\ttotal: 5.42s\tremaining: 2.21s\n",
      "284:\tlearn: 0.0747107\ttotal: 5.44s\tremaining: 2.19s\n",
      "285:\tlearn: 0.0742531\ttotal: 5.46s\tremaining: 2.17s\n",
      "286:\tlearn: 0.0737926\ttotal: 5.48s\tremaining: 2.16s\n",
      "287:\tlearn: 0.0735905\ttotal: 5.49s\tremaining: 2.14s\n",
      "288:\tlearn: 0.0732208\ttotal: 5.51s\tremaining: 2.12s\n",
      "289:\tlearn: 0.0727946\ttotal: 5.53s\tremaining: 2.1s\n",
      "290:\tlearn: 0.0724306\ttotal: 5.55s\tremaining: 2.08s\n",
      "291:\tlearn: 0.0720317\ttotal: 5.57s\tremaining: 2.06s\n",
      "292:\tlearn: 0.0716278\ttotal: 5.58s\tremaining: 2.04s\n",
      "293:\tlearn: 0.0712362\ttotal: 5.6s\tremaining: 2.02s\n",
      "294:\tlearn: 0.0707410\ttotal: 5.62s\tremaining: 2s\n",
      "295:\tlearn: 0.0704118\ttotal: 5.64s\tremaining: 1.98s\n",
      "296:\tlearn: 0.0700204\ttotal: 5.66s\tremaining: 1.96s\n",
      "297:\tlearn: 0.0696531\ttotal: 5.68s\tremaining: 1.94s\n",
      "298:\tlearn: 0.0692932\ttotal: 5.7s\tremaining: 1.92s\n",
      "299:\tlearn: 0.0689641\ttotal: 5.72s\tremaining: 1.91s\n",
      "300:\tlearn: 0.0687036\ttotal: 5.74s\tremaining: 1.89s\n",
      "301:\tlearn: 0.0682465\ttotal: 5.75s\tremaining: 1.87s\n",
      "302:\tlearn: 0.0679109\ttotal: 5.77s\tremaining: 1.85s\n",
      "303:\tlearn: 0.0675415\ttotal: 5.79s\tremaining: 1.83s\n",
      "304:\tlearn: 0.0671175\ttotal: 5.81s\tremaining: 1.81s\n",
      "305:\tlearn: 0.0668401\ttotal: 5.83s\tremaining: 1.79s\n",
      "306:\tlearn: 0.0665337\ttotal: 5.84s\tremaining: 1.77s\n",
      "307:\tlearn: 0.0662038\ttotal: 5.86s\tremaining: 1.75s\n",
      "308:\tlearn: 0.0659299\ttotal: 5.88s\tremaining: 1.73s\n",
      "309:\tlearn: 0.0656950\ttotal: 5.9s\tremaining: 1.71s\n",
      "310:\tlearn: 0.0653329\ttotal: 5.92s\tremaining: 1.69s\n",
      "311:\tlearn: 0.0650142\ttotal: 5.93s\tremaining: 1.67s\n",
      "312:\tlearn: 0.0647410\ttotal: 5.95s\tremaining: 1.65s\n",
      "313:\tlearn: 0.0644334\ttotal: 5.97s\tremaining: 1.63s\n",
      "314:\tlearn: 0.0641182\ttotal: 5.99s\tremaining: 1.61s\n",
      "315:\tlearn: 0.0638325\ttotal: 6s\tremaining: 1.6s\n",
      "316:\tlearn: 0.0635122\ttotal: 6.02s\tremaining: 1.58s\n",
      "317:\tlearn: 0.0631894\ttotal: 6.04s\tremaining: 1.56s\n",
      "318:\tlearn: 0.0628921\ttotal: 6.06s\tremaining: 1.54s\n",
      "319:\tlearn: 0.0625070\ttotal: 6.08s\tremaining: 1.52s\n",
      "320:\tlearn: 0.0621223\ttotal: 6.1s\tremaining: 1.5s\n",
      "321:\tlearn: 0.0619032\ttotal: 6.11s\tremaining: 1.48s\n",
      "322:\tlearn: 0.0615653\ttotal: 6.13s\tremaining: 1.46s\n",
      "323:\tlearn: 0.0613404\ttotal: 6.15s\tremaining: 1.44s\n",
      "324:\tlearn: 0.0611619\ttotal: 6.17s\tremaining: 1.42s\n",
      "325:\tlearn: 0.0608453\ttotal: 6.19s\tremaining: 1.4s\n",
      "326:\tlearn: 0.0605806\ttotal: 6.2s\tremaining: 1.39s\n",
      "327:\tlearn: 0.0603465\ttotal: 6.22s\tremaining: 1.36s\n",
      "328:\tlearn: 0.0600413\ttotal: 6.24s\tremaining: 1.35s\n",
      "329:\tlearn: 0.0597975\ttotal: 6.26s\tremaining: 1.33s\n",
      "330:\tlearn: 0.0595617\ttotal: 6.28s\tremaining: 1.31s\n",
      "331:\tlearn: 0.0593571\ttotal: 6.29s\tremaining: 1.29s\n",
      "332:\tlearn: 0.0591896\ttotal: 6.31s\tremaining: 1.27s\n",
      "333:\tlearn: 0.0589029\ttotal: 6.33s\tremaining: 1.25s\n",
      "334:\tlearn: 0.0586641\ttotal: 6.35s\tremaining: 1.23s\n",
      "335:\tlearn: 0.0584050\ttotal: 6.37s\tremaining: 1.21s\n",
      "336:\tlearn: 0.0581356\ttotal: 6.38s\tremaining: 1.19s\n",
      "337:\tlearn: 0.0579161\ttotal: 6.4s\tremaining: 1.17s\n",
      "338:\tlearn: 0.0575914\ttotal: 6.42s\tremaining: 1.16s\n",
      "339:\tlearn: 0.0574563\ttotal: 6.44s\tremaining: 1.14s\n",
      "340:\tlearn: 0.0571344\ttotal: 6.46s\tremaining: 1.12s\n",
      "341:\tlearn: 0.0568670\ttotal: 6.47s\tremaining: 1.1s\n",
      "342:\tlearn: 0.0566196\ttotal: 6.49s\tremaining: 1.08s\n",
      "343:\tlearn: 0.0563139\ttotal: 6.51s\tremaining: 1.06s\n",
      "344:\tlearn: 0.0560689\ttotal: 6.53s\tremaining: 1.04s\n",
      "345:\tlearn: 0.0557970\ttotal: 6.55s\tremaining: 1.02s\n",
      "346:\tlearn: 0.0555744\ttotal: 6.57s\tremaining: 1s\n",
      "347:\tlearn: 0.0553663\ttotal: 6.58s\tremaining: 984ms\n",
      "348:\tlearn: 0.0550987\ttotal: 6.61s\tremaining: 965ms\n",
      "349:\tlearn: 0.0548580\ttotal: 6.63s\tremaining: 947ms\n",
      "350:\tlearn: 0.0546271\ttotal: 6.64s\tremaining: 928ms\n",
      "351:\tlearn: 0.0544627\ttotal: 6.66s\tremaining: 909ms\n",
      "352:\tlearn: 0.0542735\ttotal: 6.68s\tremaining: 890ms\n",
      "353:\tlearn: 0.0540566\ttotal: 6.7s\tremaining: 871ms\n",
      "354:\tlearn: 0.0537670\ttotal: 6.72s\tremaining: 852ms\n",
      "355:\tlearn: 0.0534931\ttotal: 6.74s\tremaining: 833ms\n",
      "356:\tlearn: 0.0532458\ttotal: 6.76s\tremaining: 814ms\n",
      "357:\tlearn: 0.0530554\ttotal: 6.78s\tremaining: 795ms\n",
      "358:\tlearn: 0.0528663\ttotal: 6.8s\tremaining: 777ms\n",
      "359:\tlearn: 0.0525924\ttotal: 6.82s\tremaining: 757ms\n",
      "360:\tlearn: 0.0523859\ttotal: 6.83s\tremaining: 738ms\n",
      "361:\tlearn: 0.0521683\ttotal: 6.85s\tremaining: 719ms\n",
      "362:\tlearn: 0.0519234\ttotal: 6.87s\tremaining: 700ms\n",
      "363:\tlearn: 0.0516844\ttotal: 6.89s\tremaining: 681ms\n",
      "364:\tlearn: 0.0515064\ttotal: 6.9s\tremaining: 662ms\n",
      "365:\tlearn: 0.0512855\ttotal: 6.92s\tremaining: 643ms\n",
      "366:\tlearn: 0.0511138\ttotal: 6.94s\tremaining: 624ms\n",
      "367:\tlearn: 0.0508781\ttotal: 6.96s\tremaining: 605ms\n",
      "368:\tlearn: 0.0506872\ttotal: 6.97s\tremaining: 586ms\n",
      "369:\tlearn: 0.0504707\ttotal: 6.99s\tremaining: 567ms\n",
      "370:\tlearn: 0.0502115\ttotal: 7.01s\tremaining: 548ms\n",
      "371:\tlearn: 0.0500338\ttotal: 7.03s\tremaining: 529ms\n",
      "372:\tlearn: 0.0498045\ttotal: 7.04s\tremaining: 510ms\n",
      "373:\tlearn: 0.0495770\ttotal: 7.07s\tremaining: 491ms\n",
      "374:\tlearn: 0.0494049\ttotal: 7.08s\tremaining: 472ms\n",
      "375:\tlearn: 0.0491638\ttotal: 7.11s\tremaining: 454ms\n",
      "376:\tlearn: 0.0489267\ttotal: 7.12s\tremaining: 435ms\n",
      "377:\tlearn: 0.0487657\ttotal: 7.14s\tremaining: 416ms\n",
      "378:\tlearn: 0.0485915\ttotal: 7.16s\tremaining: 397ms\n",
      "379:\tlearn: 0.0484395\ttotal: 7.18s\tremaining: 378ms\n",
      "380:\tlearn: 0.0482877\ttotal: 7.2s\tremaining: 359ms\n",
      "381:\tlearn: 0.0480858\ttotal: 7.21s\tremaining: 340ms\n",
      "382:\tlearn: 0.0478687\ttotal: 7.24s\tremaining: 321ms\n",
      "383:\tlearn: 0.0476853\ttotal: 7.25s\tremaining: 302ms\n",
      "384:\tlearn: 0.0474993\ttotal: 7.27s\tremaining: 283ms\n",
      "385:\tlearn: 0.0472943\ttotal: 7.29s\tremaining: 264ms\n",
      "386:\tlearn: 0.0470829\ttotal: 7.31s\tremaining: 246ms\n",
      "387:\tlearn: 0.0468742\ttotal: 7.33s\tremaining: 227ms\n",
      "388:\tlearn: 0.0466857\ttotal: 7.35s\tremaining: 208ms\n",
      "389:\tlearn: 0.0465460\ttotal: 7.37s\tremaining: 189ms\n",
      "390:\tlearn: 0.0463967\ttotal: 7.38s\tremaining: 170ms\n",
      "391:\tlearn: 0.0462451\ttotal: 7.4s\tremaining: 151ms\n",
      "392:\tlearn: 0.0460232\ttotal: 7.42s\tremaining: 132ms\n",
      "393:\tlearn: 0.0458569\ttotal: 7.44s\tremaining: 113ms\n",
      "394:\tlearn: 0.0455861\ttotal: 7.46s\tremaining: 94.4ms\n",
      "395:\tlearn: 0.0454279\ttotal: 7.47s\tremaining: 75.5ms\n",
      "396:\tlearn: 0.0452607\ttotal: 7.49s\tremaining: 56.6ms\n",
      "397:\tlearn: 0.0450495\ttotal: 7.51s\tremaining: 37.7ms\n",
      "398:\tlearn: 0.0449415\ttotal: 7.53s\tremaining: 18.9ms\n",
      "399:\tlearn: 0.0447709\ttotal: 7.55s\tremaining: 0us\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.61      0.63        44\n",
      "           1       0.56      0.60      0.58        30\n",
      "           2       0.64      0.52      0.57        31\n",
      "           3       0.65      0.63      0.64        27\n",
      "           4       0.57      0.74      0.65        31\n",
      "           5       0.64      0.61      0.62        49\n",
      "\n",
      "    accuracy                           0.62       212\n",
      "   macro avg       0.62      0.62      0.62       212\n",
      "weighted avg       0.62      0.62      0.62       212\n"
     ]
    }
   ],
   "source": [
    "import catboost as cb\n",
    "\n",
    "model = cb.CatBoostClassifier(iterations=500, task_type='GPU')\n",
    "model.fit(X_train, y_train)\n",
    "# Generate classification report\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T00:01:44.601009500Z",
     "start_time": "2024-02-22T00:01:36.225891400Z"
    }
   },
   "id": "c14f291177ba9793",
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "source": [
    "Very interesting to note is that the Catboost model gave the lowest accuracy of 0.62,\n",
    "which is lower than the other models we have tried so far, \n",
    "meaning that the more complex models do not always outperform the simpler ones."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66b66ab411e286e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network\n",
    "\n",
    "While neural networks need a lot of data to perform well, let's try a simple neural network model to see how it performs.\n",
    "\n",
    "We will use the `keras` library to build the neural network model.\n",
    "\n",
    "Let's start by making the train and test splits from above into TensorFlow datasets.\n",
    "\n",
    "We will also define a validation dataset from the training data to monitor the model's performance during training.\n",
    "All the tensors will be batched and pre-fetched to improve performance and also cached to avoid loading the data from the disk during training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be15e3c101722f61"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 600  # Adjust based on your dataset size\n",
    "VALIDATION_SPLIT = 0.2  # Fraction of training data to use for validation\n",
    "\n",
    "# Calculate the number of validation samples\n",
    "n_validation_samples = int(len(X_train) * VALIDATION_SPLIT)\n",
    "\n",
    "# Create and process the validation dataset from the training data\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_train[:n_validation_samples], y_train[:n_validation_samples]))\n",
    "val_ds = val_ds.batch(BATCH_SIZE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Update the training dataset to exclude the validation data and process it\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train[n_validation_samples:], y_train[n_validation_samples:]))\n",
    "train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Create and process the test dataset\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_ds = test_ds.batch(BATCH_SIZE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:21:03.997451400Z",
     "start_time": "2024-02-24T00:21:03.972444200Z"
    }
   },
   "id": "fac050077fcb120",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that done, let's have a look at the first batch of the training dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1bc0a932557566"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [[ 0.01441193  0.01679993 -0.03109741 ...  0.01856995 -0.08996582\n",
      "   0.03582764]\n",
      " [ 0.04104614  0.00654221 -0.02853394 ...  0.00632477 -0.04309082\n",
      "  -0.00440216]\n",
      " [ 0.00843811 -0.01992798 -0.02073669 ...  0.02677917 -0.03234863\n",
      "  -0.03442383]\n",
      " ...\n",
      " [ 0.00078201 -0.00634003 -0.01808167 ...  0.00569534 -0.04315186\n",
      "   0.00452042]\n",
      " [-0.02398682  0.04241943 -0.0280304  ...  0.03460693 -0.0682373\n",
      "   0.00652695]\n",
      " [ 0.03005981 -0.01261139 -0.03665161 ...  0.03747559 -0.04443359\n",
      "  -0.00604248]], Target: [2 5 0 5 5 1 1 5 0 4 0 5 0 0 3 1 3 0 2 2 3 2 4 0 5 1 3 4 3 0 5 0]\n"
     ]
    }
   ],
   "source": [
    "for feat, targ in train_ds.take(1):\n",
    "    print('Features: {}, Target: {}'.format(feat, targ))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:21:04.265283600Z",
     "start_time": "2024-02-24T00:21:04.252424800Z"
    }
   },
   "id": "28b6cd6e52c06ff9",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the training dataset is structured as expected, with the features and the targets batched together with a size of 32."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2200f63d62e00072"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's define an early stopping callback to stop training the model when the validation loss does not improve after a certain number of epochs. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55faaf898903bf36"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    patience=30,  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:21:04.723347Z",
     "start_time": "2024-02-24T00:21:04.717835100Z"
    }
   },
   "id": "a2a85456f6f0206a",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another useful callback method in TensorFlow is the ModelCheckpoint callback. This callback saves the model at regular intervals during training, allowing you to retain and load the best version of your model based on a certain monitored metric, such as validation loss or accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58e3c995a8f8d943"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model_{epoch:02d}-{val_loss:.2f}.keras',  # Change the extension to '.keras'\n",
    "    save_weights_only=False,  # Set to True to save only weights, False to save the entire model\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    mode='min',  # The model is saved when the monitored metric stops decreasing\n",
    "    save_best_only=True,  # Only the best model is saved\n",
    "    verbose=1  # Verbosity mode, 1 or 0\n",
    ")\n",
    "\n",
    "CALLBACKS = [\n",
    "    tfdocs.modeling.EpochDots(),\n",
    "    early_stopping_callback,\n",
    "    model_checkpoint_callback\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:21:05.030638900Z",
     "start_time": "2024-02-24T00:21:05.026128500Z"
    }
   },
   "id": "e99e3b0058d27c4b",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly, let's define a scheduler callback to adjust the learning rate during training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e70949e4096c0a1f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "STEPS_PER_EPOCH = X_train.shape[0] // BATCH_SIZE\n",
    "lr_scheduler = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    initial_learning_rate=1e-3,  # Initial learning rate\n",
    "    decay_steps=STEPS_PER_EPOCH * 1000,  # Decay steps\n",
    "    decay_rate=0.9,  # Decay rate\n",
    "    staircase=True  # If True, decay the learning rate at discrete intervals\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:21:05.355806100Z",
     "start_time": "2024-02-24T00:21:05.350293700Z"
    }
   },
   "id": "b24cce175aac2630",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "With all the callbacks and the learning rate scheduler defined, let's now build the neural network model.\n",
    "\n",
    "We will use a simple neural network with two hidden layers, each with 64 units, and a ReLU activation function.\n",
    "\n",
    "Also, we will use the Adam optimizer and the SparseCategoricalCrossentropy loss function, as well as the accuracy metric.\n",
    "\n",
    "Lastly, we will add WeightRegularization to the model as well as DropoutLayers to prevent overfitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "396c45887ea4ddcd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import keras \n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "from keras import regularizers\n",
    "\n",
    "weight_regularizer = regularizers.l2(0.0001)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=weight_regularizer),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(16, activation='relu', kernel_regularizer=weight_regularizer),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(6, activation='softmax')\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:21:05.715571800Z",
     "start_time": "2024-02-24T00:21:05.686759300Z"
    }
   },
   "id": "ed010b4c406e22d7",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the model created, let's compile it using the Adam optimizer, the SparseCategoricalCrossentropy loss function, and the accuracy metric."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9600efc4cd8cdafb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=(['accuracy',\n",
    "                        tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                            from_logits=False, name='sparse_categorical_crossentropy')]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:21:06.022734700Z",
     "start_time": "2024-02-24T00:21:06.015395900Z"
    }
   },
   "id": "3dc26bb0fade8741",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see now the model summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d19eb4b55cb1da4d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 32)                24608     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25238 (98.59 KB)\n",
      "Trainable params: 25238 (98.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:21:06.415246Z",
     "start_time": "2024-02-24T00:21:06.397734700Z"
    }
   },
   "id": "7794956153aea89d",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the model has 24608 parameters to train, which are quite a lot for the small dataset we have.\n",
    "However, with the Dropout layers and the WeightRegularization, we hope to prevent excessive overfitting.\n",
    "\n",
    "Let's now train the model using the training dataset and the validation dataset we created earlier."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "689f9ac49e7e4324"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1/22 [>.............................] - ETA: 7s - loss: 1.7970 - accuracy: 0.2812 - sparse_categorical_crossentropy: 1.7888\n",
      "Epoch: 0, accuracy:0.1879,  loss:1.7952,  sparse_categorical_crossentropy:1.7863,  val_accuracy:0.2249,  val_loss:1.7883,  val_sparse_categorical_crossentropy:1.7767,  \n",
      ".\n",
      "Epoch 1: val_loss improved from inf to 1.78831, saving model to model_01-1.79.keras\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1.7952 - accuracy: 0.1879 - sparse_categorical_crossentropy: 1.7863 - val_loss: 1.7883 - val_accuracy: 0.2249 - val_sparse_categorical_crossentropy: 1.7767\n",
      "Epoch 2/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7606 - accuracy: 0.4375 - sparse_categorical_crossentropy: 1.7526.\n",
      "Epoch 2: val_loss improved from 1.78831 to 1.76768, saving model to model_02-1.77.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7698 - accuracy: 0.2485 - sparse_categorical_crossentropy: 1.7560 - val_loss: 1.7677 - val_accuracy: 0.2899 - val_sparse_categorical_crossentropy: 1.7576\n",
      "Epoch 3/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7280 - accuracy: 0.2812 - sparse_categorical_crossentropy: 1.7200.\n",
      "Epoch 3: val_loss improved from 1.76768 to 1.74539, saving model to model_03-1.75.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7455 - accuracy: 0.2811 - sparse_categorical_crossentropy: 1.7346 - val_loss: 1.7454 - val_accuracy: 0.3550 - val_sparse_categorical_crossentropy: 1.7346\n",
      "Epoch 4/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.6743 - accuracy: 0.4062 - sparse_categorical_crossentropy: 1.6664.\n",
      "Epoch 4: val_loss improved from 1.74539 to 1.72217, saving model to model_04-1.72.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7119 - accuracy: 0.3077 - sparse_categorical_crossentropy: 1.7004 - val_loss: 1.7222 - val_accuracy: 0.3728 - val_sparse_categorical_crossentropy: 1.7108\n",
      "Epoch 5/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7034 - accuracy: 0.2812 - sparse_categorical_crossentropy: 1.6953.\n",
      "Epoch 5: val_loss improved from 1.72217 to 1.69211, saving model to model_05-1.69.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6825 - accuracy: 0.3388 - sparse_categorical_crossentropy: 1.6656 - val_loss: 1.6921 - val_accuracy: 0.3846 - val_sparse_categorical_crossentropy: 1.6815\n",
      "Epoch 6/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.6473 - accuracy: 0.4375 - sparse_categorical_crossentropy: 1.6391.\n",
      "Epoch 6: val_loss improved from 1.69211 to 1.66660, saving model to model_06-1.67.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6465 - accuracy: 0.3772 - sparse_categorical_crossentropy: 1.6251 - val_loss: 1.6666 - val_accuracy: 0.3905 - val_sparse_categorical_crossentropy: 1.6510\n",
      "Epoch 7/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.6155 - accuracy: 0.3750 - sparse_categorical_crossentropy: 1.6071.\n",
      "Epoch 7: val_loss improved from 1.66660 to 1.64540, saving model to model_07-1.65.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6056 - accuracy: 0.3846 - sparse_categorical_crossentropy: 1.5909 - val_loss: 1.6454 - val_accuracy: 0.4497 - val_sparse_categorical_crossentropy: 1.6327\n",
      "Epoch 8/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.5516 - accuracy: 0.4688 - sparse_categorical_crossentropy: 1.5429.\n",
      "Epoch 8: val_loss improved from 1.64540 to 1.62147, saving model to model_08-1.62.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5645 - accuracy: 0.4453 - sparse_categorical_crossentropy: 1.5483 - val_loss: 1.6215 - val_accuracy: 0.4320 - val_sparse_categorical_crossentropy: 1.6037\n",
      "Epoch 9/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.4637 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.4546.\n",
      "Epoch 9: val_loss improved from 1.62147 to 1.60048, saving model to model_09-1.60.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5040 - accuracy: 0.4719 - sparse_categorical_crossentropy: 1.4945 - val_loss: 1.6005 - val_accuracy: 0.4438 - val_sparse_categorical_crossentropy: 1.5816\n",
      "Epoch 10/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.5262 - accuracy: 0.4062 - sparse_categorical_crossentropy: 1.5167.\n",
      "Epoch 10: val_loss improved from 1.60048 to 1.55870, saving model to model_10-1.56.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4942 - accuracy: 0.4379 - sparse_categorical_crossentropy: 1.4698 - val_loss: 1.5587 - val_accuracy: 0.4556 - val_sparse_categorical_crossentropy: 1.5402\n",
      "Epoch 11/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.4961 - accuracy: 0.4062 - sparse_categorical_crossentropy: 1.4862.\n",
      "Epoch 11: val_loss improved from 1.55870 to 1.53080, saving model to model_11-1.53.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4275 - accuracy: 0.4808 - sparse_categorical_crossentropy: 1.4117 - val_loss: 1.5308 - val_accuracy: 0.4675 - val_sparse_categorical_crossentropy: 1.5056\n",
      "Epoch 12/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.4337 - accuracy: 0.4688 - sparse_categorical_crossentropy: 1.4232.\n",
      "Epoch 12: val_loss improved from 1.53080 to 1.49169, saving model to model_12-1.49.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3884 - accuracy: 0.5089 - sparse_categorical_crossentropy: 1.3589 - val_loss: 1.4917 - val_accuracy: 0.4675 - val_sparse_categorical_crossentropy: 1.4632\n",
      "Epoch 13/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.4618 - accuracy: 0.4688 - sparse_categorical_crossentropy: 1.4507.\n",
      "Epoch 13: val_loss improved from 1.49169 to 1.45628, saving model to model_13-1.46.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3254 - accuracy: 0.5340 - sparse_categorical_crossentropy: 1.2978 - val_loss: 1.4563 - val_accuracy: 0.4675 - val_sparse_categorical_crossentropy: 1.4291\n",
      "Epoch 14/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2865 - accuracy: 0.5312 - sparse_categorical_crossentropy: 1.2748.\n",
      "Epoch 14: val_loss improved from 1.45628 to 1.43289, saving model to model_14-1.43.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3291 - accuracy: 0.4985 - sparse_categorical_crossentropy: 1.3056 - val_loss: 1.4329 - val_accuracy: 0.4852 - val_sparse_categorical_crossentropy: 1.3999\n",
      "Epoch 15/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2671 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.2548.\n",
      "Epoch 15: val_loss improved from 1.43289 to 1.40060, saving model to model_15-1.40.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2816 - accuracy: 0.5133 - sparse_categorical_crossentropy: 1.2643 - val_loss: 1.4006 - val_accuracy: 0.5030 - val_sparse_categorical_crossentropy: 1.3682\n",
      "Epoch 16/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2720 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.2591.\n",
      "Epoch 16: val_loss improved from 1.40060 to 1.39223, saving model to model_16-1.39.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2488 - accuracy: 0.5473 - sparse_categorical_crossentropy: 1.2136 - val_loss: 1.3922 - val_accuracy: 0.4911 - val_sparse_categorical_crossentropy: 1.3558\n",
      "Epoch 17/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2584 - accuracy: 0.4688 - sparse_categorical_crossentropy: 1.2450.\n",
      "Epoch 17: val_loss improved from 1.39223 to 1.35359, saving model to model_17-1.35.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2003 - accuracy: 0.5740 - sparse_categorical_crossentropy: 1.1668 - val_loss: 1.3536 - val_accuracy: 0.5325 - val_sparse_categorical_crossentropy: 1.3151\n",
      "Epoch 18/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1784 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.1644.\n",
      "Epoch 18: val_loss improved from 1.35359 to 1.34083, saving model to model_18-1.34.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1702 - accuracy: 0.5695 - sparse_categorical_crossentropy: 1.1318 - val_loss: 1.3408 - val_accuracy: 0.5266 - val_sparse_categorical_crossentropy: 1.3001\n",
      "Epoch 19/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2478 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.2333.\n",
      "Epoch 19: val_loss improved from 1.34083 to 1.33327, saving model to model_19-1.33.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1597 - accuracy: 0.5947 - sparse_categorical_crossentropy: 1.1242 - val_loss: 1.3333 - val_accuracy: 0.5325 - val_sparse_categorical_crossentropy: 1.2914\n",
      "Epoch 20/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2102 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.1951.\n",
      "Epoch 20: val_loss did not improve from 1.33327\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.1226 - accuracy: 0.5976 - sparse_categorical_crossentropy: 1.0873 - val_loss: 1.3412 - val_accuracy: 0.5089 - val_sparse_categorical_crossentropy: 1.3011\n",
      "Epoch 21/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0727 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.0571.\n",
      "Epoch 21: val_loss improved from 1.33327 to 1.29693, saving model to model_21-1.30.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0887 - accuracy: 0.6124 - sparse_categorical_crossentropy: 1.0464 - val_loss: 1.2969 - val_accuracy: 0.5385 - val_sparse_categorical_crossentropy: 1.2481\n",
      "Epoch 22/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1348 - accuracy: 0.5625 - sparse_categorical_crossentropy: 1.1187.\n",
      "Epoch 22: val_loss improved from 1.29693 to 1.29140, saving model to model_22-1.29.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0536 - accuracy: 0.6391 - sparse_categorical_crossentropy: 1.0178 - val_loss: 1.2914 - val_accuracy: 0.5325 - val_sparse_categorical_crossentropy: 1.2493\n",
      "Epoch 23/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0956 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.0789.\n",
      "Epoch 23: val_loss improved from 1.29140 to 1.25679, saving model to model_23-1.26.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0776 - accuracy: 0.6139 - sparse_categorical_crossentropy: 1.0402 - val_loss: 1.2568 - val_accuracy: 0.5858 - val_sparse_categorical_crossentropy: 1.2071\n",
      "Epoch 24/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0718 - accuracy: 0.7500 - sparse_categorical_crossentropy: 1.0546.\n",
      "Epoch 24: val_loss improved from 1.25679 to 1.25532, saving model to model_24-1.26.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0267 - accuracy: 0.6464 - sparse_categorical_crossentropy: 1.0097 - val_loss: 1.2553 - val_accuracy: 0.5858 - val_sparse_categorical_crossentropy: 1.2087\n",
      "Epoch 25/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0832 - accuracy: 0.5938 - sparse_categorical_crossentropy: 1.0655.\n",
      "Epoch 25: val_loss improved from 1.25532 to 1.24624, saving model to model_25-1.25.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9857 - accuracy: 0.6864 - sparse_categorical_crossentropy: 0.9478 - val_loss: 1.2462 - val_accuracy: 0.5680 - val_sparse_categorical_crossentropy: 1.1998\n",
      "Epoch 26/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0637 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.0455.\n",
      "Epoch 26: val_loss improved from 1.24624 to 1.21800, saving model to model_26-1.22.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9554 - accuracy: 0.6746 - sparse_categorical_crossentropy: 0.9228 - val_loss: 1.2180 - val_accuracy: 0.6036 - val_sparse_categorical_crossentropy: 1.1633\n",
      "Epoch 27/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9664 - accuracy: 0.6250 - sparse_categorical_crossentropy: 0.9476.\n",
      "Epoch 27: val_loss did not improve from 1.21800\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9931 - accuracy: 0.6538 - sparse_categorical_crossentropy: 0.9513 - val_loss: 1.2481 - val_accuracy: 0.5740 - val_sparse_categorical_crossentropy: 1.2047\n",
      "Epoch 28/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0892 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.0698.\n",
      "Epoch 28: val_loss did not improve from 1.21800\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9303 - accuracy: 0.7130 - sparse_categorical_crossentropy: 0.8909 - val_loss: 1.2289 - val_accuracy: 0.6036 - val_sparse_categorical_crossentropy: 1.1801\n",
      "Epoch 29/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8808 - accuracy: 0.6875 - sparse_categorical_crossentropy: 0.8610.\n",
      "Epoch 29: val_loss improved from 1.21800 to 1.20826, saving model to model_29-1.21.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9593 - accuracy: 0.6760 - sparse_categorical_crossentropy: 0.9172 - val_loss: 1.2083 - val_accuracy: 0.6095 - val_sparse_categorical_crossentropy: 1.1569\n",
      "Epoch 30/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9338 - accuracy: 0.6562 - sparse_categorical_crossentropy: 0.9134.\n",
      "Epoch 30: val_loss did not improve from 1.20826\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9049 - accuracy: 0.7071 - sparse_categorical_crossentropy: 0.8664 - val_loss: 1.2131 - val_accuracy: 0.6036 - val_sparse_categorical_crossentropy: 1.1615\n",
      "Epoch 31/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0846 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.0638.\n",
      "Epoch 31: val_loss improved from 1.20826 to 1.20104, saving model to model_31-1.20.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9054 - accuracy: 0.7086 - sparse_categorical_crossentropy: 0.8545 - val_loss: 1.2010 - val_accuracy: 0.5976 - val_sparse_categorical_crossentropy: 1.1441\n",
      "Epoch 32/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1454 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.1241.\n",
      "Epoch 32: val_loss improved from 1.20104 to 1.18157, saving model to model_32-1.18.keras\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8736 - accuracy: 0.7041 - sparse_categorical_crossentropy: 0.8374 - val_loss: 1.1816 - val_accuracy: 0.6213 - val_sparse_categorical_crossentropy: 1.1238\n",
      "Epoch 33/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0234 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.0016.\n",
      "Epoch 33: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8943 - accuracy: 0.7130 - sparse_categorical_crossentropy: 0.8621 - val_loss: 1.2076 - val_accuracy: 0.6095 - val_sparse_categorical_crossentropy: 1.1530\n",
      "Epoch 34/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8948 - accuracy: 0.6562 - sparse_categorical_crossentropy: 0.8726.\n",
      "Epoch 34: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8922 - accuracy: 0.7086 - sparse_categorical_crossentropy: 0.8446 - val_loss: 1.2031 - val_accuracy: 0.5976 - val_sparse_categorical_crossentropy: 1.1508\n",
      "Epoch 35/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8003 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.7776.\n",
      "Epoch 35: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8350 - accuracy: 0.7263 - sparse_categorical_crossentropy: 0.7974 - val_loss: 1.1855 - val_accuracy: 0.6154 - val_sparse_categorical_crossentropy: 1.1328\n",
      "Epoch 36/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7784 - accuracy: 0.8125 - sparse_categorical_crossentropy: 0.7552.\n",
      "Epoch 36: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8097 - accuracy: 0.7411 - sparse_categorical_crossentropy: 0.7746 - val_loss: 1.1955 - val_accuracy: 0.5917 - val_sparse_categorical_crossentropy: 1.1433\n",
      "Epoch 37/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7699 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.7462.\n",
      "Epoch 37: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7877 - accuracy: 0.7515 - sparse_categorical_crossentropy: 0.7405 - val_loss: 1.1911 - val_accuracy: 0.5976 - val_sparse_categorical_crossentropy: 1.1322\n",
      "Epoch 38/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8884 - accuracy: 0.6875 - sparse_categorical_crossentropy: 0.8643.\n",
      "Epoch 38: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7948 - accuracy: 0.7544 - sparse_categorical_crossentropy: 0.7493 - val_loss: 1.1937 - val_accuracy: 0.5740 - val_sparse_categorical_crossentropy: 1.1360\n",
      "Epoch 39/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8566 - accuracy: 0.6562 - sparse_categorical_crossentropy: 0.8320.\n",
      "Epoch 39: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8027 - accuracy: 0.7263 - sparse_categorical_crossentropy: 0.7574 - val_loss: 1.1982 - val_accuracy: 0.5799 - val_sparse_categorical_crossentropy: 1.1453\n",
      "Epoch 40/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7335 - accuracy: 0.8125 - sparse_categorical_crossentropy: 0.7085.\n",
      "Epoch 40: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7435 - accuracy: 0.7589 - sparse_categorical_crossentropy: 0.7123 - val_loss: 1.1920 - val_accuracy: 0.5976 - val_sparse_categorical_crossentropy: 1.1343\n",
      "Epoch 41/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9764 - accuracy: 0.7188 - sparse_categorical_crossentropy: 0.9509.\n",
      "Epoch 41: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7566 - accuracy: 0.7633 - sparse_categorical_crossentropy: 0.7111 - val_loss: 1.1898 - val_accuracy: 0.5858 - val_sparse_categorical_crossentropy: 1.1336\n",
      "Epoch 42/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9218 - accuracy: 0.7188 - sparse_categorical_crossentropy: 0.8959.\n",
      "Epoch 42: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7588 - accuracy: 0.7559 - sparse_categorical_crossentropy: 0.7109 - val_loss: 1.1991 - val_accuracy: 0.5799 - val_sparse_categorical_crossentropy: 1.1421\n",
      "Epoch 43/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7918 - accuracy: 0.7188 - sparse_categorical_crossentropy: 0.7655.\n",
      "Epoch 43: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7023 - accuracy: 0.7722 - sparse_categorical_crossentropy: 0.6879 - val_loss: 1.2059 - val_accuracy: 0.5799 - val_sparse_categorical_crossentropy: 1.1442\n",
      "Epoch 44/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6243 - accuracy: 0.8438 - sparse_categorical_crossentropy: 0.5975.\n",
      "Epoch 44: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6803 - accuracy: 0.7796 - sparse_categorical_crossentropy: 0.6471 - val_loss: 1.1999 - val_accuracy: 0.5799 - val_sparse_categorical_crossentropy: 1.1366\n",
      "Epoch 45/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5902 - accuracy: 0.8438 - sparse_categorical_crossentropy: 0.5630.\n",
      "Epoch 45: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6605 - accuracy: 0.7988 - sparse_categorical_crossentropy: 0.6206 - val_loss: 1.2048 - val_accuracy: 0.6095 - val_sparse_categorical_crossentropy: 1.1437\n",
      "Epoch 46/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6756 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.6480.\n",
      "Epoch 46: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6863 - accuracy: 0.7973 - sparse_categorical_crossentropy: 0.6451 - val_loss: 1.2161 - val_accuracy: 0.5740 - val_sparse_categorical_crossentropy: 1.1576\n",
      "Epoch 47/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6501 - accuracy: 0.8438 - sparse_categorical_crossentropy: 0.6221.\n",
      "Epoch 47: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6647 - accuracy: 0.8062 - sparse_categorical_crossentropy: 0.6244 - val_loss: 1.2015 - val_accuracy: 0.5740 - val_sparse_categorical_crossentropy: 1.1425\n",
      "Epoch 48/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7738 - accuracy: 0.7812 - sparse_categorical_crossentropy: 0.7453.\n",
      "Epoch 48: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6791 - accuracy: 0.7692 - sparse_categorical_crossentropy: 0.6361 - val_loss: 1.1868 - val_accuracy: 0.6036 - val_sparse_categorical_crossentropy: 1.1215\n",
      "Epoch 49/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7648 - accuracy: 0.6875 - sparse_categorical_crossentropy: 0.7359.\n",
      "Epoch 49: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6536 - accuracy: 0.8062 - sparse_categorical_crossentropy: 0.6069 - val_loss: 1.2007 - val_accuracy: 0.5799 - val_sparse_categorical_crossentropy: 1.1333\n",
      "Epoch 50/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6341 - accuracy: 0.8125 - sparse_categorical_crossentropy: 0.6048.\n",
      "Epoch 50: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6310 - accuracy: 0.8136 - sparse_categorical_crossentropy: 0.5842 - val_loss: 1.2334 - val_accuracy: 0.5740 - val_sparse_categorical_crossentropy: 1.1758\n",
      "Epoch 51/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6784 - accuracy: 0.8750 - sparse_categorical_crossentropy: 0.6487.\n",
      "Epoch 51: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6575 - accuracy: 0.7959 - sparse_categorical_crossentropy: 0.6103 - val_loss: 1.2214 - val_accuracy: 0.5680 - val_sparse_categorical_crossentropy: 1.1571\n",
      "Epoch 52/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6536 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.6235.\n",
      "Epoch 52: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5965 - accuracy: 0.8062 - sparse_categorical_crossentropy: 0.5482 - val_loss: 1.2318 - val_accuracy: 0.5917 - val_sparse_categorical_crossentropy: 1.1675\n",
      "Epoch 53/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7592 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.7287.\n",
      "Epoch 53: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5998 - accuracy: 0.8195 - sparse_categorical_crossentropy: 0.5533 - val_loss: 1.2141 - val_accuracy: 0.5976 - val_sparse_categorical_crossentropy: 1.1467\n",
      "Epoch 54/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6270 - accuracy: 0.9375 - sparse_categorical_crossentropy: 0.5961.\n",
      "Epoch 54: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6151 - accuracy: 0.8240 - sparse_categorical_crossentropy: 0.5796 - val_loss: 1.2473 - val_accuracy: 0.5976 - val_sparse_categorical_crossentropy: 1.1800\n",
      "Epoch 55/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7430 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.7117.\n",
      "Epoch 55: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6089 - accuracy: 0.8284 - sparse_categorical_crossentropy: 0.5690 - val_loss: 1.2350 - val_accuracy: 0.5621 - val_sparse_categorical_crossentropy: 1.1648\n",
      "Epoch 56/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7870 - accuracy: 0.7812 - sparse_categorical_crossentropy: 0.7552.\n",
      "Epoch 56: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5982 - accuracy: 0.8107 - sparse_categorical_crossentropy: 0.5525 - val_loss: 1.2401 - val_accuracy: 0.5799 - val_sparse_categorical_crossentropy: 1.1665\n",
      "Epoch 57/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5944 - accuracy: 0.7812 - sparse_categorical_crossentropy: 0.5623.\n",
      "Epoch 57: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6148 - accuracy: 0.8077 - sparse_categorical_crossentropy: 0.5615 - val_loss: 1.2452 - val_accuracy: 0.5740 - val_sparse_categorical_crossentropy: 1.1710\n",
      "Epoch 58/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6822 - accuracy: 0.7812 - sparse_categorical_crossentropy: 0.6497.\n",
      "Epoch 58: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5746 - accuracy: 0.8358 - sparse_categorical_crossentropy: 0.5268 - val_loss: 1.2735 - val_accuracy: 0.5621 - val_sparse_categorical_crossentropy: 1.1954\n",
      "Epoch 59/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5412 - accuracy: 0.8438 - sparse_categorical_crossentropy: 0.5083.\n",
      "Epoch 59: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5724 - accuracy: 0.8136 - sparse_categorical_crossentropy: 0.5253 - val_loss: 1.2515 - val_accuracy: 0.5740 - val_sparse_categorical_crossentropy: 1.1713\n",
      "Epoch 60/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6353 - accuracy: 0.7812 - sparse_categorical_crossentropy: 0.6021.\n",
      "Epoch 60: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5513 - accuracy: 0.8417 - sparse_categorical_crossentropy: 0.5035 - val_loss: 1.2515 - val_accuracy: 0.6036 - val_sparse_categorical_crossentropy: 1.1694\n",
      "Epoch 61/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6050 - accuracy: 0.8438 - sparse_categorical_crossentropy: 0.5714.\n",
      "Epoch 61: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5665 - accuracy: 0.8343 - sparse_categorical_crossentropy: 0.5280 - val_loss: 1.2732 - val_accuracy: 0.5621 - val_sparse_categorical_crossentropy: 1.1952\n",
      "Epoch 62/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5537 - accuracy: 0.8438 - sparse_categorical_crossentropy: 0.5197.\n",
      "Epoch 62: val_loss did not improve from 1.18157\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5236 - accuracy: 0.8565 - sparse_categorical_crossentropy: 0.4717 - val_loss: 1.3111 - val_accuracy: 0.5680 - val_sparse_categorical_crossentropy: 1.2354\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=100, batch_size=BATCH_SIZE, validation_data=val_ds, callbacks=CALLBACKS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:21:09.821035Z",
     "start_time": "2024-02-24T00:21:06.709876Z"
    }
   },
   "id": "18398cf390d74222",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now using tfdocs.plots.HistoryPlotter, we can visualize the training and validation loss and accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc642328f72f6966"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "size_histories = {}\n",
    "size_histories['Baseline'] = history\n",
    "plotter = tfdocs.plots.HistoryPlotter(metric='sparse_categorical_crossentropy', smoothing_std=10)\n",
    "plotter.plot(size_histories)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:21:09.901764100Z",
     "start_time": "2024-02-24T00:21:09.821035Z"
    }
   },
   "id": "a7a2ea0769fdc279",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the model the validation loss becomes stagnant after  about 40 epochs.\n",
    "\n",
    "Let's now evaluate the model on the test dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a97fe8c00c1e28e0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 834us/step - loss: 1.0923 - accuracy: 0.6651 - sparse_categorical_crossentropy: 1.0560\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1.0922902822494507, 0.6650943160057068, 1.0559929609298706]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds, verbose=1, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:21:09.923456Z",
     "start_time": "2024-02-24T00:21:09.901764100Z"
    }
   },
   "id": "d52c2208aed74b8b",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also generate a classification report for the model along with the confusion matrix."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb4f229959c3287d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 667us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.70        44\n",
      "           1       0.63      0.63      0.63        30\n",
      "           2       0.82      0.58      0.68        31\n",
      "           3       0.65      0.63      0.64        27\n",
      "           4       0.64      0.74      0.69        31\n",
      "           5       0.65      0.63      0.64        49\n",
      "\n",
      "    accuracy                           0.67       212\n",
      "   macro avg       0.68      0.66      0.66       212\n",
      "weighted avg       0.67      0.67      0.66       212\n",
      "\n",
      "[[33  2  1  2  0  6]\n",
      " [ 3 19  2  1  3  2]\n",
      " [ 3  4 18  0  3  3]\n",
      " [ 4  1  0 17  2  3]\n",
      " [ 1  2  0  2 23  3]\n",
      " [ 6  2  1  4  5 31]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(test_ds)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T00:21:19.370968200Z",
     "start_time": "2024-02-24T00:21:19.293456500Z"
    }
   },
   "id": "73a947990a3f8705",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the model has an accuracy of 0.67, which is not bad considering the small dataset we have.\n",
    "Notable are the precision and recall scores for class 3, which are 0.67 and 0.30, respectively, showing that the model is not performing well on this class\n",
    "in comparison to the other classes. This class is about 'WORDS RELATING TO THE INTELLECTUAL FACULTIES', whose words may be more challenging to classify\n",
    "due to the philosophical nature of the words.\n",
    "\n",
    "However, we can see that the model is overfitting the training data, as the validation loss becomes higher than the training loss quickly,\n",
    "which makes sense given the small dataset we have.\n",
    "\n",
    "Ironically, the simpler models such as logistic regression and SGD outperformed the neural network model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ab41048404b3e7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Two-Level Classification\n",
    "\n",
    "As we saw from before, we got the best results using the SGD Classifier.\n",
    "\n",
    "Let's now try to perform a two-level classification, where we first classify the class of the word and then classify the division/section of the word based on the class it belongs to.\n",
    "\n",
    "Firstly, we will create a new column in the dataframe named division/section, \n",
    "where it has as value the division if the division is not 4 (meaning `N/A` in our case),\n",
    " otherwise it has the value of the section.\n",
    "\n",
    "Then we will split the data into training and testing sets, using 80% of the data for training and 20% for testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cece51585462464"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# For the cls_df dataframe create a new column named division/section where it has as value the division if the division is not 4, otherwise it has the value of the section\n",
    "cls_df['division/section'] = cls_df['division']\n",
    "\n",
    "# 4 Means No Division\n",
    "cls_df.loc[cls_df['division'] == 4, 'division/section'] = cls_df['section']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:33:55.287154400Z",
     "start_time": "2024-02-23T23:33:55.269341Z"
    }
   },
   "id": "a7f158c0df67a4a0",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's split the data into training and testing sets, using 80% of the data for training and 20% for testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2411460d85eb8e55"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get class and division/section\n",
    "y = cls_df[['class', 'division/section']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:33:55.609492100Z",
     "start_time": "2024-02-23T23:33:55.603533400Z"
    }
   },
   "id": "85fde21674eaec17",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having done that we can now approach the two-level classification using the SGD Classifier as before, but this\n",
    "time we will use the `MultiOutputClassifier` from sklearn to perform the multi-output classification.\n",
    "However, there is a drawback to this approach, because at present, no metric in sklearn.metrics supports the multiclass-multioutput classification task.\n",
    "This means that we will have to create our own custom accuracy metric for the multi-output classification."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25d67e79115b7dd8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-output Accuracy: 0.4386792452830189\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Create a multi-output classifier\n",
    "multi_output_model = MultiOutputClassifier(\n",
    "    sgd_model, n_jobs=-1)\n",
    "\n",
    "# Train the multi-output classifier\n",
    "multi_output_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = multi_output_model.predict(X_test)\n",
    "\n",
    "# Custom accuracy for multi-output\n",
    "def multioutput_accuracy(y_true, y_pred):\n",
    "    correct = np.all(y_true == y_pred, axis=1)\n",
    "    return np.mean(correct)\n",
    "\n",
    "\n",
    "accuracy = multioutput_accuracy(y_test, y_pred)\n",
    "print(f\"Multi-output Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:33:59.683944400Z",
     "start_time": "2024-02-23T23:33:56.308865800Z"
    }
   },
   "id": "79b2a49fecb749be",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the multi-output accuracy is 0.438 which is not bad considering the small dataset we have,\n",
    "however, it is not as good as the single-output classification we performed earlier.\n",
    "Also, the custom accuracy metric we created may not be the best way to evaluate the multi-output classification.\n",
    "\n",
    "For that reason, let's try and train separate models for the class and the division/section and see how they perform.\n",
    "\n",
    "We will train a separate model (or models) for predicting the division/section within each class. This model will be trained only on data from a specific class. For example, you might have one model for each class, or a single model that uses the class prediction as an additional input."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7326f9175f93ceb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Accuracy: 0.6410256410256411\n",
      "Class 1 Accuracy: 0.7666666666666667\n",
      "Class 2 Accuracy: 0.7586206896551724\n",
      "Class 3 Accuracy: 0.696969696969697\n",
      "Class 4 Accuracy: 0.6756756756756757\n",
      "Class 5 Accuracy: 0.7608695652173914\n",
      "\n",
      "Average Accuracy: 0.7166379892017073\n"
     ]
    }
   ],
   "source": [
    "# For each class, train a separate model to predict the division/section\n",
    "class_models = {}\n",
    "accuracies = []\n",
    "for i in range(6):\n",
    "    # Get the data for the class\n",
    "    class_data = cls_df[cls_df['class'] == i]\n",
    "    X_C = np.vstack(class_data['embedding'])\n",
    "    y_C = class_data['division/section']\n",
    "    # Split the data into training and testing sets\n",
    "    X_C_train, X_C_test, y_C_train, y_C_test = train_test_split(X_C, y_C, test_size=0.2, random_state=62)\n",
    "\n",
    "    # Train the model\n",
    "    model = sgd_model\n",
    "    model.fit(X_C_train, y_C_train)\n",
    "\n",
    "    # Generate classification report\n",
    "    y_C_pred = model.predict(X_C_test)\n",
    "    print(f\"Class {i} Accuracy: {metrics.accuracy_score(y_C_test, y_C_pred)}\")\n",
    "    # Multi-output accuracy (multiply the accuracy for each class from the class model with the accuracy for the division/section model)\n",
    "    \n",
    "    # Firstly, get the f1-score for the class we are checking from the sgd model\n",
    "    f1_score = metrics.f1_score(y_C_test, y_C_pred, average='weighted')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    accuracies.append(metrics.accuracy_score(y_C_test, y_C_pred))\n",
    "    class_models[i] = model\n",
    "\n",
    "print()\n",
    "print(f\"Average Accuracy: {np.mean(accuracies)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:36:04.021641700Z",
     "start_time": "2024-02-23T23:36:03.764498700Z"
    }
   },
   "id": "200c547705fbd699",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now having the separate models for each class, we can now chain them together to predict the division/section based on the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec307bba8ff898af"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Chain the models together to predict the division/section based on the class\n",
    "def predict_word_class_and_division(df):\n",
    "    predictions = []\n",
    "\n",
    "    for word in df[word_column]:\n",
    "        # Predict the class\n",
    "        predicted_class = sgd_model.predict(word)[0]  # Assuming the output is a list\n",
    "\n",
    "        # Predict the division/section within the predicted class\n",
    "        predicted_division = class_models[predicted_class].predict([word])[0]  # Assuming the output is a list\n",
    "\n",
    "        predictions.append({'class': predicted_class, 'division/section': predicted_division})\n",
    "\n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "y_pred = predict_word_class_and_division(X_test)\n",
    "\n",
    "# Custom accuracy for multi-output\n",
    "def multioutput_accuracy(y_true, y_pred):\n",
    "    correct = np.all(y_true == y_pred, axis=1)\n",
    "    return np.mean(correct)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4007c2ce0b400bf8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
