{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Classification\n",
    "\n",
    "Let's now retrieve the classification embeddings from the chromadb vector database and perform classification on them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb007503dc588dee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = client.get_collection(\"nomic_classification_v1\")"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:54:47.425655800Z",
     "start_time": "2024-02-23T12:54:47.409655700Z"
    }
   },
   "id": "initial_id",
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us get the words along with their classes and division and sections from the json file we created earlier."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84452ecc48ce9511"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"hierarchy.json\", \"r\") as f:\n",
    "    categories = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:54:47.668844600Z",
     "start_time": "2024-02-23T12:54:47.663820500Z"
    }
   },
   "id": "6185d3b32070a313",
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's retrieve the embeddings and the words along with the class, division, and section from the chromadb and create a dataframe with them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11d5ef59ed70ec91"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                   word                                          embedding  \\\n0             Existence  [-0.029754638671875, -0.0263671875, -0.0127792...   \n1           Inexistence  [-0.01029205322265625, 0.019256591796875, -0.0...   \n2         Consanguinity  [0.0386962890625, -0.01026153564453125, -0.013...   \n3            Trisection  [0.007350921630859375, -0.001903533935546875, ...   \n4             Innocence  [0.0338134765625, 0.0139923095703125, -0.03097...   \n...                 ...                                                ...   \n1052              Knave  [0.0032215118408203125, 0.01201629638671875, -...   \n1053  Disinterestedness  [-0.01776123046875, 0.020965576171875, -0.0221...   \n1054        Selfishness  [0.01071929931640625, 0.049468994140625, -0.03...   \n1055             Virtue  [0.0201416015625, 0.006626129150390625, -0.035...   \n1056               Vice  [0.0035228729248046875, -0.028533935546875, -0...   \n\n                                                class division  \\\n0                 WORDS EXPRESSING ABSTRACT RELATIONS      N/A   \n1                 WORDS EXPRESSING ABSTRACT RELATIONS      N/A   \n2                 WORDS EXPRESSING ABSTRACT RELATIONS      N/A   \n3                 WORDS EXPRESSING ABSTRACT RELATIONS      N/A   \n4     WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n...                                               ...      ...   \n1052  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n1053  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n1054  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n1055  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n1056  WORDS RELATING TO THE SENTIENT AND MORAL POWERS      N/A   \n\n               section  \n0            EXISTENCE  \n1            EXISTENCE  \n2             RELATION  \n3               NUMBER  \n4     MORAL AFFECTIONS  \n...                ...  \n1052  MORAL AFFECTIONS  \n1053  MORAL AFFECTIONS  \n1054  MORAL AFFECTIONS  \n1055  MORAL AFFECTIONS  \n1056  MORAL AFFECTIONS  \n\n[1057 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>embedding</th>\n      <th>class</th>\n      <th>division</th>\n      <th>section</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Existence</td>\n      <td>[-0.029754638671875, -0.0263671875, -0.0127792...</td>\n      <td>WORDS EXPRESSING ABSTRACT RELATIONS</td>\n      <td>N/A</td>\n      <td>EXISTENCE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Inexistence</td>\n      <td>[-0.01029205322265625, 0.019256591796875, -0.0...</td>\n      <td>WORDS EXPRESSING ABSTRACT RELATIONS</td>\n      <td>N/A</td>\n      <td>EXISTENCE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Consanguinity</td>\n      <td>[0.0386962890625, -0.01026153564453125, -0.013...</td>\n      <td>WORDS EXPRESSING ABSTRACT RELATIONS</td>\n      <td>N/A</td>\n      <td>RELATION</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Trisection</td>\n      <td>[0.007350921630859375, -0.001903533935546875, ...</td>\n      <td>WORDS EXPRESSING ABSTRACT RELATIONS</td>\n      <td>N/A</td>\n      <td>NUMBER</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Innocence</td>\n      <td>[0.0338134765625, 0.0139923095703125, -0.03097...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1052</th>\n      <td>Knave</td>\n      <td>[0.0032215118408203125, 0.01201629638671875, -...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>1053</th>\n      <td>Disinterestedness</td>\n      <td>[-0.01776123046875, 0.020965576171875, -0.0221...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>1054</th>\n      <td>Selfishness</td>\n      <td>[0.01071929931640625, 0.049468994140625, -0.03...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>1055</th>\n      <td>Virtue</td>\n      <td>[0.0201416015625, 0.006626129150390625, -0.035...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n    <tr>\n      <th>1056</th>\n      <td>Vice</td>\n      <td>[0.0035228729248046875, -0.028533935546875, -0...</td>\n      <td>WORDS RELATING TO THE SENTIENT AND MORAL POWERS</td>\n      <td>N/A</td>\n      <td>MORAL AFFECTIONS</td>\n    </tr>\n  </tbody>\n</table>\n<p>1057 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Retrieve the embeddings, words, and metadata\n",
    "words = collection.get(include=[\"embeddings\", \"documents\"])['documents']\n",
    "embeddings = collection.get(include=[\"embeddings\", \"documents\"])['embeddings']\n",
    "metadata = collection.get(include=[\"embeddings\", \"documents\", \"metadatas\"])['metadatas']\n",
    "\n",
    "# Create a dataframe with the embeddings and the words\n",
    "cls_df = pd.DataFrame({'word': words, 'embedding': embeddings})\n",
    "\n",
    "# Add the class, division, and section from metadata to the dataframe\n",
    "cls_df['class'] = [md['class'] for md in metadata]\n",
    "cls_df['division'] = [md['division'] for md in metadata]\n",
    "cls_df['section'] = [md['section'] for md in metadata]\n",
    "\n",
    "cls_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:54:48.073159800Z",
     "start_time": "2024-02-23T12:54:47.981158700Z"
    }
   },
   "id": "b09af619c5292133",
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's prepare the data for classification by converting the class, division, and section to numerical values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3501a068561a6136"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                   word                                          embedding  \\\n0             Existence  [-0.029754638671875, -0.0263671875, -0.0127792...   \n1           Inexistence  [-0.01029205322265625, 0.019256591796875, -0.0...   \n2         Consanguinity  [0.0386962890625, -0.01026153564453125, -0.013...   \n3            Trisection  [0.007350921630859375, -0.001903533935546875, ...   \n4             Innocence  [0.0338134765625, 0.0139923095703125, -0.03097...   \n...                 ...                                                ...   \n1052              Knave  [0.0032215118408203125, 0.01201629638671875, -...   \n1053  Disinterestedness  [-0.01776123046875, 0.020965576171875, -0.0221...   \n1054        Selfishness  [0.01071929931640625, 0.049468994140625, -0.03...   \n1055             Virtue  [0.0201416015625, 0.006626129150390625, -0.035...   \n1056               Vice  [0.0035228729248046875, -0.028533935546875, -0...   \n\n      class  division  section  \n0         0         4        7  \n1         0         4        7  \n2         0         4       29  \n3         0         4       19  \n4         4         4       16  \n...     ...       ...      ...  \n1052      4         4       16  \n1053      4         4       16  \n1054      4         4       16  \n1055      4         4       16  \n1056      4         4       16  \n\n[1057 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>embedding</th>\n      <th>class</th>\n      <th>division</th>\n      <th>section</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Existence</td>\n      <td>[-0.029754638671875, -0.0263671875, -0.0127792...</td>\n      <td>0</td>\n      <td>4</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Inexistence</td>\n      <td>[-0.01029205322265625, 0.019256591796875, -0.0...</td>\n      <td>0</td>\n      <td>4</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Consanguinity</td>\n      <td>[0.0386962890625, -0.01026153564453125, -0.013...</td>\n      <td>0</td>\n      <td>4</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Trisection</td>\n      <td>[0.007350921630859375, -0.001903533935546875, ...</td>\n      <td>0</td>\n      <td>4</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Innocence</td>\n      <td>[0.0338134765625, 0.0139923095703125, -0.03097...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1052</th>\n      <td>Knave</td>\n      <td>[0.0032215118408203125, 0.01201629638671875, -...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1053</th>\n      <td>Disinterestedness</td>\n      <td>[-0.01776123046875, 0.020965576171875, -0.0221...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1054</th>\n      <td>Selfishness</td>\n      <td>[0.01071929931640625, 0.049468994140625, -0.03...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1055</th>\n      <td>Virtue</td>\n      <td>[0.0201416015625, 0.006626129150390625, -0.035...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1056</th>\n      <td>Vice</td>\n      <td>[0.0035228729248046875, -0.028533935546875, -0...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n<p>1057 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all classes and divisions/sections as numerical values\n",
    "cls_df['class'] = pd.Categorical(cls_df['class'])\n",
    "cls_df['division'] = pd.Categorical(cls_df['division'])\n",
    "cls_df['section'] = pd.Categorical(cls_df['section'])\n",
    "\n",
    "cls_df['class'] = cls_df['class'].cat.codes\n",
    "cls_df['division'] = cls_df['division'].cat.codes\n",
    "cls_df['section'] = cls_df['section'].cat.codes\n",
    "cls_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:54:48.764844Z",
     "start_time": "2024-02-23T12:54:48.748498800Z"
    }
   },
   "id": "516a1edf25fbdd15",
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also prepare the embeddings for classification by converting them to a numpy array."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "891ef0bbf5ddeecc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.vstack(cls_df['embedding'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:54:49.607018600Z",
     "start_time": "2024-02-23T12:54:49.581018200Z"
    }
   },
   "id": "1d2b8c0fca50cf83",
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class logistic regression\n",
    "\n",
    "Let us start by training a multi-class logistic regression model on the embeddings and the class.\n",
    "\n",
    "Before we do that, however, let's split the data into training and testing sets, using 80% of the data for training and 20% for testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0da1d957a8e1533"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = cls_df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:54:50.274366900Z",
     "start_time": "2024-02-23T12:54:50.261741400Z"
    }
   },
   "id": "e541551066686048",
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's train a logistic regression model on the training data and evaluate it on the testing data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b8a7c1f9edc9e63"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=1000, multi_class='multinomial', n_jobs=-1,\n                   random_state=42)",
      "text/html": "<style>#sk-container-id-2 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-2 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-2 pre {\n  padding: 0;\n}\n\n#sk-container-id-2 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-2 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-2 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-2 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-2 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-2 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-2 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-2 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-2 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"â–¸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-2 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"â–¾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n#sk-container-id-2 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-2 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-2 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-2 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-2 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-2 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-2 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;multinomial&#x27;, n_jobs=-1,\n                   random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;multinomial&#x27;, n_jobs=-1,\n                   random_state=42)</pre></div> </div></div></div></div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, multi_class='multinomial', n_jobs=-1, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T10:26:42.224918100Z",
     "start_time": "2024-02-23T10:26:41.429442700Z"
    }
   },
   "id": "2a20c0e4ce5e28ab",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 833us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[54], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Generate classification report\u001B[39;00m\n\u001B[0;32m      5\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclassification_report\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2592\u001B[0m, in \u001B[0;36mclassification_report\u001B[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001B[0m\n\u001B[0;32m   2457\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m   2458\u001B[0m     {\n\u001B[0;32m   2459\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2483\u001B[0m     zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2484\u001B[0m ):\n\u001B[0;32m   2485\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001B[39;00m\n\u001B[0;32m   2486\u001B[0m \n\u001B[0;32m   2487\u001B[0m \u001B[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2589\u001B[0m \u001B[38;5;124;03m    <BLANKLINE>\u001B[39;00m\n\u001B[0;32m   2590\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2592\u001B[0m     y_type, y_true, y_pred \u001B[38;5;241m=\u001B[39m \u001B[43m_check_targets\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2594\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2595\u001B[0m         labels \u001B[38;5;241m=\u001B[39m unique_labels(y_true, y_pred)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:94\u001B[0m, in \u001B[0;36m_check_targets\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m     91\u001B[0m     y_type \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(y_type) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 94\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     95\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClassification metrics can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt handle a mix of \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m targets\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m     96\u001B[0m             type_true, type_pred\n\u001B[0;32m     97\u001B[0m         )\n\u001B[0;32m     98\u001B[0m     )\n\u001B[0;32m    100\u001B[0m \u001B[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001B[39;00m\n\u001B[0;32m    101\u001B[0m y_type \u001B[38;5;241m=\u001B[39m y_type\u001B[38;5;241m.\u001B[39mpop()\n",
      "\u001B[1;31mValueError\u001B[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Generate classification report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:54:53.218794500Z",
     "start_time": "2024-02-23T12:54:53.116794600Z"
    }
   },
   "id": "8caa5ddbc9c9df03",
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having trained and evaluated the model, we can see that the model has an accuracy of 0.64, \n",
    "which is not bad considering the previous results we got from the clustering of the embeddings.\n",
    "\n",
    "However, let's use a hyperparameter tuning library such as `optuna` to find the best hyperparameters for the model.\n",
    "\n",
    "We will use the `cross_val_score` function from `sklearn` to evaluate the model using cross-validation and the `optuna` library to find the best hyperparameters for the model based on the maximization of the accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f52da790af8442a3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-23 12:26:45,484] A new study created in memory with name: no-name-305a575b-7f01-493b-a017-e69eaec63beb\n",
      "[I 2024-02-23 12:26:51,664] Trial 0 finished with value: 0.6023668639053255 and parameters: {'multi_class': 'ovr', 'C': 661.376255805555}. Best is trial 0 with value: 0.6023668639053255.\n",
      "[I 2024-02-23 12:26:57,083] Trial 1 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'multinomial', 'C': 6.385372774006955}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:02,258] Trial 2 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'multinomial', 'C': 0.0011120920175576884}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:07,827] Trial 3 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'multinomial', 'C': 18.9677376569383}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:13,553] Trial 4 finished with value: 0.5917159763313611 and parameters: {'multi_class': 'multinomial', 'C': 770.3969372004187}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:18,846] Trial 5 finished with value: 0.6307692307692307 and parameters: {'multi_class': 'multinomial', 'C': 1.2739863944024226}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:24,045] Trial 6 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'multinomial', 'C': 0.0003812640729113557}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:29,278] Trial 7 finished with value: 0.5952662721893491 and parameters: {'multi_class': 'multinomial', 'C': 0.43421583486972876}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:34,995] Trial 8 finished with value: 0.5905325443786982 and parameters: {'multi_class': 'multinomial', 'C': 845.63135619237}. Best is trial 1 with value: 0.6378698224852071.\n",
      "[I 2024-02-23 12:27:40,229] Trial 9 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 5.2102742278469645}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:27:45,437] Trial 10 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'ovr', 'C': 0.017055805956169127}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:27:50,663] Trial 11 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'ovr', 'C': 10.059386790458108}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:27:55,797] Trial 12 finished with value: 0.30532544378698223 and parameters: {'multi_class': 'ovr', 'C': 0.08425911639227203}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:01,116] Trial 13 finished with value: 0.6414201183431953 and parameters: {'multi_class': 'ovr', 'C': 16.59365955781666}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:06,346] Trial 14 finished with value: 0.6260355029585798 and parameters: {'multi_class': 'ovr', 'C': 59.50760529299438}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:11,523] Trial 15 finished with value: 0.591715976331361 and parameters: {'multi_class': 'ovr', 'C': 7216.5593305356115}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:16,703] Trial 16 finished with value: 0.6071005917159763 and parameters: {'multi_class': 'ovr', 'C': 0.7901032638322626}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:21,994] Trial 17 finished with value: 0.6236686390532544 and parameters: {'multi_class': 'ovr', 'C': 96.87649470981486}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:27,115] Trial 18 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'ovr', 'C': 0.01473177987863997}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:32,421] Trial 19 finished with value: 0.6366863905325443 and parameters: {'multi_class': 'ovr', 'C': 2.796732742045829}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:37,619] Trial 20 finished with value: 0.3739644970414201 and parameters: {'multi_class': 'ovr', 'C': 0.1224591334896701}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:43,325] Trial 21 finished with value: 0.6390532544378698 and parameters: {'multi_class': 'multinomial', 'C': 6.648596341660178}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:48,505] Trial 22 finished with value: 0.6295857988165681 and parameters: {'multi_class': 'ovr', 'C': 50.68157333010183}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:53,858] Trial 23 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'multinomial', 'C': 4.0417741981614785}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:28:59,052] Trial 24 finished with value: 0.6201183431952663 and parameters: {'multi_class': 'ovr', 'C': 112.3754937620061}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:04,239] Trial 25 finished with value: 0.5230769230769231 and parameters: {'multi_class': 'ovr', 'C': 0.28707050854235727}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:09,871] Trial 26 finished with value: 0.6355029585798817 and parameters: {'multi_class': 'multinomial', 'C': 16.639448040057466}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:15,183] Trial 27 finished with value: 0.5928994082840238 and parameters: {'multi_class': 'ovr', 'C': 365.9690017954215}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:20,468] Trial 28 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'multinomial', 'C': 2.3071119730258167}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:25,680] Trial 29 finished with value: 0.5940828402366864 and parameters: {'multi_class': 'ovr', 'C': 3630.6840145203096}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:30,987] Trial 30 finished with value: 0.6071005917159764 and parameters: {'multi_class': 'ovr', 'C': 302.235452264418}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:36,400] Trial 31 finished with value: 0.6355029585798817 and parameters: {'multi_class': 'multinomial', 'C': 3.945286668819994}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:42,003] Trial 32 finished with value: 0.634319526627219 and parameters: {'multi_class': 'multinomial', 'C': 10.332592844614958}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:47,546] Trial 33 finished with value: 0.6343195266272189 and parameters: {'multi_class': 'multinomial', 'C': 28.398455460462856}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:53,035] Trial 34 finished with value: 0.6402366863905326 and parameters: {'multi_class': 'multinomial', 'C': 8.211781468875872}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:29:58,351] Trial 35 finished with value: 0.6224852071005917 and parameters: {'multi_class': 'multinomial', 'C': 1.0357543481531144}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:04,022] Trial 36 finished with value: 0.6071005917159764 and parameters: {'multi_class': 'multinomial', 'C': 160.01055238934677}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:09,607] Trial 37 finished with value: 0.6343195266272189 and parameters: {'multi_class': 'multinomial', 'C': 24.208438251671875}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:15,228] Trial 38 finished with value: 0.6390532544378698 and parameters: {'multi_class': 'multinomial', 'C': 6.885732966665508}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:20,469] Trial 39 finished with value: 0.4650887573964497 and parameters: {'multi_class': 'multinomial', 'C': 0.16755469596442507}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:25,641] Trial 40 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'multinomial', 'C': 0.014295772856372542}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:31,139] Trial 41 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'multinomial', 'C': 9.60642215484635}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:36,600] Trial 42 finished with value: 0.6390532544378698 and parameters: {'multi_class': 'multinomial', 'C': 6.217965034063374}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:41,931] Trial 43 finished with value: 0.6355029585798817 and parameters: {'multi_class': 'multinomial', 'C': 1.8226486222643368}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:47,172] Trial 44 finished with value: 0.6023668639053255 and parameters: {'multi_class': 'multinomial', 'C': 0.4970341888476986}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:52,864] Trial 45 finished with value: 0.634319526627219 and parameters: {'multi_class': 'multinomial', 'C': 35.62201117319802}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:30:58,412] Trial 46 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'multinomial', 'C': 16.465104804530142}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:03,612] Trial 47 finished with value: 0.6390532544378698 and parameters: {'multi_class': 'ovr', 'C': 4.7949183968320535}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:08,913] Trial 48 finished with value: 0.6307692307692307 and parameters: {'multi_class': 'multinomial', 'C': 1.1726580601816083}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:14,081] Trial 49 finished with value: 0.22485207100591714 and parameters: {'multi_class': 'ovr', 'C': 0.0478587296704084}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:19,373] Trial 50 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'ovr', 'C': 0.0035093330905880712}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:24,775] Trial 51 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'multinomial', 'C': 6.3297806322736605}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:30,276] Trial 52 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'multinomial', 'C': 7.688967878037122}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:35,535] Trial 53 finished with value: 0.6047337278106509 and parameters: {'multi_class': 'multinomial', 'C': 0.6244994307197752}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:41,251] Trial 54 finished with value: 0.6213017751479291 and parameters: {'multi_class': 'multinomial', 'C': 62.996774096346016}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:46,633] Trial 55 finished with value: 0.6355029585798817 and parameters: {'multi_class': 'multinomial', 'C': 2.078839473459044}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:51,868] Trial 56 finished with value: 0.6390532544378699 and parameters: {'multi_class': 'ovr', 'C': 26.2547003123418}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:31:57,135] Trial 57 finished with value: 0.591715976331361 and parameters: {'multi_class': 'ovr', 'C': 1220.6023861193805}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:02,387] Trial 58 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 15.792046645759205}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:07,570] Trial 59 finished with value: 0.21183431952662723 and parameters: {'multi_class': 'ovr', 'C': 0.00015352858302659125}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:12,835] Trial 60 finished with value: 0.6142011834319527 and parameters: {'multi_class': 'ovr', 'C': 165.47585425202297}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:18,083] Trial 61 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 14.648631693359567}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:23,404] Trial 62 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 17.50253244549469}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:28,620] Trial 63 finished with value: 0.621301775147929 and parameters: {'multi_class': 'ovr', 'C': 66.04473729032397}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:33,795] Trial 64 finished with value: 0.6426035502958579 and parameters: {'multi_class': 'ovr', 'C': 17.88270057993157}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:38,969] Trial 65 finished with value: 0.6402366863905326 and parameters: {'multi_class': 'ovr', 'C': 17.667140072483264}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:44,148] Trial 66 finished with value: 0.634319526627219 and parameters: {'multi_class': 'ovr', 'C': 41.99389949594814}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:49,325] Trial 67 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 16.891057826172432}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:54,512] Trial 68 finished with value: 0.6106508875739645 and parameters: {'multi_class': 'ovr', 'C': 234.80038306804983}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:32:59,689] Trial 69 finished with value: 0.5905325443786983 and parameters: {'multi_class': 'ovr', 'C': 640.0606412891965}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:04,866] Trial 70 finished with value: 0.6248520710059171 and parameters: {'multi_class': 'ovr', 'C': 96.77608961581541}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:10,041] Trial 71 finished with value: 0.6402366863905326 and parameters: {'multi_class': 'ovr', 'C': 14.076753577795639}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:15,217] Trial 72 finished with value: 0.6331360946745562 and parameters: {'multi_class': 'ovr', 'C': 3.3326868233309828}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:20,403] Trial 73 finished with value: 0.6390532544378699 and parameters: {'multi_class': 'ovr', 'C': 10.64315933366284}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:25,578] Trial 74 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'ovr', 'C': 35.85815491493345}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:30,743] Trial 75 finished with value: 0.6343195266272189 and parameters: {'multi_class': 'ovr', 'C': 1.6291475015023487}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:35,919] Trial 76 finished with value: 0.6426035502958579 and parameters: {'multi_class': 'ovr', 'C': 17.09475533954898}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:41,106] Trial 77 finished with value: 0.6177514792899408 and parameters: {'multi_class': 'ovr', 'C': 92.3346445053013}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:46,649] Trial 78 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 16.434419047914194}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:51,825] Trial 79 finished with value: 0.6331360946745562 and parameters: {'multi_class': 'ovr', 'C': 3.3671202273770136}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:33:56,999] Trial 80 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'ovr', 'C': 19.713894728713353}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:02,176] Trial 81 finished with value: 0.6390532544378698 and parameters: {'multi_class': 'ovr', 'C': 13.639402766158021}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:07,364] Trial 82 finished with value: 0.6260355029585799 and parameters: {'multi_class': 'ovr', 'C': 51.767958696541534}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:12,542] Trial 83 finished with value: 0.6414201183431952 and parameters: {'multi_class': 'ovr', 'C': 27.085673128730605}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:17,720] Trial 84 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'ovr', 'C': 5.566740318860573}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:22,897] Trial 85 finished with value: 0.6378698224852071 and parameters: {'multi_class': 'ovr', 'C': 11.663986934718494}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:28,072] Trial 86 finished with value: 0.6355029585798817 and parameters: {'multi_class': 'ovr', 'C': 2.9361826385008905}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:33,248] Trial 87 finished with value: 0.6402366863905324 and parameters: {'multi_class': 'ovr', 'C': 19.16753329768419}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:38,434] Trial 88 finished with value: 0.6343195266272189 and parameters: {'multi_class': 'ovr', 'C': 41.49601158098556}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:43,610] Trial 89 finished with value: 0.6260355029585799 and parameters: {'multi_class': 'ovr', 'C': 73.16040581483503}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:48,784] Trial 90 finished with value: 0.6402366863905324 and parameters: {'multi_class': 'ovr', 'C': 4.561689325633564}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:53,958] Trial 91 finished with value: 0.6366863905325444 and parameters: {'multi_class': 'ovr', 'C': 28.81946050144084}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:34:59,144] Trial 92 finished with value: 0.6165680473372781 and parameters: {'multi_class': 'ovr', 'C': 149.0801805434398}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:35:04,319] Trial 93 finished with value: 0.6390532544378698 and parameters: {'multi_class': 'ovr', 'C': 25.62085438236995}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:35:09,494] Trial 94 finished with value: 0.6402366863905324 and parameters: {'multi_class': 'ovr', 'C': 12.516103841010564}. Best is trial 9 with value: 0.642603550295858.\n",
      "[I 2024-02-23 12:35:14,669] Trial 95 finished with value: 0.6437869822485206 and parameters: {'multi_class': 'ovr', 'C': 8.038461586013085}. Best is trial 95 with value: 0.6437869822485206.\n",
      "[I 2024-02-23 12:35:19,845] Trial 96 finished with value: 0.6414201183431952 and parameters: {'multi_class': 'ovr', 'C': 8.971620727605226}. Best is trial 95 with value: 0.6437869822485206.\n",
      "[I 2024-02-23 12:35:25,021] Trial 97 finished with value: 0.642603550295858 and parameters: {'multi_class': 'ovr', 'C': 4.627839059606699}. Best is trial 95 with value: 0.6437869822485206.\n",
      "[I 2024-02-23 12:35:30,195] Trial 98 finished with value: 0.6343195266272189 and parameters: {'multi_class': 'ovr', 'C': 2.0813198836245714}. Best is trial 95 with value: 0.6437869822485206.\n",
      "[I 2024-02-23 12:35:35,372] Trial 99 finished with value: 0.6260355029585799 and parameters: {'multi_class': 'ovr', 'C': 1.3974374559335296}. Best is trial 95 with value: 0.6437869822485206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'multi_class': 'ovr', 'C': 8.038461586013085}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    multi_class = trial.suggest_categorical('multi_class', ['ovr', 'multinomial'])\n",
    "    C = trial.suggest_float('C', 1e-4, 1e4, log=True)\n",
    "\n",
    "    # Create logistic regression model with suggested hyperparameters\n",
    "    model = LogisticRegression(C=C, random_state=42, max_iter=1000, multi_class=multi_class, n_jobs=-1)\n",
    "    # Perform cross-validation and return the mean score\n",
    "    score = cross_val_score(model, X, y, scoring='accuracy')\n",
    "    return score.mean()\n",
    "\n",
    "\n",
    "# Create a study object and specify the optimization direction\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print('Best trial:', study.best_trial.params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T10:35:35.377104500Z",
     "start_time": "2024-02-23T10:26:45.022047500Z"
    }
   },
   "id": "9e76682b23c63239",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class SVM\n",
    "\n",
    "Let's now train a multi-class SVM model on the embeddings and the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e86e3dc9de132389"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "SVC(random_state=22)",
      "text/html": "<style>#sk-container-id-16 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-16 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-16 pre {\n  padding: 0;\n}\n\n#sk-container-id-16 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-16 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-16 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-16 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-16 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-16 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-16 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-16 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-16 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-16 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-16 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-16 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-16 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-16 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"â–¸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-16 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-16 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-16 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-16 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"â–¾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-16 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-16 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-16 div.sk-label label.sk-toggleable__label,\n#sk-container-id-16 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-16 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-16 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-16 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-16 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-16 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-16 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-16 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-16 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-16 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-16 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-16 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(random_state=22)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(random_state=22)</pre></div> </div></div></div></div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='rbf', random_state=22)\n",
    "svm_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T13:02:39.162338200Z",
     "start_time": "2024-02-23T13:02:39.075291700Z"
    }
   },
   "id": "2c66c7edab0d64",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.73      0.70        44\n",
      "           1       0.62      0.53      0.57        30\n",
      "           2       0.71      0.48      0.58        31\n",
      "           3       0.73      0.81      0.77        27\n",
      "           4       0.62      0.77      0.69        31\n",
      "           5       0.63      0.63      0.63        49\n",
      "\n",
      "    accuracy                           0.66       212\n",
      "   macro avg       0.67      0.66      0.66       212\n",
      "weighted avg       0.66      0.66      0.66       212\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T13:02:39.325760700Z",
     "start_time": "2024-02-23T13:02:39.280369500Z"
    }
   },
   "id": "62835de5722c9451",
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use the `optuna` library to find the best hyperparameters for the model based on the maximization of the accuracy.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a87564b264fa6a83"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-23 15:00:44,770] A new study created in memory with name: no-name-e83effc6-f9ff-42c6-819a-7d7b2e053cbe\n",
      "[I 2024-02-23 15:00:45,653] Trial 0 finished with value: 0.21570240543682373 and parameters: {'C': 0.6858905527983484, 'kernel': 'poly', 'gamma': 'auto', 'degree': 5}. Best is trial 0 with value: 0.21570240543682373.\n",
      "[I 2024-02-23 15:00:46,416] Trial 1 finished with value: 0.21570240543682373 and parameters: {'C': 0.00029909714045756334, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 0 with value: 0.21570240543682373.\n",
      "[I 2024-02-23 15:00:46,847] Trial 2 finished with value: 0.4219708486094966 and parameters: {'C': 401.6327806521961, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 2 with value: 0.4219708486094966.\n",
      "[I 2024-02-23 15:00:47,779] Trial 3 finished with value: 0.21570240543682373 and parameters: {'C': 0.05489127315400744, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.4219708486094966.\n",
      "[I 2024-02-23 15:00:48,560] Trial 4 finished with value: 0.21570240543682373 and parameters: {'C': 2.853036044275423, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.4219708486094966.\n",
      "[I 2024-02-23 15:00:49,004] Trial 5 finished with value: 0.5336314048108737 and parameters: {'C': 0.9367968795384186, 'kernel': 'poly', 'gamma': 'scale', 'degree': 3}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:49,910] Trial 6 finished with value: 0.21570240543682373 and parameters: {'C': 0.0030401619329988093, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:50,487] Trial 7 finished with value: 0.21570240543682373 and parameters: {'C': 28.873617996146482, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:51,298] Trial 8 finished with value: 0.21570240543682373 and parameters: {'C': 0.0007490059321435479, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:52,029] Trial 9 finished with value: 0.21570240543682373 and parameters: {'C': 0.00020321716745001284, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:52,665] Trial 10 finished with value: 0.21570240543682373 and parameters: {'C': 0.038371129551696734, 'kernel': 'poly', 'gamma': 'scale', 'degree': 2}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:53,063] Trial 11 finished with value: 0.4626263077886078 and parameters: {'C': 5045.7980319618, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:53,470] Trial 12 finished with value: 0.4626263077886078 and parameters: {'C': 2695.0544686715093, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:53,852] Trial 13 finished with value: 0.4626263077886078 and parameters: {'C': 57.12216783313387, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:54,235] Trial 14 finished with value: 0.4626263077886078 and parameters: {'C': 5907.2629182623505, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:54,896] Trial 15 finished with value: 0.5251184834123223 and parameters: {'C': 2.4146232938275527, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 5 with value: 0.5336314048108737.\n",
      "[I 2024-02-23 15:00:55,454] Trial 16 finished with value: 0.5364750067066082 and parameters: {'C': 0.8330039710770271, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:56,008] Trial 17 finished with value: 0.4834346776356971 and parameters: {'C': 0.25672270426019855, 'kernel': 'poly', 'gamma': 'scale', 'degree': 3}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:56,884] Trial 18 finished with value: 0.5175310739515335 and parameters: {'C': 19.193727365164328, 'kernel': 'poly', 'gamma': 'scale', 'degree': 3}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:57,442] Trial 19 finished with value: 0.21570240543682373 and parameters: {'C': 0.012001114204796359, 'kernel': 'poly', 'gamma': 'scale', 'degree': 1}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:57,955] Trial 20 finished with value: 0.30841455781096305 and parameters: {'C': 0.1503502678381934, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:58,480] Trial 21 finished with value: 0.5270052758651526 and parameters: {'C': 3.0053666391153078, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:59,005] Trial 22 finished with value: 0.5222704104444246 and parameters: {'C': 5.206806082944573, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 16 with value: 0.5364750067066082.\n",
      "[I 2024-02-23 15:00:59,478] Trial 23 finished with value: 0.5374139318608602 and parameters: {'C': 0.5574736539390779, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:00:59,954] Trial 24 finished with value: 0.537409460788697 and parameters: {'C': 0.5710010704739024, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:01:00,459] Trial 25 finished with value: 0.21570240543682373 and parameters: {'C': 0.00900565080246091, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:01:00,958] Trial 26 finished with value: 0.4020611642671913 and parameters: {'C': 0.2274328892636367, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:01:01,492] Trial 27 finished with value: 0.5288920683179826 and parameters: {'C': 80.39150177629784, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:01:02,028] Trial 28 finished with value: 0.5288920683179826 and parameters: {'C': 10.232186946875991, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:01:02,499] Trial 29 finished with value: 0.5165921487972815 and parameters: {'C': 0.35929075279492606, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 23 with value: 0.5374139318608602.\n",
      "[I 2024-02-23 15:01:02,939] Trial 30 finished with value: 0.5449879281051596 and parameters: {'C': 1.0185860890704288, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:03,400] Trial 31 finished with value: 0.536461593490119 and parameters: {'C': 0.782917999203791, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:03,917] Trial 32 finished with value: 0.21570240543682373 and parameters: {'C': 0.0765970107839401, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:04,365] Trial 33 finished with value: 0.5411964589108468 and parameters: {'C': 0.8891815875481585, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:04,884] Trial 34 finished with value: 0.21570240543682373 and parameters: {'C': 0.01647351032103306, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:05,398] Trial 35 finished with value: 0.21570240543682373 and parameters: {'C': 6.5580976045012775, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:05,790] Trial 36 finished with value: 0.41722256997227936 and parameters: {'C': 272.4697304013837, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:06,194] Trial 37 finished with value: 0.5421532683537513 and parameters: {'C': 1.7455358794464686, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:06,700] Trial 38 finished with value: 0.21570240543682373 and parameters: {'C': 1.7337022687894112, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:07,218] Trial 39 finished with value: 0.21570240543682373 and parameters: {'C': 0.09132691816657261, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:07,729] Trial 40 finished with value: 0.21570240543682373 and parameters: {'C': 14.568529744128709, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:08,215] Trial 41 finished with value: 0.4928954663328266 and parameters: {'C': 0.5558256752155564, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:08,634] Trial 42 finished with value: 0.5431011356523294 and parameters: {'C': 1.3915608732583902, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:09,011] Trial 43 finished with value: 0.5317311991415542 and parameters: {'C': 2.819516663850495, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:09,538] Trial 44 finished with value: 0.21570240543682373 and parameters: {'C': 0.04018250814350936, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:10,080] Trial 45 finished with value: 0.5364615934901189 and parameters: {'C': 1.443961722416836, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:10,445] Trial 46 finished with value: 0.4456049360636681 and parameters: {'C': 36.74009892934497, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:10,960] Trial 47 finished with value: 0.21570240543682373 and parameters: {'C': 6.191261541193038, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:11,474] Trial 48 finished with value: 0.21570240543682373 and parameters: {'C': 0.12767078101655344, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:12,117] Trial 49 finished with value: 0.21570240543682373 and parameters: {'C': 0.0008903807858525186, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:12,480] Trial 50 finished with value: 0.4626263077886078 and parameters: {'C': 182.18971384708402, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:12,964] Trial 51 finished with value: 0.5004739336492892 and parameters: {'C': 0.5794925566166821, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:13,470] Trial 52 finished with value: 0.42100062595010285 and parameters: {'C': 0.3722912680888432, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:13,911] Trial 53 finished with value: 0.5449879281051596 and parameters: {'C': 0.9888002310498957, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:14,278] Trial 54 finished with value: 0.5232093355986767 and parameters: {'C': 3.655195409031323, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 30 with value: 0.5449879281051596.\n",
      "[I 2024-02-23 15:01:14,710] Trial 55 finished with value: 0.5459357954037378 and parameters: {'C': 1.1024927438677246, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:15,121] Trial 56 finished with value: 0.5383573280872753 and parameters: {'C': 1.5664565990685015, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:15,628] Trial 57 finished with value: 0.21570240543682373 and parameters: {'C': 1.143942725755809, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:16,044] Trial 58 finished with value: 0.42480997943306803 and parameters: {'C': 1279.6636374340785, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:16,559] Trial 59 finished with value: 0.23837521237592774 and parameters: {'C': 0.21578932374329343, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:16,900] Trial 60 finished with value: 0.4815657694715193 and parameters: {'C': 9.790669566961917, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:17,327] Trial 61 finished with value: 0.5421577394259144 and parameters: {'C': 1.283658220072994, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:17,717] Trial 62 finished with value: 0.5364750067066082 and parameters: {'C': 2.2856034593866843, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 55 with value: 0.5459357954037378.\n",
      "[I 2024-02-23 15:01:18,157] Trial 63 finished with value: 0.5478270589287311 and parameters: {'C': 1.0428184569634444, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:18,512] Trial 64 finished with value: 0.5147053563444514 and parameters: {'C': 4.849687352262712, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:19,142] Trial 65 finished with value: 0.4720781543414111 and parameters: {'C': 0.31822696471349954, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:19,478] Trial 66 finished with value: 0.4815881248323349 and parameters: {'C': 17.75776602316465, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:20,021] Trial 67 finished with value: 0.21570240543682373 and parameters: {'C': 0.02270826486074277, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:20,534] Trial 68 finished with value: 0.21570240543682373 and parameters: {'C': 0.15262186347727472, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:20,878] Trial 69 finished with value: 0.4976750424751855 and parameters: {'C': 8.429254046971945, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:21,246] Trial 70 finished with value: 0.5251050701958329 and parameters: {'C': 3.305818202714844, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:21,696] Trial 71 finished with value: 0.5402530626844317 and parameters: {'C': 0.8904763161618924, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 63 with value: 0.5478270589287311.\n",
      "[I 2024-02-23 15:01:22,133] Trial 72 finished with value: 0.5497138513815613 and parameters: {'C': 1.0629033978826774, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:22,539] Trial 73 finished with value: 0.541205401055173 and parameters: {'C': 1.654511203025566, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:23,045] Trial 74 finished with value: 0.45412232853438256 and parameters: {'C': 0.40679067430084803, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:23,432] Trial 75 finished with value: 0.5431011356523294 and parameters: {'C': 1.053044941397734, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:23,824] Trial 76 finished with value: 0.535522668335867 and parameters: {'C': 0.9501447665487768, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:24,298] Trial 77 finished with value: 0.34907895913440046 and parameters: {'C': 0.19525645783848636, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:24,779] Trial 78 finished with value: 0.21570240543682373 and parameters: {'C': 0.0868858891060779, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:25,125] Trial 79 finished with value: 0.5128140928194581 and parameters: {'C': 3.6083601443402857, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:25,703] Trial 80 finished with value: 0.5430921935080032 and parameters: {'C': 0.6475012997439689, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:26,296] Trial 81 finished with value: 0.5364750067066082 and parameters: {'C': 0.528625065412582, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:26,846] Trial 82 finished with value: 0.5345926853259412 and parameters: {'C': 1.2483620596006244, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:27,397] Trial 83 finished with value: 0.5336090494500582 and parameters: {'C': 2.283922608137933, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:27,968] Trial 84 finished with value: 0.5383617991594385 and parameters: {'C': 0.7309935233581463, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:28,430] Trial 85 finished with value: 0.4843870160064384 and parameters: {'C': 0.35735762189230674, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:28,780] Trial 86 finished with value: 0.506187963873737 and parameters: {'C': 5.889925610084395, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:29,422] Trial 87 finished with value: 0.4370830725207905 and parameters: {'C': 0.24636871190910423, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:29,852] Trial 88 finished with value: 0.545001341321649 and parameters: {'C': 1.167103644722507, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:30,346] Trial 89 finished with value: 0.21570240543682373 and parameters: {'C': 0.1324785066699525, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:30,734] Trial 90 finished with value: 0.5355316104801932 and parameters: {'C': 2.379351268455524, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:31,234] Trial 91 finished with value: 0.21570240543682373 and parameters: {'C': 0.00010032262764299192, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 72 with value: 0.5497138513815613.\n",
      "[I 2024-02-23 15:01:31,670] Trial 92 finished with value: 0.5506617186801395 and parameters: {'C': 1.0744695722664126, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:32,162] Trial 93 finished with value: 0.470195832960744 and parameters: {'C': 0.4928572331214867, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:32,629] Trial 94 finished with value: 0.5374139318608602 and parameters: {'C': 0.744737965353237, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:33,057] Trial 95 finished with value: 0.5440534740230708 and parameters: {'C': 1.2286191543843268, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:33,417] Trial 96 finished with value: 0.5203836179915944 and parameters: {'C': 4.414098823389576, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:33,817] Trial 97 finished with value: 0.5402620048287579 and parameters: {'C': 1.9366672742460471, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:34,251] Trial 98 finished with value: 0.545001341321649 and parameters: {'C': 1.1687543006089869, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n",
      "[I 2024-02-23 15:01:34,589] Trial 99 finished with value: 0.49009657515872307 and parameters: {'C': 14.241952756514161, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 92 with value: 0.5506617186801395.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'C': 1.0744695722664126, 'kernel': 'sigmoid', 'gamma': 'scale'}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    C = trial.suggest_float('C', 1e-4, 1e4, log=True)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "\n",
    "    # If kernel is 'poly', we also tune degree\n",
    "    if kernel == 'poly':\n",
    "        degree = trial.suggest_int('degree', 1, 5)\n",
    "\n",
    "    # Create and train the SVM model\n",
    "    if kernel == 'poly':\n",
    "        model = SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, random_state=42)\n",
    "    else:\n",
    "        model = SVC(C=C, kernel=kernel, gamma=gamma, random_state=42)\n",
    "\n",
    "    score = cross_val_score(model, X, y, scoring='accuracy')\n",
    "    return score.mean()\n",
    "\n",
    "\n",
    "# Create a study object and specify the optimization direction\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print('Best trial:', study.best_trial.params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T13:01:34.593616600Z",
     "start_time": "2024-02-23T13:00:44.770874700Z"
    }
   },
   "id": "3352fe02bac0f7c4",
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class SGD\n",
    "\n",
    "Let's now train a multi-class SGD model on the embeddings and the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78bd39336c8fa4c1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "SGDClassifier(alpha=2.6545371602330262e-06, eta0=0.007698135990041223,\n              learning_rate='adaptive', loss='squared_hinge', n_jobs=-1,\n              random_state=42)",
      "text/html": "<style>#sk-container-id-12 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-12 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-12 pre {\n  padding: 0;\n}\n\n#sk-container-id-12 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-12 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-12 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-12 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-12 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-12 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-12 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-12 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-12 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-12 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-12 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-12 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-12 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-12 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"â–¸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-12 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-12 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-12 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-12 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"â–¾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-12 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-12 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-12 div.sk-label label.sk-toggleable__label,\n#sk-container-id-12 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-12 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-12 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-12 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-12 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-12 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-12 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-12 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-12 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-12 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-12 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-12 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(alpha=2.6545371602330262e-06, eta0=0.007698135990041223,\n              learning_rate=&#x27;adaptive&#x27;, loss=&#x27;squared_hinge&#x27;, n_jobs=-1,\n              random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SGDClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.SGDClassifier.html\">?<span>Documentation for SGDClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SGDClassifier(alpha=2.6545371602330262e-06, eta0=0.007698135990041223,\n              learning_rate=&#x27;adaptive&#x27;, loss=&#x27;squared_hinge&#x27;, n_jobs=-1,\n              random_state=42)</pre></div> </div></div></div></div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_model = SGDClassifier(loss='squared_hinge', max_iter=1000, n_jobs=-1, random_state=42 , \n",
    "                          penalty='l2', alpha=2.6545371602330262e-06, \n",
    "                          learning_rate='adaptive', eta0=0.007698135990041223)\n",
    "sgd_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T13:00:16.466038200Z",
     "start_time": "2024-02-23T13:00:16.333210400Z"
    }
   },
   "id": "52c78921d9b40eea",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.70      0.67        44\n",
      "           1       0.64      0.53      0.58        30\n",
      "           2       0.67      0.58      0.62        31\n",
      "           3       0.68      0.70      0.69        27\n",
      "           4       0.65      0.77      0.71        31\n",
      "           5       0.65      0.61      0.63        49\n",
      "\n",
      "    accuracy                           0.65       212\n",
      "   macro avg       0.65      0.65      0.65       212\n",
      "weighted avg       0.65      0.65      0.65       212\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report\n",
    "y_pred = sgd_model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T13:00:16.467552800Z",
     "start_time": "2024-02-23T13:00:16.462594500Z"
    }
   },
   "id": "fe95d6f1c77a888d",
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use the `optuna` library to find the best hyperparameters for the model based on the maximization of the accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc1256b67572c199"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-23 14:55:04,575] A new study created in memory with name: no-name-6d4cad58-8733-45af-90cf-fd4a9269c631\n",
      "[I 2024-02-23 14:55:06,032] Trial 0 finished with value: 0.5372781065088759 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 0.0002843539275930434, 'learning_rate': 'optimal', 'eta0': 4.780347017456094e-05}. Best is trial 0 with value: 0.5372781065088759.\n",
      "[I 2024-02-23 14:55:06,383] Trial 1 finished with value: 0.5195266272189348 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 0.002442571598507189, 'learning_rate': 'invscaling', 'eta0': 0.0020001568877933305}. Best is trial 0 with value: 0.5372781065088759.\n",
      "[I 2024-02-23 14:55:06,995] Trial 2 finished with value: 0.16804733727810653 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.00333383263276087, 'learning_rate': 'constant', 'eta0': 0.01355398656226266}. Best is trial 0 with value: 0.5372781065088759.\n",
      "[I 2024-02-23 14:55:07,110] Trial 3 finished with value: 0.5609467455621302 and parameters: {'loss': 'perceptron', 'penalty': 'l2', 'alpha': 0.003873194749247985, 'learning_rate': 'invscaling', 'eta0': 0.014525023788061198}. Best is trial 3 with value: 0.5609467455621302.\n",
      "[I 2024-02-23 14:55:11,691] Trial 4 finished with value: 0.21183431952662723 and parameters: {'loss': 'log_loss', 'penalty': 'l1', 'alpha': 4.201252276181476e-06, 'learning_rate': 'adaptive', 'eta0': 0.00013237043454297412}. Best is trial 3 with value: 0.5609467455621302.\n",
      "[I 2024-02-23 14:55:11,952] Trial 5 finished with value: 0.5644970414201185 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 4.703141643199157e-06, 'learning_rate': 'invscaling', 'eta0': 0.00024931822306579235}. Best is trial 5 with value: 0.5644970414201185.\n",
      "[I 2024-02-23 14:55:15,652] Trial 6 finished with value: 0.21183431952662723 and parameters: {'loss': 'log_loss', 'penalty': 'elasticnet', 'alpha': 0.034451729006116046, 'learning_rate': 'invscaling', 'eta0': 0.0332569797287207}. Best is trial 5 with value: 0.5644970414201185.\n",
      "[I 2024-02-23 14:55:21,444] Trial 7 finished with value: 0.6260355029585798 and parameters: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 0.00013807805287910079, 'learning_rate': 'adaptive', 'eta0': 0.01396226601173444}. Best is trial 7 with value: 0.6260355029585798.\n",
      "[I 2024-02-23 14:55:23,767] Trial 8 finished with value: 0.21183431952662723 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.540781924320066e-05, 'learning_rate': 'invscaling', 'eta0': 0.0002764078066546451}. Best is trial 7 with value: 0.6260355029585798.\n",
      "[I 2024-02-23 14:55:24,990] Trial 9 finished with value: 0.6082840236686391 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 1.6141634951721352e-05, 'learning_rate': 'optimal', 'eta0': 0.0007062315395433922}. Best is trial 7 with value: 0.6260355029585798.\n",
      "[I 2024-02-23 14:55:25,670] Trial 10 finished with value: 0.6355029585798816 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0001502856247839524, 'learning_rate': 'adaptive', 'eta0': 0.003901726908846741}. Best is trial 10 with value: 0.6355029585798816.\n",
      "[I 2024-02-23 14:55:26,347] Trial 11 finished with value: 0.6366863905325444 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0001466537374998443, 'learning_rate': 'adaptive', 'eta0': 0.0033601873324193175}. Best is trial 11 with value: 0.6366863905325444.\n",
      "[I 2024-02-23 14:55:26,938] Trial 12 finished with value: 0.6402366863905326 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 7.528939373175857e-05, 'learning_rate': 'adaptive', 'eta0': 0.002458677394200516}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:30,566] Trial 13 finished with value: 0.536094674556213 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 3.9368020338987256e-05, 'learning_rate': 'adaptive', 'eta0': 1.0223625591063912e-05}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:31,249] Trial 14 finished with value: 0.6378698224852071 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0008193200858309525, 'learning_rate': 'adaptive', 'eta0': 0.0038186015790885675}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:31,795] Trial 15 finished with value: 0.6142011834319527 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 0.0012560849125271144, 'learning_rate': 'constant', 'eta0': 0.0911286680452879}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:32,411] Trial 16 finished with value: 0.6390532544378699 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.03593646696069724, 'learning_rate': 'adaptive', 'eta0': 0.001034610693863933}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:33,031] Trial 17 finished with value: 0.6319526627218934 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.033144035727560583, 'learning_rate': 'adaptive', 'eta0': 0.0008238601563755858}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:35,555] Trial 18 finished with value: 0.2828402366863906 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.011939801427060045, 'learning_rate': 'adaptive', 'eta0': 0.0011185709655906232}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:36,563] Trial 19 finished with value: 0.21183431952662723 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.4084071391033366e-06, 'learning_rate': 'constant', 'eta0': 2.7971539270882222e-05}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:36,775] Trial 20 finished with value: 0.21183431952662723 and parameters: {'loss': 'log_loss', 'penalty': 'l2', 'alpha': 0.0964492269445362, 'learning_rate': 'optimal', 'eta0': 0.00027537648341266426}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:37,481] Trial 21 finished with value: 0.634319526627219 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0007554821676218763, 'learning_rate': 'adaptive', 'eta0': 0.005280402839018609}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:38,076] Trial 22 finished with value: 0.6355029585798817 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 3.240840585299495e-05, 'learning_rate': 'adaptive', 'eta0': 0.0014835742301318846}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:38,778] Trial 23 finished with value: 0.6307692307692309 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.011197922357053894, 'learning_rate': 'adaptive', 'eta0': 0.006108195591229212}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:39,345] Trial 24 finished with value: 0.6071005917159764 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.000515374669371858, 'learning_rate': 'adaptive', 'eta0': 0.0004330761485342858}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:40,668] Trial 25 finished with value: 0.6378698224852071 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 6.537261909307648e-05, 'learning_rate': 'adaptive', 'eta0': 0.0022385624921684904}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:41,384] Trial 26 finished with value: 0.6248520710059171 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.007364673043653249, 'learning_rate': 'adaptive', 'eta0': 0.006712232229209226}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:42,081] Trial 27 finished with value: 0.578698224852071 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.08046497058704552, 'learning_rate': 'adaptive', 'eta0': 0.00011839039285091934}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:43,389] Trial 28 finished with value: 0.6071005917159763 and parameters: {'loss': 'log_loss', 'penalty': 'l2', 'alpha': 0.0013188658583821093, 'learning_rate': 'constant', 'eta0': 0.034375007541521924}. Best is trial 12 with value: 0.6402366863905326.\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "[I 2024-02-23 14:55:56,730] Trial 29 finished with value: 0.6236686390532544 and parameters: {'loss': 'squared_hinge', 'penalty': 'l1', 'alpha': 0.00029134865083689667, 'learning_rate': 'optimal', 'eta0': 0.0006642121609630765}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:56,910] Trial 30 finished with value: 0.6011834319526628 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.032258845592292446, 'learning_rate': 'optimal', 'eta0': 0.0026804651341440443}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:58,226] Trial 31 finished with value: 0.6366863905325444 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 7.06948460549565e-05, 'learning_rate': 'adaptive', 'eta0': 0.001718018027072834}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:55:59,544] Trial 32 finished with value: 0.634319526627219 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 6.505559354571794e-05, 'learning_rate': 'adaptive', 'eta0': 0.001867801025109297}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:01,412] Trial 33 finished with value: 0.6248520710059171 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.00038760355262999464, 'learning_rate': 'adaptive', 'eta0': 0.006651350798821405}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:06,056] Trial 34 finished with value: 0.5798816568047337 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 2.1071298551329704e-05, 'learning_rate': 'adaptive', 'eta0': 0.010210483296183508}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:07,225] Trial 35 finished with value: 0.6189349112426035 and parameters: {'loss': 'perceptron', 'penalty': 'l1', 'alpha': 7.736999341057069e-06, 'learning_rate': 'adaptive', 'eta0': 0.0029448639004582508}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:09,600] Trial 36 finished with value: 0.5928994082840238 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.0026671144238702214, 'learning_rate': 'adaptive', 'eta0': 0.0011604093409858774}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:09,866] Trial 37 finished with value: 0.5644970414201185 and parameters: {'loss': 'perceptron', 'penalty': 'elasticnet', 'alpha': 5.644827027516843e-05, 'learning_rate': 'invscaling', 'eta0': 0.029255948593762352}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:10,515] Trial 38 finished with value: 0.21183431952662723 and parameters: {'loss': 'log_loss', 'penalty': 'l2', 'alpha': 0.00012260680270162087, 'learning_rate': 'constant', 'eta0': 0.0005392721969365657}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:12,228] Trial 39 finished with value: 0.6260355029585799 and parameters: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 0.005929711418721617, 'learning_rate': 'adaptive', 'eta0': 0.002241101465156539}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:12,617] Trial 40 finished with value: 0.5704142011834319 and parameters: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 8.415821172864536e-06, 'learning_rate': 'invscaling', 'eta0': 0.011983875755429117}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:13,305] Trial 41 finished with value: 0.634319526627219 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.00016642578004349873, 'learning_rate': 'adaptive', 'eta0': 0.004656491079260712}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:13,896] Trial 42 finished with value: 0.6402366863905326 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.00021027745577761477, 'learning_rate': 'adaptive', 'eta0': 0.0029852549777640283}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:14,508] Trial 43 finished with value: 0.634319526627219 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0007442069731342302, 'learning_rate': 'adaptive', 'eta0': 0.0010857957839912774}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:15,067] Trial 44 finished with value: 0.6094674556213018 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.00021688534655629272, 'learning_rate': 'adaptive', 'eta0': 0.00046107707639953265}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:15,454] Trial 45 finished with value: 0.6295857988165681 and parameters: {'loss': 'perceptron', 'penalty': 'l2', 'alpha': 0.0001009179054768441, 'learning_rate': 'adaptive', 'eta0': 0.00017486206938871658}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:20,675] Trial 46 finished with value: 0.6378698224852071 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 3.3146059088394704e-05, 'learning_rate': 'adaptive', 'eta0': 0.02154722275175702}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:20,934] Trial 47 finished with value: 0.5609467455621301 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0014153725034913632, 'learning_rate': 'invscaling', 'eta0': 0.008419735749026921}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:21,416] Trial 48 finished with value: 0.6011834319526628 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 1.954458777827079e-06, 'learning_rate': 'optimal', 'eta0': 0.0036844474717363748}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:22,500] Trial 49 finished with value: 0.21183431952662723 and parameters: {'loss': 'log_loss', 'penalty': 'l2', 'alpha': 2.114463473365723e-05, 'learning_rate': 'adaptive', 'eta0': 0.0014580124330189482}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:22,654] Trial 50 finished with value: 0.6082840236686391 and parameters: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0004545261557204174, 'learning_rate': 'constant', 'eta0': 0.002717508624196871}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:28,326] Trial 51 finished with value: 0.6331360946745562 and parameters: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 3.877287533775401e-05, 'learning_rate': 'adaptive', 'eta0': 0.022913617363996656}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:33,005] Trial 52 finished with value: 0.6248520710059171 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 1.0698228370387801e-05, 'learning_rate': 'adaptive', 'eta0': 0.06226822152759029}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:38,159] Trial 53 finished with value: 0.6355029585798817 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 8.84965110769968e-05, 'learning_rate': 'adaptive', 'eta0': 0.01415445854699769}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:43,316] Trial 54 finished with value: 0.6378698224852071 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 2.8894937784095334e-05, 'learning_rate': 'adaptive', 'eta0': 0.02163092251181473}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:49,656] Trial 55 finished with value: 0.6153846153846154 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 0.0002309562685886662, 'learning_rate': 'adaptive', 'eta0': 0.0008775228233492432}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:55,810] Trial 56 finished with value: 0.6402366863905324 and parameters: {'loss': 'squared_hinge', 'penalty': 'elasticnet', 'alpha': 3.154405537556414e-06, 'learning_rate': 'adaptive', 'eta0': 0.0021179364445815343}. Best is trial 12 with value: 0.6402366863905326.\n",
      "[I 2024-02-23 14:56:58,731] Trial 57 finished with value: 0.6414201183431951 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 0.0006906327812424678, 'learning_rate': 'adaptive', 'eta0': 0.004159905231008274}. Best is trial 57 with value: 0.6414201183431951.\n",
      "[I 2024-02-23 14:57:01,832] Trial 58 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.7616571024226938e-06, 'learning_rate': 'adaptive', 'eta0': 0.00461086024328032}. Best is trial 58 with value: 0.642603550295858.\n",
      "[I 2024-02-23 14:57:04,925] Trial 59 finished with value: 0.6437869822485207 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 3.320710986324315e-06, 'learning_rate': 'adaptive', 'eta0': 0.00438133371029827}. Best is trial 59 with value: 0.6437869822485207.\n",
      "[I 2024-02-23 14:57:05,361] Trial 60 finished with value: 0.21183431952662723 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 4.318127424023731e-06, 'learning_rate': 'invscaling', 'eta0': 0.0045700892913599425}. Best is trial 59 with value: 0.6437869822485207.\n",
      "[I 2024-02-23 14:57:08,133] Trial 61 finished with value: 0.6449704142011834 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.3555193064698493e-06, 'learning_rate': 'adaptive', 'eta0': 0.007887094167831584}. Best is trial 61 with value: 0.6449704142011834.\n",
      "[I 2024-02-23 14:57:10,957] Trial 62 finished with value: 0.6461538461538462 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.6545371602330262e-06, 'learning_rate': 'adaptive', 'eta0': 0.007698135990041223}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:13,668] Trial 63 finished with value: 0.6402366863905324 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.0019152643615249e-06, 'learning_rate': 'adaptive', 'eta0': 0.00876192414608011}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:16,542] Trial 64 finished with value: 0.6437869822485207 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.2221949881261483e-06, 'learning_rate': 'adaptive', 'eta0': 0.006776554100679505}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:19,357] Trial 65 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.6545418136934685e-06, 'learning_rate': 'adaptive', 'eta0': 0.007390816994505168}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:22,141] Trial 66 finished with value: 0.6449704142011834 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.5613264053668994e-06, 'learning_rate': 'adaptive', 'eta0': 0.007921396892606392}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:22,673] Trial 67 finished with value: 0.5822485207100592 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.8308852509923897e-06, 'learning_rate': 'optimal', 'eta0': 0.018315228347967154}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:25,435] Trial 68 finished with value: 0.6449704142011834 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.923972865332898e-06, 'learning_rate': 'adaptive', 'eta0': 0.007984988369010196}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:27,354] Trial 69 finished with value: 0.6378698224852071 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 5.538329516512447e-06, 'learning_rate': 'constant', 'eta0': 0.01299119478567728}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:30,332] Trial 70 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.8976081721257722e-06, 'learning_rate': 'adaptive', 'eta0': 0.00568235854222267}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:33,162] Trial 71 finished with value: 0.6461538461538462 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.0000114129509014e-06, 'learning_rate': 'adaptive', 'eta0': 0.007693697022145743}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:35,759] Trial 72 finished with value: 0.6343195266272189 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.0334385120388943e-06, 'learning_rate': 'adaptive', 'eta0': 0.009977625565760845}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:38,265] Trial 73 finished with value: 0.6319526627218935 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.5890867701116983e-06, 'learning_rate': 'adaptive', 'eta0': 0.015303533813411315}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:40,603] Trial 74 finished with value: 0.6343195266272189 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 3.675053250274435e-06, 'learning_rate': 'adaptive', 'eta0': 0.03845990255031534}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:43,466] Trial 75 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 5.489197794074045e-06, 'learning_rate': 'adaptive', 'eta0': 0.0069555067326768924}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:46,091] Trial 76 finished with value: 0.6331360946745562 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.0015487821510132e-06, 'learning_rate': 'adaptive', 'eta0': 0.010652745235428104}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:49,060] Trial 77 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.2169108223472496e-05, 'learning_rate': 'adaptive', 'eta0': 0.005725439378179114}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:51,629] Trial 78 finished with value: 0.6355029585798817 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 6.712234488232065e-06, 'learning_rate': 'adaptive', 'eta0': 0.016325517713274177}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:52,106] Trial 79 finished with value: 0.5857988165680472 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.4085252336433864e-06, 'learning_rate': 'optimal', 'eta0': 0.003499540185077609}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:54,859] Trial 80 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.430315773822764e-06, 'learning_rate': 'adaptive', 'eta0': 0.00811692482713877}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:57:57,610] Trial 81 finished with value: 0.6414201183431952 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.5728819742460055e-06, 'learning_rate': 'adaptive', 'eta0': 0.008217199333106547}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:00,648] Trial 82 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 3.8508681764401255e-06, 'learning_rate': 'adaptive', 'eta0': 0.004964472238746803}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:03,468] Trial 83 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.380838881624293e-06, 'learning_rate': 'adaptive', 'eta0': 0.007401712702687876}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:06,043] Trial 84 finished with value: 0.6378698224852071 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 5.125158506394291e-06, 'learning_rate': 'adaptive', 'eta0': 0.011786116181366664}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:09,045] Trial 85 finished with value: 0.6437869822485207 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.3920214389107398e-06, 'learning_rate': 'adaptive', 'eta0': 0.005220901932982213}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:12,046] Trial 86 finished with value: 0.6426035502958579 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.1793631179450648e-06, 'learning_rate': 'adaptive', 'eta0': 0.005390028730477817}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:12,560] Trial 87 finished with value: 0.21183431952662723 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.9734857758370485e-06, 'learning_rate': 'invscaling', 'eta0': 0.003672108553247853}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:12,678] Trial 88 finished with value: 0.5301775147928994 and parameters: {'loss': 'perceptron', 'penalty': 'l2', 'alpha': 3.538437461801765e-06, 'learning_rate': 'constant', 'eta0': 0.010065546956304615}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:15,991] Trial 89 finished with value: 0.634319526627219 and parameters: {'loss': 'log_loss', 'penalty': 'l2', 'alpha': 9.489498668664933e-06, 'learning_rate': 'adaptive', 'eta0': 0.029003779258033557}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:19,351] Trial 90 finished with value: 0.6319526627218935 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 6.390683497316898e-06, 'learning_rate': 'adaptive', 'eta0': 0.0014956649773087225}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:22,240] Trial 91 finished with value: 0.6449704142011835 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.3835180322120584e-06, 'learning_rate': 'adaptive', 'eta0': 0.006491974650039931}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:25,343] Trial 92 finished with value: 0.6426035502958579 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.6417849965952658e-06, 'learning_rate': 'adaptive', 'eta0': 0.004538160810101424}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:28,566] Trial 93 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.3653172483806027e-06, 'learning_rate': 'adaptive', 'eta0': 0.0031381615495628882}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:31,451] Trial 94 finished with value: 0.6437869822485207 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 4.437910708194503e-06, 'learning_rate': 'adaptive', 'eta0': 0.006727163601079937}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:34,407] Trial 95 finished with value: 0.6402366863905326 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 4.309779516059477e-06, 'learning_rate': 'adaptive', 'eta0': 0.006365176496213275}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:36,943] Trial 96 finished with value: 0.6355029585798817 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.435943922872648e-05, 'learning_rate': 'adaptive', 'eta0': 0.012749283322758148}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:39,566] Trial 97 finished with value: 0.6355029585798817 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 1.3296323442640159e-06, 'learning_rate': 'adaptive', 'eta0': 0.009405206695751607}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:42,097] Trial 98 finished with value: 0.6414201183431953 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 7.405382681005497e-06, 'learning_rate': 'adaptive', 'eta0': 0.01825036599468478}. Best is trial 62 with value: 0.6461538461538462.\n",
      "[I 2024-02-23 14:58:45,072] Trial 99 finished with value: 0.642603550295858 and parameters: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 3.410501360698341e-06, 'learning_rate': 'adaptive', 'eta0': 0.00562905910748539}. Best is trial 62 with value: 0.6461538461538462.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'loss': 'squared_hinge', 'penalty': 'l2', 'alpha': 2.6545371602330262e-06, 'learning_rate': 'adaptive', 'eta0': 0.007698135990041223}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    loss = trial.suggest_categorical('loss', ['hinge', 'log_loss', 'squared_hinge', 'perceptron'])\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2', 'l1', 'elasticnet'])\n",
    "    alpha = trial.suggest_float('alpha', 1e-6, 1e-1, log=True)\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'optimal', 'invscaling', 'adaptive'])\n",
    "    eta0 = trial.suggest_float('eta0', 1e-5, 1e-1, log=True)  # Only relevant for certain learning rates\n",
    "\n",
    "    # Create and train the SGD Classifier\n",
    "    model = SGDClassifier(loss=loss, penalty=penalty, alpha=alpha, learning_rate=learning_rate, eta0=eta0,\n",
    "                          random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Perform cross-validation and return the mean score\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)  # cv=5 for 5-fold cross-validation\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "# Create a study object and specify the optimization direction\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)  # Adjust the number of trials as needed\n",
    "\n",
    "# Best hyperparameters\n",
    "print('Best trial:', study.best_trial.params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:58:45.075343400Z",
     "start_time": "2024-02-23T12:55:04.576219Z"
    }
   },
   "id": "4477cdb859019617",
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class Naive Bayes\n",
    "\n",
    "It is known that Naive Bayes is a simple and effective algorithm for classification for NLP tasks. \n",
    "Let's now train a multi-class Naive Bayes model on the embeddings and the class, to see how it performs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fab4ff2e2e09606"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.70      0.72        44\n",
      "           1       0.55      0.53      0.54        30\n",
      "           2       0.58      0.45      0.51        31\n",
      "           3       0.74      0.85      0.79        27\n",
      "           4       0.51      0.61      0.56        31\n",
      "           5       0.61      0.61      0.61        49\n",
      "\n",
      "    accuracy                           0.63       212\n",
      "   macro avg       0.62      0.63      0.62       212\n",
      "weighted avg       0.63      0.63      0.63       212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T00:02:04.765241900Z",
     "start_time": "2024-02-22T00:02:04.749418400Z"
    }
   },
   "id": "fa13a14098b3636c",
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the Naive Bayes model has an accuracy of 0.63, which does not differ much from the logistic regression model.\n",
    "So there is no need to use `optuna` to find the best hyperparameters for the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e859bc93b6dcd506"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class Random Forest\n",
    "\n",
    "Let's now train a multi-class Random Forest model on the embeddings and the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c9e62a3e6912a48"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.67        44\n",
      "           1       0.61      0.47      0.53        30\n",
      "           2       0.86      0.39      0.53        31\n",
      "           3       0.77      0.63      0.69        27\n",
      "           4       0.56      0.77      0.65        31\n",
      "           5       0.52      0.71      0.60        49\n",
      "\n",
      "    accuracy                           0.62       212\n",
      "   macro avg       0.67      0.61      0.61       212\n",
      "weighted avg       0.65      0.62      0.61       212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=15,\n",
    "                               max_depth=100, min_samples_split=2, min_samples_leaf=1, max_features='sqrt')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T23:59:20.973163800Z",
     "start_time": "2024-02-21T23:59:19.524753300Z"
    }
   },
   "id": "a8fc25a6a41a309b",
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-class CatBoost\n",
    "\n",
    "Having tried the above models, let us now proceed to more advanced models such as CatBoost, \n",
    "which is a gradient boosting library that is known to perform well on tabular data, to see how it performs \n",
    "and if it can outperform the other models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28cda8d8bf460600"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.134538\n",
      "0:\tlearn: 1.7228745\ttotal: 25.9ms\tremaining: 10.3s\n",
      "1:\tlearn: 1.6604869\ttotal: 47.1ms\tremaining: 9.37s\n",
      "2:\tlearn: 1.6080402\ttotal: 68.8ms\tremaining: 9.11s\n",
      "3:\tlearn: 1.5585689\ttotal: 89.7ms\tremaining: 8.88s\n",
      "4:\tlearn: 1.5091478\ttotal: 110ms\tremaining: 8.72s\n",
      "5:\tlearn: 1.4662697\ttotal: 131ms\tremaining: 8.62s\n",
      "6:\tlearn: 1.4287979\ttotal: 152ms\tremaining: 8.52s\n",
      "7:\tlearn: 1.3910584\ttotal: 173ms\tremaining: 8.46s\n",
      "8:\tlearn: 1.3552419\ttotal: 194ms\tremaining: 8.41s\n",
      "9:\tlearn: 1.3165946\ttotal: 214ms\tremaining: 8.36s\n",
      "10:\tlearn: 1.2851901\ttotal: 235ms\tremaining: 8.32s\n",
      "11:\tlearn: 1.2504925\ttotal: 257ms\tremaining: 8.31s\n",
      "12:\tlearn: 1.2190597\ttotal: 279ms\tremaining: 8.31s\n",
      "13:\tlearn: 1.1924688\ttotal: 301ms\tremaining: 8.29s\n",
      "14:\tlearn: 1.1640413\ttotal: 323ms\tremaining: 8.3s\n",
      "15:\tlearn: 1.1362085\ttotal: 346ms\tremaining: 8.29s\n",
      "16:\tlearn: 1.1100424\ttotal: 367ms\tremaining: 8.27s\n",
      "17:\tlearn: 1.0878752\ttotal: 388ms\tremaining: 8.22s\n",
      "18:\tlearn: 1.0624523\ttotal: 408ms\tremaining: 8.19s\n",
      "19:\tlearn: 1.0358713\ttotal: 429ms\tremaining: 8.16s\n",
      "20:\tlearn: 1.0105685\ttotal: 451ms\tremaining: 8.14s\n",
      "21:\tlearn: 0.9889355\ttotal: 471ms\tremaining: 8.1s\n",
      "22:\tlearn: 0.9693468\ttotal: 492ms\tremaining: 8.07s\n",
      "23:\tlearn: 0.9472586\ttotal: 513ms\tremaining: 8.04s\n",
      "24:\tlearn: 0.9278074\ttotal: 535ms\tremaining: 8.02s\n",
      "25:\tlearn: 0.9062782\ttotal: 556ms\tremaining: 7.99s\n",
      "26:\tlearn: 0.8873076\ttotal: 576ms\tremaining: 7.96s\n",
      "27:\tlearn: 0.8661164\ttotal: 597ms\tremaining: 7.94s\n",
      "28:\tlearn: 0.8476778\ttotal: 619ms\tremaining: 7.92s\n",
      "29:\tlearn: 0.8313398\ttotal: 640ms\tremaining: 7.89s\n",
      "30:\tlearn: 0.8165745\ttotal: 660ms\tremaining: 7.85s\n",
      "31:\tlearn: 0.8015096\ttotal: 681ms\tremaining: 7.84s\n",
      "32:\tlearn: 0.7840770\ttotal: 704ms\tremaining: 7.82s\n",
      "33:\tlearn: 0.7662083\ttotal: 725ms\tremaining: 7.8s\n",
      "34:\tlearn: 0.7498120\ttotal: 746ms\tremaining: 7.78s\n",
      "35:\tlearn: 0.7370386\ttotal: 766ms\tremaining: 7.74s\n",
      "36:\tlearn: 0.7256692\ttotal: 787ms\tremaining: 7.72s\n",
      "37:\tlearn: 0.7139216\ttotal: 804ms\tremaining: 7.66s\n",
      "38:\tlearn: 0.6983130\ttotal: 826ms\tremaining: 7.64s\n",
      "39:\tlearn: 0.6876474\ttotal: 846ms\tremaining: 7.61s\n",
      "40:\tlearn: 0.6749084\ttotal: 867ms\tremaining: 7.59s\n",
      "41:\tlearn: 0.6631446\ttotal: 887ms\tremaining: 7.56s\n",
      "42:\tlearn: 0.6547389\ttotal: 907ms\tremaining: 7.53s\n",
      "43:\tlearn: 0.6416111\ttotal: 928ms\tremaining: 7.51s\n",
      "44:\tlearn: 0.6300260\ttotal: 950ms\tremaining: 7.49s\n",
      "45:\tlearn: 0.6227455\ttotal: 968ms\tremaining: 7.45s\n",
      "46:\tlearn: 0.6116992\ttotal: 985ms\tremaining: 7.4s\n",
      "47:\tlearn: 0.6017839\ttotal: 1s\tremaining: 7.37s\n",
      "48:\tlearn: 0.5913679\ttotal: 1.03s\tremaining: 7.36s\n",
      "49:\tlearn: 0.5811772\ttotal: 1.05s\tremaining: 7.33s\n",
      "50:\tlearn: 0.5744012\ttotal: 1.06s\tremaining: 7.29s\n",
      "51:\tlearn: 0.5661624\ttotal: 1.09s\tremaining: 7.27s\n",
      "52:\tlearn: 0.5583602\ttotal: 1.1s\tremaining: 7.23s\n",
      "53:\tlearn: 0.5498467\ttotal: 1.12s\tremaining: 7.21s\n",
      "54:\tlearn: 0.5410277\ttotal: 1.15s\tremaining: 7.19s\n",
      "55:\tlearn: 0.5344122\ttotal: 1.17s\tremaining: 7.16s\n",
      "56:\tlearn: 0.5256119\ttotal: 1.19s\tremaining: 7.14s\n",
      "57:\tlearn: 0.5168590\ttotal: 1.21s\tremaining: 7.11s\n",
      "58:\tlearn: 0.5090309\ttotal: 1.22s\tremaining: 7.08s\n",
      "59:\tlearn: 0.5007300\ttotal: 1.24s\tremaining: 7.05s\n",
      "60:\tlearn: 0.4930892\ttotal: 1.26s\tremaining: 7.02s\n",
      "61:\tlearn: 0.4862400\ttotal: 1.28s\tremaining: 7s\n",
      "62:\tlearn: 0.4796660\ttotal: 1.3s\tremaining: 6.97s\n",
      "63:\tlearn: 0.4744049\ttotal: 1.32s\tremaining: 6.93s\n",
      "64:\tlearn: 0.4672496\ttotal: 1.34s\tremaining: 6.91s\n",
      "65:\tlearn: 0.4599536\ttotal: 1.36s\tremaining: 6.9s\n",
      "66:\tlearn: 0.4526283\ttotal: 1.38s\tremaining: 6.88s\n",
      "67:\tlearn: 0.4464831\ttotal: 1.4s\tremaining: 6.85s\n",
      "68:\tlearn: 0.4403173\ttotal: 1.42s\tremaining: 6.83s\n",
      "69:\tlearn: 0.4353264\ttotal: 1.44s\tremaining: 6.79s\n",
      "70:\tlearn: 0.4292748\ttotal: 1.46s\tremaining: 6.77s\n",
      "71:\tlearn: 0.4231607\ttotal: 1.48s\tremaining: 6.74s\n",
      "72:\tlearn: 0.4173319\ttotal: 1.5s\tremaining: 6.71s\n",
      "73:\tlearn: 0.4120874\ttotal: 1.52s\tremaining: 6.69s\n",
      "74:\tlearn: 0.4075504\ttotal: 1.54s\tremaining: 6.66s\n",
      "75:\tlearn: 0.4036470\ttotal: 1.55s\tremaining: 6.63s\n",
      "76:\tlearn: 0.3971942\ttotal: 1.58s\tremaining: 6.61s\n",
      "77:\tlearn: 0.3923635\ttotal: 1.59s\tremaining: 6.59s\n",
      "78:\tlearn: 0.3871207\ttotal: 1.62s\tremaining: 6.57s\n",
      "79:\tlearn: 0.3824068\ttotal: 1.64s\tremaining: 6.54s\n",
      "80:\tlearn: 0.3781196\ttotal: 1.65s\tremaining: 6.52s\n",
      "81:\tlearn: 0.3720005\ttotal: 1.67s\tremaining: 6.49s\n",
      "82:\tlearn: 0.3662377\ttotal: 1.69s\tremaining: 6.47s\n",
      "83:\tlearn: 0.3621305\ttotal: 1.71s\tremaining: 6.44s\n",
      "84:\tlearn: 0.3580423\ttotal: 1.73s\tremaining: 6.41s\n",
      "85:\tlearn: 0.3542212\ttotal: 1.75s\tremaining: 6.39s\n",
      "86:\tlearn: 0.3493992\ttotal: 1.77s\tremaining: 6.37s\n",
      "87:\tlearn: 0.3446388\ttotal: 1.79s\tremaining: 6.35s\n",
      "88:\tlearn: 0.3398332\ttotal: 1.81s\tremaining: 6.33s\n",
      "89:\tlearn: 0.3350392\ttotal: 1.83s\tremaining: 6.31s\n",
      "90:\tlearn: 0.3310729\ttotal: 1.85s\tremaining: 6.29s\n",
      "91:\tlearn: 0.3277042\ttotal: 1.87s\tremaining: 6.25s\n",
      "92:\tlearn: 0.3239233\ttotal: 1.89s\tremaining: 6.23s\n",
      "93:\tlearn: 0.3200518\ttotal: 1.91s\tremaining: 6.21s\n",
      "94:\tlearn: 0.3159525\ttotal: 1.93s\tremaining: 6.19s\n",
      "95:\tlearn: 0.3117227\ttotal: 1.95s\tremaining: 6.17s\n",
      "96:\tlearn: 0.3078341\ttotal: 1.97s\tremaining: 6.14s\n",
      "97:\tlearn: 0.3037615\ttotal: 1.99s\tremaining: 6.13s\n",
      "98:\tlearn: 0.3009603\ttotal: 2.01s\tremaining: 6.11s\n",
      "99:\tlearn: 0.2972008\ttotal: 2.03s\tremaining: 6.08s\n",
      "100:\tlearn: 0.2940256\ttotal: 2.04s\tremaining: 6.05s\n",
      "101:\tlearn: 0.2912744\ttotal: 2.06s\tremaining: 6.03s\n",
      "102:\tlearn: 0.2874840\ttotal: 2.08s\tremaining: 6s\n",
      "103:\tlearn: 0.2851025\ttotal: 2.1s\tremaining: 5.97s\n",
      "104:\tlearn: 0.2817369\ttotal: 2.12s\tremaining: 5.96s\n",
      "105:\tlearn: 0.2787950\ttotal: 2.14s\tremaining: 5.93s\n",
      "106:\tlearn: 0.2753153\ttotal: 2.16s\tremaining: 5.91s\n",
      "107:\tlearn: 0.2726117\ttotal: 2.18s\tremaining: 5.89s\n",
      "108:\tlearn: 0.2700277\ttotal: 2.2s\tremaining: 5.86s\n",
      "109:\tlearn: 0.2671003\ttotal: 2.22s\tremaining: 5.84s\n",
      "110:\tlearn: 0.2641920\ttotal: 2.24s\tremaining: 5.82s\n",
      "111:\tlearn: 0.2622460\ttotal: 2.25s\tremaining: 5.8s\n",
      "112:\tlearn: 0.2597229\ttotal: 2.27s\tremaining: 5.77s\n",
      "113:\tlearn: 0.2567605\ttotal: 2.29s\tremaining: 5.75s\n",
      "114:\tlearn: 0.2538125\ttotal: 2.31s\tremaining: 5.74s\n",
      "115:\tlearn: 0.2512457\ttotal: 2.33s\tremaining: 5.72s\n",
      "116:\tlearn: 0.2493987\ttotal: 2.35s\tremaining: 5.69s\n",
      "117:\tlearn: 0.2470901\ttotal: 2.37s\tremaining: 5.67s\n",
      "118:\tlearn: 0.2436367\ttotal: 2.39s\tremaining: 5.64s\n",
      "119:\tlearn: 0.2412490\ttotal: 2.41s\tremaining: 5.62s\n",
      "120:\tlearn: 0.2382601\ttotal: 2.43s\tremaining: 5.6s\n",
      "121:\tlearn: 0.2358525\ttotal: 2.45s\tremaining: 5.57s\n",
      "122:\tlearn: 0.2335635\ttotal: 2.46s\tremaining: 5.55s\n",
      "123:\tlearn: 0.2313625\ttotal: 2.48s\tremaining: 5.53s\n",
      "124:\tlearn: 0.2298685\ttotal: 2.5s\tremaining: 5.5s\n",
      "125:\tlearn: 0.2276978\ttotal: 2.52s\tremaining: 5.48s\n",
      "126:\tlearn: 0.2262614\ttotal: 2.54s\tremaining: 5.45s\n",
      "127:\tlearn: 0.2237244\ttotal: 2.55s\tremaining: 5.43s\n",
      "128:\tlearn: 0.2216352\ttotal: 2.57s\tremaining: 5.41s\n",
      "129:\tlearn: 0.2196897\ttotal: 2.59s\tremaining: 5.38s\n",
      "130:\tlearn: 0.2182188\ttotal: 2.61s\tremaining: 5.36s\n",
      "131:\tlearn: 0.2159438\ttotal: 2.63s\tremaining: 5.33s\n",
      "132:\tlearn: 0.2135043\ttotal: 2.65s\tremaining: 5.31s\n",
      "133:\tlearn: 0.2115320\ttotal: 2.67s\tremaining: 5.3s\n",
      "134:\tlearn: 0.2094532\ttotal: 2.69s\tremaining: 5.27s\n",
      "135:\tlearn: 0.2079651\ttotal: 2.71s\tremaining: 5.25s\n",
      "136:\tlearn: 0.2062597\ttotal: 2.72s\tremaining: 5.23s\n",
      "137:\tlearn: 0.2040880\ttotal: 2.74s\tremaining: 5.21s\n",
      "138:\tlearn: 0.2021655\ttotal: 2.76s\tremaining: 5.18s\n",
      "139:\tlearn: 0.2006026\ttotal: 2.77s\tremaining: 5.16s\n",
      "140:\tlearn: 0.1985812\ttotal: 2.79s\tremaining: 5.13s\n",
      "141:\tlearn: 0.1965836\ttotal: 2.81s\tremaining: 5.11s\n",
      "142:\tlearn: 0.1948308\ttotal: 2.83s\tremaining: 5.08s\n",
      "143:\tlearn: 0.1927285\ttotal: 2.85s\tremaining: 5.07s\n",
      "144:\tlearn: 0.1907884\ttotal: 2.87s\tremaining: 5.05s\n",
      "145:\tlearn: 0.1892524\ttotal: 2.89s\tremaining: 5.03s\n",
      "146:\tlearn: 0.1876610\ttotal: 2.91s\tremaining: 5.01s\n",
      "147:\tlearn: 0.1858838\ttotal: 2.93s\tremaining: 4.99s\n",
      "148:\tlearn: 0.1840987\ttotal: 2.95s\tremaining: 4.96s\n",
      "149:\tlearn: 0.1826783\ttotal: 2.96s\tremaining: 4.94s\n",
      "150:\tlearn: 0.1809587\ttotal: 2.98s\tremaining: 4.92s\n",
      "151:\tlearn: 0.1793601\ttotal: 3s\tremaining: 4.9s\n",
      "152:\tlearn: 0.1783546\ttotal: 3.02s\tremaining: 4.87s\n",
      "153:\tlearn: 0.1774691\ttotal: 3.04s\tremaining: 4.85s\n",
      "154:\tlearn: 0.1758001\ttotal: 3.05s\tremaining: 4.83s\n",
      "155:\tlearn: 0.1744380\ttotal: 3.07s\tremaining: 4.81s\n",
      "156:\tlearn: 0.1731060\ttotal: 3.09s\tremaining: 4.79s\n",
      "157:\tlearn: 0.1715699\ttotal: 3.11s\tremaining: 4.77s\n",
      "158:\tlearn: 0.1696787\ttotal: 3.13s\tremaining: 4.75s\n",
      "159:\tlearn: 0.1682388\ttotal: 3.15s\tremaining: 4.73s\n",
      "160:\tlearn: 0.1673331\ttotal: 3.17s\tremaining: 4.7s\n",
      "161:\tlearn: 0.1659585\ttotal: 3.19s\tremaining: 4.68s\n",
      "162:\tlearn: 0.1643622\ttotal: 3.2s\tremaining: 4.66s\n",
      "163:\tlearn: 0.1636660\ttotal: 3.22s\tremaining: 4.63s\n",
      "164:\tlearn: 0.1621060\ttotal: 3.24s\tremaining: 4.61s\n",
      "165:\tlearn: 0.1607724\ttotal: 3.26s\tremaining: 4.59s\n",
      "166:\tlearn: 0.1594912\ttotal: 3.27s\tremaining: 4.57s\n",
      "167:\tlearn: 0.1576651\ttotal: 3.3s\tremaining: 4.55s\n",
      "168:\tlearn: 0.1567447\ttotal: 3.31s\tremaining: 4.53s\n",
      "169:\tlearn: 0.1552850\ttotal: 3.33s\tremaining: 4.51s\n",
      "170:\tlearn: 0.1543368\ttotal: 3.35s\tremaining: 4.49s\n",
      "171:\tlearn: 0.1526851\ttotal: 3.37s\tremaining: 4.46s\n",
      "172:\tlearn: 0.1516548\ttotal: 3.39s\tremaining: 4.44s\n",
      "173:\tlearn: 0.1503451\ttotal: 3.4s\tremaining: 4.42s\n",
      "174:\tlearn: 0.1489448\ttotal: 3.42s\tremaining: 4.4s\n",
      "175:\tlearn: 0.1479130\ttotal: 3.44s\tremaining: 4.38s\n",
      "176:\tlearn: 0.1468269\ttotal: 3.46s\tremaining: 4.35s\n",
      "177:\tlearn: 0.1457061\ttotal: 3.48s\tremaining: 4.33s\n",
      "178:\tlearn: 0.1447622\ttotal: 3.49s\tremaining: 4.31s\n",
      "179:\tlearn: 0.1438747\ttotal: 3.51s\tremaining: 4.29s\n",
      "180:\tlearn: 0.1430759\ttotal: 3.53s\tremaining: 4.27s\n",
      "181:\tlearn: 0.1418740\ttotal: 3.55s\tremaining: 4.25s\n",
      "182:\tlearn: 0.1406074\ttotal: 3.57s\tremaining: 4.23s\n",
      "183:\tlearn: 0.1393027\ttotal: 3.59s\tremaining: 4.21s\n",
      "184:\tlearn: 0.1384271\ttotal: 3.61s\tremaining: 4.19s\n",
      "185:\tlearn: 0.1379717\ttotal: 3.62s\tremaining: 4.17s\n",
      "186:\tlearn: 0.1372064\ttotal: 3.64s\tremaining: 4.15s\n",
      "187:\tlearn: 0.1363515\ttotal: 3.66s\tremaining: 4.13s\n",
      "188:\tlearn: 0.1353696\ttotal: 3.68s\tremaining: 4.1s\n",
      "189:\tlearn: 0.1345634\ttotal: 3.69s\tremaining: 4.08s\n",
      "190:\tlearn: 0.1334954\ttotal: 3.71s\tremaining: 4.06s\n",
      "191:\tlearn: 0.1325677\ttotal: 3.73s\tremaining: 4.04s\n",
      "192:\tlearn: 0.1315723\ttotal: 3.75s\tremaining: 4.02s\n",
      "193:\tlearn: 0.1309626\ttotal: 3.76s\tremaining: 4s\n",
      "194:\tlearn: 0.1300535\ttotal: 3.78s\tremaining: 3.98s\n",
      "195:\tlearn: 0.1288905\ttotal: 3.8s\tremaining: 3.96s\n",
      "196:\tlearn: 0.1277924\ttotal: 3.82s\tremaining: 3.94s\n",
      "197:\tlearn: 0.1265409\ttotal: 3.84s\tremaining: 3.92s\n",
      "198:\tlearn: 0.1255739\ttotal: 3.86s\tremaining: 3.9s\n",
      "199:\tlearn: 0.1248684\ttotal: 3.88s\tremaining: 3.88s\n",
      "200:\tlearn: 0.1241837\ttotal: 3.89s\tremaining: 3.86s\n",
      "201:\tlearn: 0.1232042\ttotal: 3.91s\tremaining: 3.83s\n",
      "202:\tlearn: 0.1222603\ttotal: 3.93s\tremaining: 3.81s\n",
      "203:\tlearn: 0.1213459\ttotal: 3.95s\tremaining: 3.79s\n",
      "204:\tlearn: 0.1205817\ttotal: 3.97s\tremaining: 3.78s\n",
      "205:\tlearn: 0.1198251\ttotal: 3.99s\tremaining: 3.76s\n",
      "206:\tlearn: 0.1189689\ttotal: 4.01s\tremaining: 3.74s\n",
      "207:\tlearn: 0.1181024\ttotal: 4.03s\tremaining: 3.72s\n",
      "208:\tlearn: 0.1172190\ttotal: 4.04s\tremaining: 3.69s\n",
      "209:\tlearn: 0.1166187\ttotal: 4.06s\tremaining: 3.67s\n",
      "210:\tlearn: 0.1158746\ttotal: 4.08s\tremaining: 3.65s\n",
      "211:\tlearn: 0.1153167\ttotal: 4.1s\tremaining: 3.63s\n",
      "212:\tlearn: 0.1148056\ttotal: 4.11s\tremaining: 3.61s\n",
      "213:\tlearn: 0.1139654\ttotal: 4.13s\tremaining: 3.59s\n",
      "214:\tlearn: 0.1133325\ttotal: 4.15s\tremaining: 3.57s\n",
      "215:\tlearn: 0.1127448\ttotal: 4.17s\tremaining: 3.55s\n",
      "216:\tlearn: 0.1118552\ttotal: 4.19s\tremaining: 3.53s\n",
      "217:\tlearn: 0.1109735\ttotal: 4.21s\tremaining: 3.51s\n",
      "218:\tlearn: 0.1101663\ttotal: 4.23s\tremaining: 3.49s\n",
      "219:\tlearn: 0.1094057\ttotal: 4.25s\tremaining: 3.47s\n",
      "220:\tlearn: 0.1086842\ttotal: 4.27s\tremaining: 3.46s\n",
      "221:\tlearn: 0.1081635\ttotal: 4.29s\tremaining: 3.44s\n",
      "222:\tlearn: 0.1074615\ttotal: 4.3s\tremaining: 3.42s\n",
      "223:\tlearn: 0.1067836\ttotal: 4.32s\tremaining: 3.4s\n",
      "224:\tlearn: 0.1061558\ttotal: 4.34s\tremaining: 3.38s\n",
      "225:\tlearn: 0.1054296\ttotal: 4.36s\tremaining: 3.36s\n",
      "226:\tlearn: 0.1047350\ttotal: 4.38s\tremaining: 3.34s\n",
      "227:\tlearn: 0.1040358\ttotal: 4.4s\tremaining: 3.32s\n",
      "228:\tlearn: 0.1033799\ttotal: 4.41s\tremaining: 3.3s\n",
      "229:\tlearn: 0.1028751\ttotal: 4.43s\tremaining: 3.28s\n",
      "230:\tlearn: 0.1021686\ttotal: 4.45s\tremaining: 3.26s\n",
      "231:\tlearn: 0.1014326\ttotal: 4.47s\tremaining: 3.23s\n",
      "232:\tlearn: 0.1009049\ttotal: 4.49s\tremaining: 3.21s\n",
      "233:\tlearn: 0.1003008\ttotal: 4.5s\tremaining: 3.19s\n",
      "234:\tlearn: 0.0999129\ttotal: 4.52s\tremaining: 3.17s\n",
      "235:\tlearn: 0.0993210\ttotal: 4.54s\tremaining: 3.15s\n",
      "236:\tlearn: 0.0988379\ttotal: 4.56s\tremaining: 3.13s\n",
      "237:\tlearn: 0.0982119\ttotal: 4.58s\tremaining: 3.11s\n",
      "238:\tlearn: 0.0976786\ttotal: 4.59s\tremaining: 3.1s\n",
      "239:\tlearn: 0.0970612\ttotal: 4.61s\tremaining: 3.08s\n",
      "240:\tlearn: 0.0966120\ttotal: 4.63s\tremaining: 3.06s\n",
      "241:\tlearn: 0.0959420\ttotal: 4.65s\tremaining: 3.04s\n",
      "242:\tlearn: 0.0953684\ttotal: 4.67s\tremaining: 3.02s\n",
      "243:\tlearn: 0.0947231\ttotal: 4.69s\tremaining: 3s\n",
      "244:\tlearn: 0.0942221\ttotal: 4.7s\tremaining: 2.98s\n",
      "245:\tlearn: 0.0936252\ttotal: 4.72s\tremaining: 2.96s\n",
      "246:\tlearn: 0.0930829\ttotal: 4.74s\tremaining: 2.94s\n",
      "247:\tlearn: 0.0926261\ttotal: 4.76s\tremaining: 2.92s\n",
      "248:\tlearn: 0.0920309\ttotal: 4.78s\tremaining: 2.9s\n",
      "249:\tlearn: 0.0913719\ttotal: 4.79s\tremaining: 2.88s\n",
      "250:\tlearn: 0.0908123\ttotal: 4.81s\tremaining: 2.86s\n",
      "251:\tlearn: 0.0901615\ttotal: 4.83s\tremaining: 2.84s\n",
      "252:\tlearn: 0.0895904\ttotal: 4.85s\tremaining: 2.82s\n",
      "253:\tlearn: 0.0889963\ttotal: 4.87s\tremaining: 2.8s\n",
      "254:\tlearn: 0.0884139\ttotal: 4.89s\tremaining: 2.78s\n",
      "255:\tlearn: 0.0880217\ttotal: 4.91s\tremaining: 2.76s\n",
      "256:\tlearn: 0.0875964\ttotal: 4.92s\tremaining: 2.74s\n",
      "257:\tlearn: 0.0870218\ttotal: 4.94s\tremaining: 2.72s\n",
      "258:\tlearn: 0.0865999\ttotal: 4.96s\tremaining: 2.7s\n",
      "259:\tlearn: 0.0860540\ttotal: 4.98s\tremaining: 2.68s\n",
      "260:\tlearn: 0.0853690\ttotal: 5s\tremaining: 2.66s\n",
      "261:\tlearn: 0.0847425\ttotal: 5.02s\tremaining: 2.64s\n",
      "262:\tlearn: 0.0841182\ttotal: 5.03s\tremaining: 2.62s\n",
      "263:\tlearn: 0.0835568\ttotal: 5.05s\tremaining: 2.6s\n",
      "264:\tlearn: 0.0830860\ttotal: 5.08s\tremaining: 2.58s\n",
      "265:\tlearn: 0.0826953\ttotal: 5.09s\tremaining: 2.56s\n",
      "266:\tlearn: 0.0821504\ttotal: 5.11s\tremaining: 2.54s\n",
      "267:\tlearn: 0.0816057\ttotal: 5.13s\tremaining: 2.53s\n",
      "268:\tlearn: 0.0811668\ttotal: 5.15s\tremaining: 2.51s\n",
      "269:\tlearn: 0.0806370\ttotal: 5.17s\tremaining: 2.49s\n",
      "270:\tlearn: 0.0803147\ttotal: 5.18s\tremaining: 2.47s\n",
      "271:\tlearn: 0.0799147\ttotal: 5.2s\tremaining: 2.45s\n",
      "272:\tlearn: 0.0796684\ttotal: 5.22s\tremaining: 2.43s\n",
      "273:\tlearn: 0.0791950\ttotal: 5.24s\tremaining: 2.41s\n",
      "274:\tlearn: 0.0787248\ttotal: 5.26s\tremaining: 2.39s\n",
      "275:\tlearn: 0.0782419\ttotal: 5.28s\tremaining: 2.37s\n",
      "276:\tlearn: 0.0780074\ttotal: 5.29s\tremaining: 2.35s\n",
      "277:\tlearn: 0.0775958\ttotal: 5.31s\tremaining: 2.33s\n",
      "278:\tlearn: 0.0771180\ttotal: 5.33s\tremaining: 2.31s\n",
      "279:\tlearn: 0.0767341\ttotal: 5.35s\tremaining: 2.29s\n",
      "280:\tlearn: 0.0763011\ttotal: 5.37s\tremaining: 2.27s\n",
      "281:\tlearn: 0.0758194\ttotal: 5.38s\tremaining: 2.25s\n",
      "282:\tlearn: 0.0754358\ttotal: 5.4s\tremaining: 2.23s\n",
      "283:\tlearn: 0.0750667\ttotal: 5.42s\tremaining: 2.21s\n",
      "284:\tlearn: 0.0747107\ttotal: 5.44s\tremaining: 2.19s\n",
      "285:\tlearn: 0.0742531\ttotal: 5.46s\tremaining: 2.17s\n",
      "286:\tlearn: 0.0737926\ttotal: 5.48s\tremaining: 2.16s\n",
      "287:\tlearn: 0.0735905\ttotal: 5.49s\tremaining: 2.14s\n",
      "288:\tlearn: 0.0732208\ttotal: 5.51s\tremaining: 2.12s\n",
      "289:\tlearn: 0.0727946\ttotal: 5.53s\tremaining: 2.1s\n",
      "290:\tlearn: 0.0724306\ttotal: 5.55s\tremaining: 2.08s\n",
      "291:\tlearn: 0.0720317\ttotal: 5.57s\tremaining: 2.06s\n",
      "292:\tlearn: 0.0716278\ttotal: 5.58s\tremaining: 2.04s\n",
      "293:\tlearn: 0.0712362\ttotal: 5.6s\tremaining: 2.02s\n",
      "294:\tlearn: 0.0707410\ttotal: 5.62s\tremaining: 2s\n",
      "295:\tlearn: 0.0704118\ttotal: 5.64s\tremaining: 1.98s\n",
      "296:\tlearn: 0.0700204\ttotal: 5.66s\tremaining: 1.96s\n",
      "297:\tlearn: 0.0696531\ttotal: 5.68s\tremaining: 1.94s\n",
      "298:\tlearn: 0.0692932\ttotal: 5.7s\tremaining: 1.92s\n",
      "299:\tlearn: 0.0689641\ttotal: 5.72s\tremaining: 1.91s\n",
      "300:\tlearn: 0.0687036\ttotal: 5.74s\tremaining: 1.89s\n",
      "301:\tlearn: 0.0682465\ttotal: 5.75s\tremaining: 1.87s\n",
      "302:\tlearn: 0.0679109\ttotal: 5.77s\tremaining: 1.85s\n",
      "303:\tlearn: 0.0675415\ttotal: 5.79s\tremaining: 1.83s\n",
      "304:\tlearn: 0.0671175\ttotal: 5.81s\tremaining: 1.81s\n",
      "305:\tlearn: 0.0668401\ttotal: 5.83s\tremaining: 1.79s\n",
      "306:\tlearn: 0.0665337\ttotal: 5.84s\tremaining: 1.77s\n",
      "307:\tlearn: 0.0662038\ttotal: 5.86s\tremaining: 1.75s\n",
      "308:\tlearn: 0.0659299\ttotal: 5.88s\tremaining: 1.73s\n",
      "309:\tlearn: 0.0656950\ttotal: 5.9s\tremaining: 1.71s\n",
      "310:\tlearn: 0.0653329\ttotal: 5.92s\tremaining: 1.69s\n",
      "311:\tlearn: 0.0650142\ttotal: 5.93s\tremaining: 1.67s\n",
      "312:\tlearn: 0.0647410\ttotal: 5.95s\tremaining: 1.65s\n",
      "313:\tlearn: 0.0644334\ttotal: 5.97s\tremaining: 1.63s\n",
      "314:\tlearn: 0.0641182\ttotal: 5.99s\tremaining: 1.61s\n",
      "315:\tlearn: 0.0638325\ttotal: 6s\tremaining: 1.6s\n",
      "316:\tlearn: 0.0635122\ttotal: 6.02s\tremaining: 1.58s\n",
      "317:\tlearn: 0.0631894\ttotal: 6.04s\tremaining: 1.56s\n",
      "318:\tlearn: 0.0628921\ttotal: 6.06s\tremaining: 1.54s\n",
      "319:\tlearn: 0.0625070\ttotal: 6.08s\tremaining: 1.52s\n",
      "320:\tlearn: 0.0621223\ttotal: 6.1s\tremaining: 1.5s\n",
      "321:\tlearn: 0.0619032\ttotal: 6.11s\tremaining: 1.48s\n",
      "322:\tlearn: 0.0615653\ttotal: 6.13s\tremaining: 1.46s\n",
      "323:\tlearn: 0.0613404\ttotal: 6.15s\tremaining: 1.44s\n",
      "324:\tlearn: 0.0611619\ttotal: 6.17s\tremaining: 1.42s\n",
      "325:\tlearn: 0.0608453\ttotal: 6.19s\tremaining: 1.4s\n",
      "326:\tlearn: 0.0605806\ttotal: 6.2s\tremaining: 1.39s\n",
      "327:\tlearn: 0.0603465\ttotal: 6.22s\tremaining: 1.36s\n",
      "328:\tlearn: 0.0600413\ttotal: 6.24s\tremaining: 1.35s\n",
      "329:\tlearn: 0.0597975\ttotal: 6.26s\tremaining: 1.33s\n",
      "330:\tlearn: 0.0595617\ttotal: 6.28s\tremaining: 1.31s\n",
      "331:\tlearn: 0.0593571\ttotal: 6.29s\tremaining: 1.29s\n",
      "332:\tlearn: 0.0591896\ttotal: 6.31s\tremaining: 1.27s\n",
      "333:\tlearn: 0.0589029\ttotal: 6.33s\tremaining: 1.25s\n",
      "334:\tlearn: 0.0586641\ttotal: 6.35s\tremaining: 1.23s\n",
      "335:\tlearn: 0.0584050\ttotal: 6.37s\tremaining: 1.21s\n",
      "336:\tlearn: 0.0581356\ttotal: 6.38s\tremaining: 1.19s\n",
      "337:\tlearn: 0.0579161\ttotal: 6.4s\tremaining: 1.17s\n",
      "338:\tlearn: 0.0575914\ttotal: 6.42s\tremaining: 1.16s\n",
      "339:\tlearn: 0.0574563\ttotal: 6.44s\tremaining: 1.14s\n",
      "340:\tlearn: 0.0571344\ttotal: 6.46s\tremaining: 1.12s\n",
      "341:\tlearn: 0.0568670\ttotal: 6.47s\tremaining: 1.1s\n",
      "342:\tlearn: 0.0566196\ttotal: 6.49s\tremaining: 1.08s\n",
      "343:\tlearn: 0.0563139\ttotal: 6.51s\tremaining: 1.06s\n",
      "344:\tlearn: 0.0560689\ttotal: 6.53s\tremaining: 1.04s\n",
      "345:\tlearn: 0.0557970\ttotal: 6.55s\tremaining: 1.02s\n",
      "346:\tlearn: 0.0555744\ttotal: 6.57s\tremaining: 1s\n",
      "347:\tlearn: 0.0553663\ttotal: 6.58s\tremaining: 984ms\n",
      "348:\tlearn: 0.0550987\ttotal: 6.61s\tremaining: 965ms\n",
      "349:\tlearn: 0.0548580\ttotal: 6.63s\tremaining: 947ms\n",
      "350:\tlearn: 0.0546271\ttotal: 6.64s\tremaining: 928ms\n",
      "351:\tlearn: 0.0544627\ttotal: 6.66s\tremaining: 909ms\n",
      "352:\tlearn: 0.0542735\ttotal: 6.68s\tremaining: 890ms\n",
      "353:\tlearn: 0.0540566\ttotal: 6.7s\tremaining: 871ms\n",
      "354:\tlearn: 0.0537670\ttotal: 6.72s\tremaining: 852ms\n",
      "355:\tlearn: 0.0534931\ttotal: 6.74s\tremaining: 833ms\n",
      "356:\tlearn: 0.0532458\ttotal: 6.76s\tremaining: 814ms\n",
      "357:\tlearn: 0.0530554\ttotal: 6.78s\tremaining: 795ms\n",
      "358:\tlearn: 0.0528663\ttotal: 6.8s\tremaining: 777ms\n",
      "359:\tlearn: 0.0525924\ttotal: 6.82s\tremaining: 757ms\n",
      "360:\tlearn: 0.0523859\ttotal: 6.83s\tremaining: 738ms\n",
      "361:\tlearn: 0.0521683\ttotal: 6.85s\tremaining: 719ms\n",
      "362:\tlearn: 0.0519234\ttotal: 6.87s\tremaining: 700ms\n",
      "363:\tlearn: 0.0516844\ttotal: 6.89s\tremaining: 681ms\n",
      "364:\tlearn: 0.0515064\ttotal: 6.9s\tremaining: 662ms\n",
      "365:\tlearn: 0.0512855\ttotal: 6.92s\tremaining: 643ms\n",
      "366:\tlearn: 0.0511138\ttotal: 6.94s\tremaining: 624ms\n",
      "367:\tlearn: 0.0508781\ttotal: 6.96s\tremaining: 605ms\n",
      "368:\tlearn: 0.0506872\ttotal: 6.97s\tremaining: 586ms\n",
      "369:\tlearn: 0.0504707\ttotal: 6.99s\tremaining: 567ms\n",
      "370:\tlearn: 0.0502115\ttotal: 7.01s\tremaining: 548ms\n",
      "371:\tlearn: 0.0500338\ttotal: 7.03s\tremaining: 529ms\n",
      "372:\tlearn: 0.0498045\ttotal: 7.04s\tremaining: 510ms\n",
      "373:\tlearn: 0.0495770\ttotal: 7.07s\tremaining: 491ms\n",
      "374:\tlearn: 0.0494049\ttotal: 7.08s\tremaining: 472ms\n",
      "375:\tlearn: 0.0491638\ttotal: 7.11s\tremaining: 454ms\n",
      "376:\tlearn: 0.0489267\ttotal: 7.12s\tremaining: 435ms\n",
      "377:\tlearn: 0.0487657\ttotal: 7.14s\tremaining: 416ms\n",
      "378:\tlearn: 0.0485915\ttotal: 7.16s\tremaining: 397ms\n",
      "379:\tlearn: 0.0484395\ttotal: 7.18s\tremaining: 378ms\n",
      "380:\tlearn: 0.0482877\ttotal: 7.2s\tremaining: 359ms\n",
      "381:\tlearn: 0.0480858\ttotal: 7.21s\tremaining: 340ms\n",
      "382:\tlearn: 0.0478687\ttotal: 7.24s\tremaining: 321ms\n",
      "383:\tlearn: 0.0476853\ttotal: 7.25s\tremaining: 302ms\n",
      "384:\tlearn: 0.0474993\ttotal: 7.27s\tremaining: 283ms\n",
      "385:\tlearn: 0.0472943\ttotal: 7.29s\tremaining: 264ms\n",
      "386:\tlearn: 0.0470829\ttotal: 7.31s\tremaining: 246ms\n",
      "387:\tlearn: 0.0468742\ttotal: 7.33s\tremaining: 227ms\n",
      "388:\tlearn: 0.0466857\ttotal: 7.35s\tremaining: 208ms\n",
      "389:\tlearn: 0.0465460\ttotal: 7.37s\tremaining: 189ms\n",
      "390:\tlearn: 0.0463967\ttotal: 7.38s\tremaining: 170ms\n",
      "391:\tlearn: 0.0462451\ttotal: 7.4s\tremaining: 151ms\n",
      "392:\tlearn: 0.0460232\ttotal: 7.42s\tremaining: 132ms\n",
      "393:\tlearn: 0.0458569\ttotal: 7.44s\tremaining: 113ms\n",
      "394:\tlearn: 0.0455861\ttotal: 7.46s\tremaining: 94.4ms\n",
      "395:\tlearn: 0.0454279\ttotal: 7.47s\tremaining: 75.5ms\n",
      "396:\tlearn: 0.0452607\ttotal: 7.49s\tremaining: 56.6ms\n",
      "397:\tlearn: 0.0450495\ttotal: 7.51s\tremaining: 37.7ms\n",
      "398:\tlearn: 0.0449415\ttotal: 7.53s\tremaining: 18.9ms\n",
      "399:\tlearn: 0.0447709\ttotal: 7.55s\tremaining: 0us\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.61      0.63        44\n",
      "           1       0.56      0.60      0.58        30\n",
      "           2       0.64      0.52      0.57        31\n",
      "           3       0.65      0.63      0.64        27\n",
      "           4       0.57      0.74      0.65        31\n",
      "           5       0.64      0.61      0.62        49\n",
      "\n",
      "    accuracy                           0.62       212\n",
      "   macro avg       0.62      0.62      0.62       212\n",
      "weighted avg       0.62      0.62      0.62       212\n"
     ]
    }
   ],
   "source": [
    "import catboost as cb\n",
    "\n",
    "model = cb.CatBoostClassifier(iterations=500, task_type='GPU')\n",
    "model.fit(X_train, y_train)\n",
    "# Generate classification report\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T00:01:44.601009500Z",
     "start_time": "2024-02-22T00:01:36.225891400Z"
    }
   },
   "id": "c14f291177ba9793",
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "source": [
    "Very interesting to note is that the Catboost model gave the lowest accuracy of 0.62,\n",
    "which is lower than the other models we have tried so far, \n",
    "meaning that the more complex models do not always outperform the simpler ones."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66b66ab411e286e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network\n",
    "\n",
    "While neural networks need a lot of data to perform well, let's try a simple neural network model to see how it performs.\n",
    "\n",
    "We will use the `keras` library to build the neural network model.\n",
    "\n",
    "Let's start by making the train and test splits from above into TensorFlow datasets.\n",
    "\n",
    "We will also define a validation dataset from the training data to monitor the model's performance during training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be15e3c101722f61"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\panagiotis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 600  # Adjust based on your dataset size\n",
    "VALIDATION_SPLIT = 0.2  # Fraction of training data to use for validation\n",
    "\n",
    "# Calculate the number of validation samples\n",
    "n_validation_samples = int(len(X_train) * VALIDATION_SPLIT)\n",
    "\n",
    "# Create and process the validation dataset\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_train[:n_validation_samples], y_train[:n_validation_samples]))\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Update the training dataset to exclude the validation data and process it\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train[n_validation_samples:], y_train[n_validation_samples:]))\n",
    "train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Create and process the test dataset\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:32:11.696324900Z",
     "start_time": "2024-02-23T12:32:06.888818200Z"
    }
   },
   "id": "fac050077fcb120",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that done, let's have a look at the first batch of the training dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1bc0a932557566"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [[ 0.02937317  0.0199585  -0.02301025 ...  0.00671005 -0.05300903\n",
      "  -0.0269928 ]\n",
      " [-0.02384949  0.0174408  -0.02377319 ...  0.02064514 -0.04141235\n",
      "  -0.02015686]\n",
      " [ 0.03381348  0.01399231 -0.03097534 ... -0.01398468 -0.03561401\n",
      "  -0.03076172]\n",
      " ...\n",
      " [-0.0461731   0.00856018 -0.01756287 ...  0.00897217 -0.03820801\n",
      "   0.00946045]\n",
      " [-0.00187016  0.01035309 -0.02903748 ...  0.01985168 -0.0581665\n",
      "  -0.02468872]\n",
      " [ 0.00093603 -0.0096817  -0.02001953 ...  0.02749634 -0.06008911\n",
      "  -0.02041626]], Target: [4 4 4 2 3 0 0 3 3 2 3 1 2 3 5 4 4 5 2 5 5 0 0 5 5 2 1 3 3 0 4 4]\n"
     ]
    }
   ],
   "source": [
    "for feat, targ in train_ds.take(1):\n",
    "    print ('Features: {}, Target: {}'.format(feat, targ))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:34:01.419194400Z",
     "start_time": "2024-02-23T12:34:01.406164200Z"
    }
   },
   "id": "28b6cd6e52c06ff9",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the training dataset is structured as expected, with the features and the targets batched together with a size of 32."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2200f63d62e00072"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's define an early stopping callback to stop training the model when the validation loss does not improve after a certain number of epochs. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55faaf898903bf36"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    patience=30,  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:32:11.746700300Z",
     "start_time": "2024-02-23T12:32:11.696324900Z"
    }
   },
   "id": "a2a85456f6f0206a",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another useful callback method in TensorFlow is the ModelCheckpoint callback. This callback saves the model at regular intervals during training, allowing you to retain and load the best version of your model based on a certain monitored metric, such as validation loss or accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58e3c995a8f8d943"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model_{epoch:02d}-{val_loss:.2f}.keras',  # Change the extension to '.keras'\n",
    "    save_weights_only=False,  # Set to True to save only weights, False to save the entire model\n",
    "    monitor='val_loss',       # Monitor the validation loss\n",
    "    mode='min',               # The model is saved when the monitored metric stops decreasing\n",
    "    save_best_only=True,      # Only the best model is saved\n",
    "    verbose=1                 # Verbosity mode, 1 or 0\n",
    ")\n",
    "\n",
    "CALLBACKS = [\n",
    "    tfdocs.modeling.EpochDots(),\n",
    "    early_stopping_callback,\n",
    "    model_checkpoint_callback\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:32:11.964230800Z",
     "start_time": "2024-02-23T12:32:11.745700400Z"
    }
   },
   "id": "e99e3b0058d27c4b",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly, let's define a scheduler callback to adjust the learning rate during training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e70949e4096c0a1f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "STEPS_PER_EPOCH = X_train.shape[0] // BATCH_SIZE\n",
    "lr_scheduler = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    initial_learning_rate=1e-3,  # Initial learning rate\n",
    "    decay_steps=STEPS_PER_EPOCH * 1000,  # Decay steps\n",
    "    decay_rate=0.9,  # Decay rate\n",
    "    staircase=True  # If True, decay the learning rate at discrete intervals\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:32:11.972420700Z",
     "start_time": "2024-02-23T12:32:11.965906200Z"
    }
   },
   "id": "b24cce175aac2630",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "With all the callbacks and the learning rate scheduler defined, let's now build the neural network model.\n",
    "\n",
    "We will use a simple neural network with two hidden layers, each with 64 units, and a ReLU activation function.\n",
    "\n",
    "Also, we will use the Adam optimizer and the SparseCategoricalCrossentropy loss function, as well as the accuracy metric.\n",
    "\n",
    "Lastly, we will add WeightRegularization to the model as well as DropoutLayers to prevent overfitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "396c45887ea4ddcd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import keras \n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "from keras import regularizers\n",
    "\n",
    "weight_regularizer = regularizers.l2(0.0001)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],) , kernel_regularizer=weight_regularizer),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(32, activation='relu' , kernel_regularizer=weight_regularizer),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation='relu' , kernel_regularizer=weight_regularizer),\n",
    "    layers.Dense(6, activation='softmax')\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:33:04.041963900Z",
     "start_time": "2024-02-23T12:33:04.012587500Z"
    }
   },
   "id": "ed010b4c406e22d7",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the model created, let's compile it using the Adam optimizer, the SparseCategoricalCrossentropy loss function, and the accuracy metric."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9600efc4cd8cdafb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=(['accuracy',\n",
    "                        tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                            from_logits=False, name='sparse_categorical_crossentropy')]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:33:04.299956300Z",
     "start_time": "2024-02-23T12:33:04.290932800Z"
    }
   },
   "id": "3dc26bb0fade8741",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see now the model summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d19eb4b55cb1da4d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 64)                49216     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 6)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 51926 (202.84 KB)\n",
      "Trainable params: 51926 (202.84 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:33:04.613226900Z",
     "start_time": "2024-02-23T12:33:04.586624Z"
    }
   },
   "id": "7794956153aea89d",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the model has 51926 parameters to train, which are quite a lot for the small dataset we have.\n",
    "However, with the Dropout layers and the WeightRegularization, we hope to prevent excessive overfitting.\n",
    "\n",
    "Let's now train the model using the training dataset and the validation dataset we created earlier."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "689f9ac49e7e4324"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1/22 [>.............................] - ETA: 9s - loss: 1.8088 - accuracy: 0.0938 - sparse_categorical_crossentropy: 1.7907\n",
      "Epoch: 0, accuracy:0.1479,  loss:1.8079,  sparse_categorical_crossentropy:1.7929,  val_accuracy:0.1361,  val_loss:1.8075,  val_sparse_categorical_crossentropy:1.7901,  \n",
      ".\n",
      "Epoch 1: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 1s 5ms/step - loss: 1.8079 - accuracy: 0.1479 - sparse_categorical_crossentropy: 1.7929 - val_loss: 1.8075 - val_accuracy: 0.1361 - val_sparse_categorical_crossentropy: 1.7901\n",
      "Epoch 2/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.8057 - accuracy: 0.1250 - sparse_categorical_crossentropy: 1.7887.\n",
      "Epoch 2: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.8013 - accuracy: 0.2071 - sparse_categorical_crossentropy: 1.7831 - val_loss: 1.8008 - val_accuracy: 0.1716 - val_sparse_categorical_crossentropy: 1.7805\n",
      "Epoch 3/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7969 - accuracy: 0.1875 - sparse_categorical_crossentropy: 1.7807.\n",
      "Epoch 3: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7864 - accuracy: 0.2175 - sparse_categorical_crossentropy: 1.7699 - val_loss: 1.7951 - val_accuracy: 0.1716 - val_sparse_categorical_crossentropy: 1.7715\n",
      "Epoch 4/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.8027 - accuracy: 0.1875 - sparse_categorical_crossentropy: 1.7873.\n",
      "Epoch 4: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7781 - accuracy: 0.2234 - sparse_categorical_crossentropy: 1.7600 - val_loss: 1.7880 - val_accuracy: 0.1716 - val_sparse_categorical_crossentropy: 1.7626\n",
      "Epoch 5/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7906 - accuracy: 0.1875 - sparse_categorical_crossentropy: 1.7758.\n",
      "Epoch 5: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7649 - accuracy: 0.2337 - sparse_categorical_crossentropy: 1.7499 - val_loss: 1.7845 - val_accuracy: 0.1716 - val_sparse_categorical_crossentropy: 1.7566\n",
      "Epoch 6/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7771 - accuracy: 0.2812 - sparse_categorical_crossentropy: 1.7627.\n",
      "Epoch 6: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7443 - accuracy: 0.2396 - sparse_categorical_crossentropy: 1.7296 - val_loss: 1.7682 - val_accuracy: 0.1716 - val_sparse_categorical_crossentropy: 1.7381\n",
      "Epoch 7/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7252 - accuracy: 0.2812 - sparse_categorical_crossentropy: 1.7112.\n",
      "Epoch 7: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7262 - accuracy: 0.2530 - sparse_categorical_crossentropy: 1.7023 - val_loss: 1.7572 - val_accuracy: 0.1775 - val_sparse_categorical_crossentropy: 1.7222\n",
      "Epoch 8/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.7729 - accuracy: 0.1250 - sparse_categorical_crossentropy: 1.7590.\n",
      "Epoch 8: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7191 - accuracy: 0.2722 - sparse_categorical_crossentropy: 1.7059 - val_loss: 1.7391 - val_accuracy: 0.2485 - val_sparse_categorical_crossentropy: 1.7043\n",
      "Epoch 9/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.6810 - accuracy: 0.3438 - sparse_categorical_crossentropy: 1.6673.\n",
      "Epoch 9: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6787 - accuracy: 0.3033 - sparse_categorical_crossentropy: 1.6665 - val_loss: 1.7175 - val_accuracy: 0.2367 - val_sparse_categorical_crossentropy: 1.6713\n",
      "Epoch 10/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.5951 - accuracy: 0.4375 - sparse_categorical_crossentropy: 1.5814.\n",
      "Epoch 10: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6548 - accuracy: 0.3195 - sparse_categorical_crossentropy: 1.6467 - val_loss: 1.6902 - val_accuracy: 0.2840 - val_sparse_categorical_crossentropy: 1.6432\n",
      "Epoch 11/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.6546 - accuracy: 0.3125 - sparse_categorical_crossentropy: 1.6408.\n",
      "Epoch 11: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6135 - accuracy: 0.3580 - sparse_categorical_crossentropy: 1.6037 - val_loss: 1.6604 - val_accuracy: 0.3432 - val_sparse_categorical_crossentropy: 1.6108\n",
      "Epoch 12/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.6360 - accuracy: 0.4062 - sparse_categorical_crossentropy: 1.6220.\n",
      "Epoch 12: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6010 - accuracy: 0.3905 - sparse_categorical_crossentropy: 1.5860 - val_loss: 1.6220 - val_accuracy: 0.3550 - val_sparse_categorical_crossentropy: 1.5664\n",
      "Epoch 13/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.5485 - accuracy: 0.4062 - sparse_categorical_crossentropy: 1.5343.\n",
      "Epoch 13: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5537 - accuracy: 0.4290 - sparse_categorical_crossentropy: 1.5403 - val_loss: 1.5985 - val_accuracy: 0.3964 - val_sparse_categorical_crossentropy: 1.5463\n",
      "Epoch 14/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.6900 - accuracy: 0.2812 - sparse_categorical_crossentropy: 1.6755.\n",
      "Epoch 14: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5303 - accuracy: 0.4142 - sparse_categorical_crossentropy: 1.5169 - val_loss: 1.5643 - val_accuracy: 0.3964 - val_sparse_categorical_crossentropy: 1.5020\n",
      "Epoch 15/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.5167 - accuracy: 0.4375 - sparse_categorical_crossentropy: 1.5019.\n",
      "Epoch 15: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5027 - accuracy: 0.4379 - sparse_categorical_crossentropy: 1.4724 - val_loss: 1.5386 - val_accuracy: 0.4083 - val_sparse_categorical_crossentropy: 1.4762\n",
      "Epoch 16/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1826 - accuracy: 0.7500 - sparse_categorical_crossentropy: 1.1675.\n",
      "Epoch 16: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4222 - accuracy: 0.4615 - sparse_categorical_crossentropy: 1.3931 - val_loss: 1.5018 - val_accuracy: 0.4320 - val_sparse_categorical_crossentropy: 1.4354\n",
      "Epoch 17/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.5708 - accuracy: 0.3438 - sparse_categorical_crossentropy: 1.5553.\n",
      "Epoch 17: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4197 - accuracy: 0.4793 - sparse_categorical_crossentropy: 1.3995 - val_loss: 1.4818 - val_accuracy: 0.4260 - val_sparse_categorical_crossentropy: 1.4036\n",
      "Epoch 18/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2538 - accuracy: 0.5938 - sparse_categorical_crossentropy: 1.2380.\n",
      "Epoch 18: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4059 - accuracy: 0.4645 - sparse_categorical_crossentropy: 1.4142 - val_loss: 1.4903 - val_accuracy: 0.4142 - val_sparse_categorical_crossentropy: 1.4316\n",
      "Epoch 19/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2137 - accuracy: 0.5625 - sparse_categorical_crossentropy: 1.1976.\n",
      "Epoch 19: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3776 - accuracy: 0.4985 - sparse_categorical_crossentropy: 1.3572 - val_loss: 1.4383 - val_accuracy: 0.4970 - val_sparse_categorical_crossentropy: 1.3716\n",
      "Epoch 20/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1559 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.1394.\n",
      "Epoch 20: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3078 - accuracy: 0.5370 - sparse_categorical_crossentropy: 1.3096 - val_loss: 1.4132 - val_accuracy: 0.5030 - val_sparse_categorical_crossentropy: 1.3441\n",
      "Epoch 21/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.3439 - accuracy: 0.4375 - sparse_categorical_crossentropy: 1.3271.\n",
      "Epoch 21: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3157 - accuracy: 0.5385 - sparse_categorical_crossentropy: 1.3027 - val_loss: 1.4341 - val_accuracy: 0.4852 - val_sparse_categorical_crossentropy: 1.3532\n",
      "Epoch 22/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0968 - accuracy: 0.6875 - sparse_categorical_crossentropy: 1.0796.\n",
      "Epoch 22: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2752 - accuracy: 0.5503 - sparse_categorical_crossentropy: 1.2477 - val_loss: 1.4052 - val_accuracy: 0.5207 - val_sparse_categorical_crossentropy: 1.3416\n",
      "Epoch 23/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.4043 - accuracy: 0.4688 - sparse_categorical_crossentropy: 1.3867.\n",
      "Epoch 23: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2672 - accuracy: 0.5414 - sparse_categorical_crossentropy: 1.2449 - val_loss: 1.3517 - val_accuracy: 0.5207 - val_sparse_categorical_crossentropy: 1.2823\n",
      "Epoch 24/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1353 - accuracy: 0.6875 - sparse_categorical_crossentropy: 1.1173.\n",
      "Epoch 24: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2140 - accuracy: 0.5577 - sparse_categorical_crossentropy: 1.1595 - val_loss: 1.3651 - val_accuracy: 0.5266 - val_sparse_categorical_crossentropy: 1.2904\n",
      "Epoch 25/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1878 - accuracy: 0.5000 - sparse_categorical_crossentropy: 1.1695.\n",
      "Epoch 25: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.2103 - accuracy: 0.5769 - sparse_categorical_crossentropy: 1.1890 - val_loss: 1.3823 - val_accuracy: 0.5089 - val_sparse_categorical_crossentropy: 1.3168\n",
      "Epoch 26/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1642 - accuracy: 0.5625 - sparse_categorical_crossentropy: 1.1454.\n",
      "Epoch 26: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1623 - accuracy: 0.5799 - sparse_categorical_crossentropy: 1.1384 - val_loss: 1.3634 - val_accuracy: 0.4911 - val_sparse_categorical_crossentropy: 1.2929\n",
      "Epoch 27/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.5688 - accuracy: 0.4375 - sparse_categorical_crossentropy: 1.5497.\n",
      "Epoch 27: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1379 - accuracy: 0.6065 - sparse_categorical_crossentropy: 1.0972 - val_loss: 1.3278 - val_accuracy: 0.4911 - val_sparse_categorical_crossentropy: 1.2507\n",
      "Epoch 28/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.3009 - accuracy: 0.4375 - sparse_categorical_crossentropy: 1.2814.\n",
      "Epoch 28: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1353 - accuracy: 0.6065 - sparse_categorical_crossentropy: 1.1166 - val_loss: 1.2983 - val_accuracy: 0.5503 - val_sparse_categorical_crossentropy: 1.2227\n",
      "Epoch 29/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2410 - accuracy: 0.5938 - sparse_categorical_crossentropy: 1.2211.\n",
      "Epoch 29: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1122 - accuracy: 0.6050 - sparse_categorical_crossentropy: 1.1063 - val_loss: 1.2745 - val_accuracy: 0.5680 - val_sparse_categorical_crossentropy: 1.1954\n",
      "Epoch 30/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.2277 - accuracy: 0.5625 - sparse_categorical_crossentropy: 1.2075.\n",
      "Epoch 30: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0803 - accuracy: 0.6243 - sparse_categorical_crossentropy: 1.0498 - val_loss: 1.3027 - val_accuracy: 0.5325 - val_sparse_categorical_crossentropy: 1.2286\n",
      "Epoch 31/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1714 - accuracy: 0.5938 - sparse_categorical_crossentropy: 1.1509.\n",
      "Epoch 31: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0474 - accuracy: 0.6346 - sparse_categorical_crossentropy: 1.0276 - val_loss: 1.2510 - val_accuracy: 0.5503 - val_sparse_categorical_crossentropy: 1.1727\n",
      "Epoch 32/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9367 - accuracy: 0.6562 - sparse_categorical_crossentropy: 0.9158.\n",
      "Epoch 32: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0226 - accuracy: 0.6317 - sparse_categorical_crossentropy: 1.0057 - val_loss: 1.2350 - val_accuracy: 0.5858 - val_sparse_categorical_crossentropy: 1.1612\n",
      "Epoch 33/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0420 - accuracy: 0.6250 - sparse_categorical_crossentropy: 1.0208.\n",
      "Epoch 33: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0314 - accuracy: 0.6420 - sparse_categorical_crossentropy: 0.9830 - val_loss: 1.3212 - val_accuracy: 0.5207 - val_sparse_categorical_crossentropy: 1.2432\n",
      "Epoch 34/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8409 - accuracy: 0.5625 - sparse_categorical_crossentropy: 0.8193.\n",
      "Epoch 34: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9945 - accuracy: 0.6568 - sparse_categorical_crossentropy: 0.9686 - val_loss: 1.3539 - val_accuracy: 0.5325 - val_sparse_categorical_crossentropy: 1.2766\n",
      "Epoch 35/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8874 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.8654.\n",
      "Epoch 35: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9757 - accuracy: 0.6672 - sparse_categorical_crossentropy: 0.9878 - val_loss: 1.2559 - val_accuracy: 0.5799 - val_sparse_categorical_crossentropy: 1.1685\n",
      "Epoch 36/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.0216 - accuracy: 0.5938 - sparse_categorical_crossentropy: 0.9993.\n",
      "Epoch 36: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9823 - accuracy: 0.6746 - sparse_categorical_crossentropy: 0.9717 - val_loss: 1.2450 - val_accuracy: 0.5740 - val_sparse_categorical_crossentropy: 1.1622\n",
      "Epoch 37/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 1.1489 - accuracy: 0.6562 - sparse_categorical_crossentropy: 1.1262.\n",
      "Epoch 37: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9696 - accuracy: 0.6672 - sparse_categorical_crossentropy: 0.9545 - val_loss: 1.3511 - val_accuracy: 0.5207 - val_sparse_categorical_crossentropy: 1.2581\n",
      "Epoch 38/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9050 - accuracy: 0.6875 - sparse_categorical_crossentropy: 0.8820.\n",
      "Epoch 38: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9414 - accuracy: 0.6746 - sparse_categorical_crossentropy: 0.9218 - val_loss: 1.2929 - val_accuracy: 0.5562 - val_sparse_categorical_crossentropy: 1.2018\n",
      "Epoch 39/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8870 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.8637.\n",
      "Epoch 39: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9194 - accuracy: 0.6923 - sparse_categorical_crossentropy: 0.8967 - val_loss: 1.3041 - val_accuracy: 0.5503 - val_sparse_categorical_crossentropy: 1.2170\n",
      "Epoch 40/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7642 - accuracy: 0.7188 - sparse_categorical_crossentropy: 0.7405.\n",
      "Epoch 40: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9035 - accuracy: 0.6849 - sparse_categorical_crossentropy: 0.8711 - val_loss: 1.2763 - val_accuracy: 0.5799 - val_sparse_categorical_crossentropy: 1.1863\n",
      "Epoch 41/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9495 - accuracy: 0.6562 - sparse_categorical_crossentropy: 0.9256.\n",
      "Epoch 41: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8786 - accuracy: 0.6997 - sparse_categorical_crossentropy: 0.8685 - val_loss: 1.3235 - val_accuracy: 0.5444 - val_sparse_categorical_crossentropy: 1.2285\n",
      "Epoch 42/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7072 - accuracy: 0.7812 - sparse_categorical_crossentropy: 0.6830.\n",
      "Epoch 42: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8767 - accuracy: 0.7130 - sparse_categorical_crossentropy: 0.8389 - val_loss: 1.3463 - val_accuracy: 0.5444 - val_sparse_categorical_crossentropy: 1.2445\n",
      "Epoch 43/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7148 - accuracy: 0.7812 - sparse_categorical_crossentropy: 0.6903.\n",
      "Epoch 43: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8752 - accuracy: 0.7160 - sparse_categorical_crossentropy: 0.8708 - val_loss: 1.3173 - val_accuracy: 0.5621 - val_sparse_categorical_crossentropy: 1.2187\n",
      "Epoch 44/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9045 - accuracy: 0.6562 - sparse_categorical_crossentropy: 0.8796.\n",
      "Epoch 44: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9087 - accuracy: 0.7056 - sparse_categorical_crossentropy: 0.8799 - val_loss: 1.3551 - val_accuracy: 0.5444 - val_sparse_categorical_crossentropy: 1.2534\n",
      "Epoch 45/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.9764 - accuracy: 0.6875 - sparse_categorical_crossentropy: 0.9512.\n",
      "Epoch 45: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8320 - accuracy: 0.7367 - sparse_categorical_crossentropy: 0.8052 - val_loss: 1.3414 - val_accuracy: 0.5503 - val_sparse_categorical_crossentropy: 1.2422\n",
      "Epoch 46/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8523 - accuracy: 0.6875 - sparse_categorical_crossentropy: 0.8268.\n",
      "Epoch 46: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8291 - accuracy: 0.7175 - sparse_categorical_crossentropy: 0.7983 - val_loss: 1.2983 - val_accuracy: 0.5680 - val_sparse_categorical_crossentropy: 1.1923\n",
      "Epoch 47/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6241 - accuracy: 0.9062 - sparse_categorical_crossentropy: 0.5983.\n",
      "Epoch 47: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7935 - accuracy: 0.7367 - sparse_categorical_crossentropy: 0.7655 - val_loss: 1.3366 - val_accuracy: 0.5325 - val_sparse_categorical_crossentropy: 1.2334\n",
      "Epoch 48/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8479 - accuracy: 0.5625 - sparse_categorical_crossentropy: 0.8218.\n",
      "Epoch 48: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8075 - accuracy: 0.7130 - sparse_categorical_crossentropy: 0.7898 - val_loss: 1.3373 - val_accuracy: 0.5444 - val_sparse_categorical_crossentropy: 1.2349\n",
      "Epoch 49/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8568 - accuracy: 0.5625 - sparse_categorical_crossentropy: 0.8304.\n",
      "Epoch 49: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7845 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.7535 - val_loss: 1.3645 - val_accuracy: 0.5385 - val_sparse_categorical_crossentropy: 1.2543\n",
      "Epoch 50/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5294 - accuracy: 0.8125 - sparse_categorical_crossentropy: 0.5027.\n",
      "Epoch 50: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7418 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.7114 - val_loss: 1.3180 - val_accuracy: 0.5799 - val_sparse_categorical_crossentropy: 1.1996\n",
      "Epoch 51/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7676 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.7406.\n",
      "Epoch 51: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7379 - accuracy: 0.7426 - sparse_categorical_crossentropy: 0.7154 - val_loss: 1.4047 - val_accuracy: 0.5385 - val_sparse_categorical_crossentropy: 1.2871\n",
      "Epoch 52/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7218 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.6945.\n",
      "Epoch 52: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7329 - accuracy: 0.7367 - sparse_categorical_crossentropy: 0.6971 - val_loss: 1.4248 - val_accuracy: 0.5444 - val_sparse_categorical_crossentropy: 1.3015\n",
      "Epoch 53/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7356 - accuracy: 0.8750 - sparse_categorical_crossentropy: 0.7080.\n",
      "Epoch 53: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7053 - accuracy: 0.7633 - sparse_categorical_crossentropy: 0.6655 - val_loss: 1.3760 - val_accuracy: 0.5503 - val_sparse_categorical_crossentropy: 1.2577\n",
      "Epoch 54/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7251 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.6972.\n",
      "Epoch 54: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7650 - accuracy: 0.7500 - sparse_categorical_crossentropy: 0.7315 - val_loss: 1.3791 - val_accuracy: 0.5562 - val_sparse_categorical_crossentropy: 1.2652\n",
      "Epoch 55/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5962 - accuracy: 0.8125 - sparse_categorical_crossentropy: 0.5680.\n",
      "Epoch 55: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7034 - accuracy: 0.7544 - sparse_categorical_crossentropy: 0.6861 - val_loss: 1.4109 - val_accuracy: 0.5621 - val_sparse_categorical_crossentropy: 1.2823\n",
      "Epoch 56/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8970 - accuracy: 0.7188 - sparse_categorical_crossentropy: 0.8685.\n",
      "Epoch 56: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6813 - accuracy: 0.7707 - sparse_categorical_crossentropy: 0.6516 - val_loss: 1.4273 - val_accuracy: 0.5325 - val_sparse_categorical_crossentropy: 1.3086\n",
      "Epoch 57/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6405 - accuracy: 0.8125 - sparse_categorical_crossentropy: 0.6116.\n",
      "Epoch 57: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6994 - accuracy: 0.7648 - sparse_categorical_crossentropy: 0.6896 - val_loss: 1.4388 - val_accuracy: 0.5503 - val_sparse_categorical_crossentropy: 1.3178\n",
      "Epoch 58/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7888 - accuracy: 0.9062 - sparse_categorical_crossentropy: 0.7597.\n",
      "Epoch 58: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7117 - accuracy: 0.7766 - sparse_categorical_crossentropy: 0.7157 - val_loss: 1.3871 - val_accuracy: 0.5740 - val_sparse_categorical_crossentropy: 1.2726\n",
      "Epoch 59/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5940 - accuracy: 0.8438 - sparse_categorical_crossentropy: 0.5646.\n",
      "Epoch 59: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.7781 - sparse_categorical_crossentropy: 0.6436 - val_loss: 1.4298 - val_accuracy: 0.5562 - val_sparse_categorical_crossentropy: 1.3147\n",
      "Epoch 60/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8952 - accuracy: 0.7188 - sparse_categorical_crossentropy: 0.8655.\n",
      "Epoch 60: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6547 - accuracy: 0.7825 - sparse_categorical_crossentropy: 0.6341 - val_loss: 1.5249 - val_accuracy: 0.5621 - val_sparse_categorical_crossentropy: 1.4054\n",
      "Epoch 61/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.8837 - accuracy: 0.6875 - sparse_categorical_crossentropy: 0.8537.\n",
      "Epoch 61: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6595 - accuracy: 0.7722 - sparse_categorical_crossentropy: 0.6366 - val_loss: 1.4154 - val_accuracy: 0.5503 - val_sparse_categorical_crossentropy: 1.2992\n",
      "Epoch 62/100\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.4218 - accuracy: 0.8750 - sparse_categorical_crossentropy: 0.3916.\n",
      "Epoch 62: val_loss did not improve from 1.18581\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6308 - accuracy: 0.8107 - sparse_categorical_crossentropy: 0.5822 - val_loss: 1.4888 - val_accuracy: 0.5621 - val_sparse_categorical_crossentropy: 1.3602\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=100, batch_size=BATCH_SIZE, validation_data=val_ds, callbacks=CALLBACKS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:33:07.896811Z",
     "start_time": "2024-02-23T12:33:04.881744100Z"
    }
   },
   "id": "18398cf390d74222",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now using tfdocs.plots.HistoryPlotter, we can visualize the training and validation loss and accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc642328f72f6966"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "size_histories = {}\n",
    "size_histories['Baseline'] = history\n",
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'sparse_categorical_crossentropy', smoothing_std=10)\n",
    "plotter.plot(size_histories)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:33:07.980201500Z",
     "start_time": "2024-02-23T12:33:07.896811Z"
    }
   },
   "id": "a7a2ea0769fdc279",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the model the validation loss becomes stagnant after 30 epochs.\n",
    "\n",
    "Let's now evaluate the model on the test dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a97fe8c00c1e28e0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 833us/step - loss: 1.1549 - accuracy: 0.6132 - sparse_categorical_crossentropy: 1.1164\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1.1548932790756226, 0.6132075190544128, 1.1164045333862305]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds, verbose=1, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:33:07.999222300Z",
     "start_time": "2024-02-23T12:33:07.981200900Z"
    }
   },
   "id": "d52c2208aed74b8b",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also generate a classification report for the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb4f229959c3287d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 667us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.77      0.62        44\n",
      "           1       0.57      0.70      0.63        30\n",
      "           2       0.70      0.45      0.55        31\n",
      "           3       0.74      0.63      0.68        27\n",
      "           4       0.59      0.61      0.60        31\n",
      "           5       0.71      0.51      0.60        49\n",
      "\n",
      "    accuracy                           0.61       212\n",
      "   macro avg       0.64      0.61      0.61       212\n",
      "weighted avg       0.64      0.61      0.61       212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(test_ds)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:39:54.781016Z",
     "start_time": "2024-02-23T12:39:54.754328200Z"
    }
   },
   "id": "73a947990a3f8705",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the model has an accuracy of 0.61, which is not bad considering the small dataset we have.\n",
    "\n",
    "However, we can see that the model is overfitting the training data, as the validation loss becomes higher than the training loss quickly,\n",
    "which makes sense given the small dataset we have.\n",
    "\n",
    "Ironically, the simpler models such as logistic regression and SGD outperformed the neural network model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ab41048404b3e7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Two-Level Classification\n",
    "\n",
    "As we saw from before, we got the best results using the SGD Classifier.\n",
    "\n",
    "Let's now try to perform a two-level classification, where we first classify the class of the word and then classify the division/section of the word based on the class it belongs to."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cece51585462464"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "79b2a49fecb749be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
