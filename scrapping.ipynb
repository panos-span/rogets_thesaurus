{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Roget's Thesaurus in the 21st Century\n",
    "---\n",
    ">Spanakis Panagiotis-Alexios, Pregraduate Student\n",
    "Department of Management Science and Technology\n",
    "Athens University of Economics and Business\n",
    "t8200158@aueb.gr"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0de3c4c368d8a2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Before we start\n",
    "\n",
    "We first need to go over the dependencies needed for this notebook to function properly"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8085dad604aba2e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dependencies for this Notebook\n",
    "\n",
    "\n",
    "We will need to use libraries that are not included in the Python Standard Library which are handled using Poetry\n",
    "\n",
    "In order to install the dependencies, we will need to run the following commands in the terminal \n",
    "\n",
    "1. (If we haven't already installed Poetry)\n",
    "```bash\n",
    "pip install poetry\n",
    "```\n",
    "\n",
    "2. After we have installed Poetry, we will need to run the following command in the terminal\n",
    "```bash\n",
    "poetry install\n",
    "```\n",
    "\n",
    "3. (Optional) If we want to use the Jupyter Notebook with the virtual environment created by Poetry, we will need to run the following command in the terminal\n",
    "```bash\n",
    "poetry shell\n",
    "```\n",
    "\n",
    "4. After we have installed the dependencies, we will need to run the following command in the terminal\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d1ad5219b142504"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the following libraries:\n",
    "\n",
    "- `pandas` for data manipulation\n",
    "- `numpy` for numerical operations\n",
    "- `matplotlib` for plotting\n",
    "\n",
    "---\n",
    "- `requests` for making HTTP requests\n",
    "- `beautifulsoup4` for web scraping\n",
    "---\n",
    "- `re` for regular expressions\n",
    "- `os` for file manipulation\n",
    "- `json` for JSON manipulation\n",
    "---\n",
    "- `langchain`  to augment the power of LLMs with our data and to handle the embedding models\n",
    "- `chromadb` to store the embeddings to a vector database\n",
    "- `nomic` in order to interact with the nomic API\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3881e25aeb3eb08"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T16:09:24.454116400Z",
     "start_time": "2024-02-18T16:09:24.300939100Z"
    }
   },
   "id": "b8a4d04f81d70b0f",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T16:09:25.198760700Z",
     "start_time": "2024-02-18T16:09:24.454116400Z"
    }
   },
   "id": "ed6418dddd0621ba",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Roget's Thesaurus Classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45398213a7eaca50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's explain how the words along with their classes and divisions and sections are organized in the Gutenberg page\n",
    "\n",
    "The words are organized in the page in the following manner:\n",
    "\n",
    "1) They belong to a class\n",
    "2) They belong to a division (if it exists) within the class\n",
    "3) They belong to a section within the division (if it exists) or within the class\n",
    "\n",
    "Let's start by getting the page and parsing it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "841f2d7796f60f8b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get the page\n",
    "r = requests.get(\"https://www.gutenberg.org/files/10681/old/20040627-10681-h-body-pos.htm\")\n",
    "# Parse the page\n",
    "html = r.text\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T16:09:28.017109Z",
     "start_time": "2024-02-18T16:09:25.198760700Z"
    }
   },
   "id": "aff37d0cdd42e896",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "1) We can notice that all classes on the page are represented by a `<dt>` tag with an `<a>` tag inside it. \n",
    "The `<a>` tag has a name attribute that starts with **`CLASS`**. The text of the `<a>` tag is the name of the class.\n",
    "\n",
    "\n",
    "2) We can notice that all divisions on the page are represented by a `<dt>` tag with an `<a>` tag inside it.\n",
    "The `<a>` tag has a name attribute that starts with **`DIVISION`**. The text of the `<a>` tag is the name of the division.\n",
    "\n",
    "\n",
    "3) We can notice that all sections on the page are represented by a `<dt>` tag with an `<a>` tag inside it.\n",
    "The `<a>` tag has a name attribute that starts with **`SECTION`**. The text of the `<a>` tag is the name of the section.\n",
    "\n",
    "Lastly, the words are represented by a `<dt>` tag with an `<a>` tag inside it. The `<a>` tag has a name attribute that is a number or a number with a letter at the end.\n",
    "We notice that the words come after the `<a>` tag where the name attribute is a number and also that tag is inside a `<b>` tag.\n",
    "So in order to get the words we must get the text of the `<b>` tag that comes after the `<a>` tag where the name attribute is a number."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f33736d5050a47f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the above in mind, we can extract the hierarchy of classes, divisions, sections, and words from the page.\n",
    "We can then create a directory structure that mirrors the structure of the page and save the words under each section to a file in the directory structure \n",
    "using a dictionary to hold the entire hierarchy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea14f7c774083d2a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold the entire hierarchy\n",
    "hierarchy = {}\n",
    "current_class = None\n",
    "current_division = None\n",
    "current_section = None\n",
    "\n",
    "# Find all <dt> tags\n",
    "dt_tags = soup.find_all('dt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T16:09:28.036747400Z",
     "start_time": "2024-02-18T16:09:28.018110100Z"
    }
   },
   "id": "584ab5f5572f866e",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having the dictionary initialized and the `<dt>` tags found, we can now iterate through each `<dt>` tag and extract the hierarchy of classes, divisions, sections, and words from the page. Also, split the words if they have a comma or a dot and add them to the hierarchy separately.\n",
    "\n",
    "We will use the following logic to extract the hierarchy:\n",
    "\n",
    "1) If the `<dt>` tag has an `<a>` tag with a name attribute that starts with **`CLASS`** then we have found a class. We will add the class to the hierarchy and continue to the next `<dt>` tag to check for division/section.\n",
    "2) If the `<dt>` tag has an `<a>` tag with a name attribute that starts with **`DIVISION`** then we have found a division. We will add the division to the hierarchy and continue to the next `<dt>` tag to check for section.\n",
    "3) If the `<dt>` tag has an `<a>` tag with a name attribute that starts with **`SECTION`** then we have found a section. We will add the section to the hierarchy and continue to the next `<dt>` tag to check for words.\n",
    "4) Lastly, if the `<dt>` tag has an `<a>` tag with a name attribute that is a number or a number with a letter at the end then we have found a word. We will add the word to the hierarchy under the section it belongs to. Important to note that we will split the word regarding the comma and . and add the words to the hierarchy separately. Also, if the word has (\\u0086), a cross,  then we will remove it so that we can have a clean word."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ba3ffdff86f959c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Iterate through each <dt> tag\n",
    "for dt in dt_tags:\n",
    "    # Check for class\n",
    "    class_a_tag = dt.find('a', attrs={'name': re.compile(\"^CLASS\")})\n",
    "    if class_a_tag:\n",
    "        current_class = re.sub(r'\\s+', ' ', class_a_tag.text).strip()\n",
    "        hierarchy[current_class] = {'divisions': {}, 'sections': {}}\n",
    "        current_division = None\n",
    "        current_section = None\n",
    "        # Now that we got the class we can continue to the next <dt> tag to check for division/section\n",
    "        continue\n",
    "\n",
    "    # Check for division\n",
    "    division_a_tag = dt.find('a', attrs={'name': re.compile(\"^DIVISION\")})\n",
    "    if division_a_tag and current_class:\n",
    "        current_division = re.sub(r'\\s+', ' ', division_a_tag.text).strip()\n",
    "        hierarchy[current_class]['divisions'][current_division] = {'sections': {}}\n",
    "        current_section = None\n",
    "        # Now that we got the division we can continue to the next <dt> tag to check for section\n",
    "        continue\n",
    "\n",
    "    # Check for section\n",
    "    section_a_tag = dt.find('a', attrs={'name': re.compile(\"^SECTION\")})\n",
    "    if section_a_tag:\n",
    "        current_section = re.sub(r'\\s+', ' ', section_a_tag.text).strip()\n",
    "        if current_division:\n",
    "            hierarchy[current_class]['divisions'][current_division]['sections'][current_section] = []\n",
    "        else:\n",
    "            hierarchy[current_class]['sections'][current_section] = []\n",
    "        # Now that we got the section we can continue to the next <dt> tag to check for words\n",
    "        continue\n",
    "\n",
    "    # Check for words (the words are before an a tag with a name attribute that is a number (integer or float))\n",
    "    word_a_tags = dt.find_all('a', attrs={'name': re.compile(\"^\\d+(\\.\\d+)?$\")})\n",
    "    for word_a_tag in word_a_tags:\n",
    "        word = word_a_tag.find_next('b').get_text() if word_a_tag.find_next('b') else ''\n",
    "        word = re.sub(r'\\s+', ' ', word).strip()\n",
    "\n",
    "        # Split the word regarding the comma and . and add the words to the hierarchy\n",
    "        words = re.split(r',|\\.', word)\n",
    "        for word in words:\n",
    "            word = word.strip()\n",
    "            # If the word has (\\u0086) then remove it\n",
    "            if '†' in word:\n",
    "                word = word.replace('†', '')\n",
    "            if current_section and word:\n",
    "                if current_division:\n",
    "                    hierarchy[current_class]['divisions'][current_division]['sections'][current_section].append(word)\n",
    "                else:\n",
    "                    hierarchy[current_class]['sections'][current_section].append(word)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T16:09:28.226739Z",
     "start_time": "2024-02-18T16:09:28.038747800Z"
    }
   },
   "id": "b7aa087148964d5c",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the hierarchy of classes, divisions, sections, and words, we can take at the hierarchy we created."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f19c9ec29f59de8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'divisions': {},\n 'sections': {'EXISTENCE': ['Existence',\n   'Inexistence',\n   'Substantiality',\n   'Unsubstantiality',\n   'Intrinsicality',\n   'Extrinsicality',\n   'State',\n   'Circumstance'],\n  'RELATION': ['Relation',\n   'Irrelation',\n   'Consanguinity',\n   'Correlation',\n   'Identity',\n   'Contrariety',\n   'Difference',\n   'Uniformity',\n   'Nonuniformity',\n   'Similarity',\n   'Dissimilarity',\n   'Imitation',\n   'Nonimitation',\n   'Variation',\n   'Copy',\n   'Prototype',\n   'Agreement',\n   'Disagreement'],\n  'QUANTITY': ['Quantity',\n   'Degree',\n   'Equality',\n   'Inequality',\n   'Mean',\n   'Compensation',\n   'Greatness',\n   'Smallness',\n   'Superiority',\n   'Inferiority',\n   'Increase',\n   'Nonincrease',\n   'Decrease',\n   'Addition',\n   'Nonaddition',\n   'Subtraction',\n   'Adjunct',\n   'Remainder',\n   'Decrement',\n   'Mixture',\n   'Simpleness',\n   'Junction',\n   'Disjunction',\n   'Connection',\n   'Coherence',\n   'Incoherence',\n   'Combination',\n   'Decomposition',\n   'Whole',\n   'Part',\n   'Completeness',\n   'Incompleteness',\n   'Composition',\n   'Exclusion',\n   'Component',\n   'Extraneousness'],\n  'ORDER': ['Order',\n   'Disorder',\n   'Complexity',\n   'Arrangement',\n   'Derangement',\n   'Precedence',\n   'Sequence',\n   'Precursor',\n   'Sequel',\n   'Beginning',\n   'End',\n   'Middle',\n   'Continuity',\n   'Discontinuity',\n   'Term',\n   'Assemblage',\n   'Nonassemblage',\n   'Dispersion',\n   'Focus',\n   'Class',\n   'Inclusion',\n   'Exclusion',\n   'Generality',\n   'Speciality',\n   'Normality',\n   'Multiformity',\n   'Conformity',\n   'Unconformity'],\n  'NUMBER': ['Number',\n   'Numeration',\n   'List',\n   'Unity',\n   'Accompaniment',\n   'Duality',\n   'Duplication',\n   'bisection',\n   'Triality',\n   'Triplication',\n   'Trisection',\n   'Four',\n   'Quadruplication',\n   'Quadrisection',\n   'Five',\n   'Quinquesection',\n   'Plurality',\n   'Fraction',\n   'Zero',\n   'Multitude',\n   'Fewness',\n   'Repetition',\n   'Infinity'],\n  'TIME': ['Time',\n   'Neverness',\n   'Period',\n   'Contingent Duration',\n   'Course',\n   'Diuturnity',\n   'Transientness',\n   'Perpetuity',\n   'Instantaneity',\n   'Chronometry',\n   'Anachronism',\n   'Priority',\n   'Posteriority',\n   'The Present Time',\n   'Different time',\n   'Synchronism',\n   'Futurity',\n   'The Past',\n   'Newness',\n   'Oldness',\n   'Morning',\n   'Evening',\n   'Youth',\n   'Age',\n   'Infant',\n   'Veteran',\n   'Adolescence',\n   'Earliness',\n   'Punctuality',\n   'Lateness',\n   'Occasion',\n   'Untimeliness',\n   'Frequency',\n   'Infrequency',\n   'Periodicity',\n   'Irregularity of recurrence'],\n  'CHANGE': ['Change',\n   'Permanence',\n   'Cessation',\n   'Continuance in action',\n   'Conversion',\n   'Reversion',\n   'Revolution',\n   'Substitution',\n   'Interchange',\n   'Changeableness',\n   'Stability',\n   'Eventuality',\n   'Destiny'],\n  'CAUSATION': ['Cause',\n   'Effect',\n   'Attribution',\n   'Chance',\n   'Power',\n   'Impotence',\n   'Strength',\n   'Weakness',\n   'Production',\n   'Destruction',\n   'Reproduction',\n   'Producer',\n   'Destroyer',\n   'Paternity',\n   'Posterity',\n   'Productiveness',\n   'Unproductiveness',\n   'Agency',\n   'Physical Energy',\n   'Physical Inertness',\n   'Violence',\n   'Moderation',\n   'Influence',\n   'Absence of Influence',\n   'Tendency',\n   'Liability',\n   'Concurrence',\n   'Counteraction']}}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchy['WORDS EXPRESSING ABSTRACT RELATIONS']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T16:09:28.234882200Z",
     "start_time": "2024-02-18T16:09:28.226739Z"
    }
   },
   "id": "aec564994e3210be",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now save the hierarchy to a JSON file so that we can use it later."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e174ef1a4c29c70a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save the hierarchy to a json file\n",
    "import json\n",
    "\n",
    "with open('hierarchy.json', 'w') as file:\n",
    "    json.dump(hierarchy, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T16:09:28.241864400Z",
     "start_time": "2024-02-18T16:09:28.231880800Z"
    }
   },
   "id": "12981f3c6aadc361",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the directory structure and save the words to files\n",
    "\n",
    "Now that we have the hierarchy, we can create a directory structure that mirrors the structure of the page and save the words under each section to a file in the directory structure.\n",
    "\n",
    "Let's start by creating a function that writes the words to a file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a61acc8b48c234f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def write_words_to_file(words, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T16:09:28.241864400Z",
     "start_time": "2024-02-18T16:09:28.236875800Z"
    }
   },
   "id": "4a0df396346d21f4",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by creating a directory structure that mirrors the structure of the page."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a32b5d427e6421e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_directory_structure(base_path, hierarchy):\n",
    "    for class_name, class_content in hierarchy.items():\n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        os.makedirs(class_path, exist_ok=True)\n",
    "\n",
    "        for division_name, division_content in class_content.get('divisions', {}).items():\n",
    "            division_path = os.path.join(class_path, division_name)\n",
    "            os.makedirs(division_path, exist_ok=True)\n",
    "\n",
    "            for section_name, words in division_content.get('sections', {}).items():\n",
    "                section_path = os.path.join(division_path, section_name)\n",
    "                os.makedirs(section_path, exist_ok=True)\n",
    "                write_words_to_file(words, os.path.join(section_path, 'words.txt'))\n",
    "\n",
    "        for section_name, words in class_content.get('sections', {}).items():\n",
    "            section_path = os.path.join(class_path, section_name)\n",
    "            os.makedirs(section_path, exist_ok=True)\n",
    "            write_words_to_file(words, os.path.join(section_path, 'words.txt'))\n",
    "\n",
    "\n",
    "# Create the directory structure\n",
    "base_path = 'roget_thesaurus_final'\n",
    "create_directory_structure(base_path, hierarchy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T16:09:28.272498200Z",
     "start_time": "2024-02-18T16:09:28.240864Z"
    }
   },
   "id": "16320faac31a0471",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have created the directory structure and saved the words to files, we can take a look at how many words we have in the thesaurus."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6787e3714a46f793"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1057"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the lines in the files\n",
    "line_count = sum([sum([len(open(os.path\n",
    "                                .join(root, file)).readlines()) for file in files]) for root, dirs, files in\n",
    "                  os.walk(base_path)])\n",
    "line_count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T16:09:28.284844900Z",
     "start_time": "2024-02-18T16:09:28.265486500Z"
    }
   },
   "id": "3ec59f5b464d6237",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have 1057 words in the thesaurus. \n",
    "\n",
    "> Note: Some words can exist in multiple sections, so the actual number of unique words is less than 1057."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aba880ee49fcbde1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Word Embeddings\n",
    "\n",
    "We can get different embedding models from various sources such as `GloVe`, `fastText`, `Word2Vec`, `BERT`. \n",
    "Even better from models such as `Gemini` or `OpenAI's` `text-embedding-3` which are more advanced and can provide better embeddings.\n",
    "\n",
    "In our case, we will use the brand new `Nomic` embeddings and more specifically the `nomic-embed-text-v1.5`,\n",
    "a text embedding model that supports variable output sizes.\n",
    "\n",
    "It supports for variable embedding size and specialized for retrieval, similarity, \n",
    "clustering and classification. Recommended output sizes are 768, 512, 256, 128 and 64.\n",
    "Source: [Nomic Embeddings Descriptions](https://docs.nomic.ai/atlas/models/text-embedding).\n",
    "\n",
    "Noteworthy to mention is that the `nomic-embed-text-v1.5` has a MTEB Score of 62.28 which is a very good score and close to the best score\n",
    "of the `text-embedding-3-large` which has a MTEB Score of 64.6 of OPENAI with the same input sequence length of 8192.\n",
    "However, the `nomic-embed-text-v1.5` is open-source and free to use, with a free trial of 1M tokens.\n",
    "\n",
    "More Details here : [Nomic Embeddings](https://blog.nomic.ai/posts/nomic-embed-text-v1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ca9c53585b2d691"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will save two different versions of the embeddings. \n",
    "One will be embeddings with a task type specialized for clustering\n",
    "and one will be embeddings with a task type specialized for classification."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dee3c4906a00d915"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best option to use Nomic Embed is through our production-ready Nomic Embedding API.\n",
    "\n",
    "We can access the API via the python package `nomic` which is a Python client for the Nomic API.\n",
    "We will access them nomic library to get the embeddings for the words in the thesaurus.\n",
    "\n",
    "We can get a Nomic Atlas API key at [Nomic Atlas](https://atlas.nomic.ai/) after following the instructions to create an account and get the API key.\n",
    "\n",
    "Then in our terminal we need to run the following command to set the API key as an environment variable\n",
    "```bash\n",
    "nomic login ```YOUR_API_KEY```\n",
    "```\n",
    "\n",
    "\n",
    "With the above set we can now get the embedding model and use it to get the embeddings for the words in the thesaurus."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2de02e7c969ac20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Langchain provides many useful tools.\n",
    "\n",
    "We can start by loading the words from the files using the `DirectoryLoader` and `TextLoader` classes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26df6b80bb151fc6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:00<00:00, 5571.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "# pip install langchain-community==0.0.19\n",
    "\n",
    "path = \"roget_thesaurus\"\n",
    "text_loader_kwargs = {'autodetect_encoding': True}\n",
    "loader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader,\n",
    "                         loader_kwargs=text_loader_kwargs, show_progress=True,\n",
    "                         use_multithreading=True)\n",
    "docs = loader.load()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T16:21:21.054181400Z",
     "start_time": "2024-02-18T16:21:21.033621300Z"
    }
   },
   "id": "c357c6e7881a60ad",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create a list of the documents where each document is a word from the files and split the documents according to \\n\n",
    "docs = [doc.page_content.split(\"\\n\") for doc in docs]\n",
    "# Now make the documents a flat list\n",
    "docs = [word for doc in docs for word in doc]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T16:21:22.669478800Z",
     "start_time": "2024-02-18T16:21:22.665182100Z"
    }
   },
   "id": "130e6d0c889c63a2",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will store the embeddings to a vector database using `chromadb` which is a vector database that can store and query embeddings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41d468a8f1c5a471"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection_clustering = client.create_collection(name=\"nomic_clustering_v1\")\n",
    "collection_classification = client.create_collection(name=\"nomic_classification_v1.5\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5a9facda5b176b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now use the `nomic` library to get the embeddings for the words in the thesaurus with the `nomic-embed-text-v1.5` model\n",
    "and the specialized task type for classification.\n",
    "\n",
    "From some background tests, it was concluded that the `nomic-embed-text-v1` model produces better embeddings for clustering\n",
    "so we will use this model for the specialized task type for clustering."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "917e51650018a920"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from nomic import embed\n",
    "\n",
    "nomic_embeddings_clustering = embed.text(\n",
    "    texts= docs,\n",
    "    model='nomic-embed-text-v1',\n",
    "    task_type='clustering'\n",
    ")\n",
    "\n",
    "nomic_embeddings_classification = embed.text(\n",
    "    texts= docs,\n",
    "    model='nomic-embed-text-v1.5',\n",
    "    task_type='classification'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c6eac001e6fb98e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now add the embeddings to the vector database using `chromadb` into two different collections, one for clustering and one for classification."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5439a0d28512984b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "collection_clustering.add(\n",
    "    documents=docs,\n",
    "    embeddings=nomic_embeddings_clustering['embeddings'],\n",
    "    metadatas=[{\"word\": word} for word in docs],\n",
    "    ids=[f\"word_{i}\" for i in range(len(docs))]\n",
    ")\n",
    "\n",
    "collection_classification.add(\n",
    "    documents=docs,\n",
    "    embeddings=nomic_embeddings_classification['embeddings'],\n",
    "    metadatas=[{\"word\": word} for word in docs],\n",
    "    ids=[f\"word_{i}\" for i in range(len(docs))]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bee8030e4ca31d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
